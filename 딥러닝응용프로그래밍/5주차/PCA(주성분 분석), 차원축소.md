<div align="center">
  <h2>PCA(주성분 분석), 차원축소</h2>
</div>

### 🟣 주성분 분석 PCA, Principal Component Analysis
```
차원 축소에 널리 사용되는 대표적 기법
PCA의 주요 목적은 데이터 내에 존재하는 여러 변수들 중에서 데이터를 잘 설명할 수 있는
핵심적인 주성분을 찾아내어 차원을 줄이는 것
이는 데이터의 중요한 특성을 보존하면서, 복잡도를 줄이기 위한 방법
```
### 🟣 PCA의 주요 개념
#### ◼️ 1. 차원축소
```
데이터를 잘 설명할 수 있는 중요한 주성분을 선택함으로써 전체 데이터에서 중요하지 않은 차원을
제거하여 차원을 축소하는 과정 -> 데이터의 정보 손실 최소화 & 데이터의 복잡성 줄이는 것이 목표
```
#### ◼️ 2. 차원의 저주 방지
```
차원이 커질수록 데이터 분석이 어려워지며, 차원이 너무 많으면 데이터의 밀도가 낮아지는 문제
PCA는 이러한 문제를 완화하는데 도움을 줌
```
#### ◼️ 3. 양적 변수
```
PCA는 등간, 비율척도와 같은 양적 변수들 간의 관계를 분석하며, 각 변수가 서로 독립적이고
정규분포를 따른다고 가정 
```
### 🟣 비지도학습인 PCA
#### ◼️ 1. 비지도학습 Unsupervised Learning에 속함
```
PCA는 레이블, 정답이 없는 데이터에 적용하여 데이터의 패턴을 발견하는 기법으로,
지도학습처럼 정답을 예측하는 것이 아니라, 데이터의 차원을 줄여 중요한 특징들을 추출
```
#### ◼️ 2. 데이터 차원의 축소 및 중요한 정보 보존
```
PCA는 데이터의 차원, 특성의 수 줄이면서도 중요한 정보를 보존하는 것이 목표
모든 특성을 사용하는 대신, 불필요하거나 유의미하지 않은 변수를 제거하고, 그 중에서 중요한 주성분 추출
=> 데이터의 복잡성을 줄이면서 중요한 특징 유지
- ex) 데이터의 각 특성을 구분하기 위해 모든 데이터를 활용할 필요가 없으며,
유의미하지 않거나 불필요한 변수를 제거하고 중요한 성분만 남기는 방식
- 잠재적 성분만을 추출하며, 데이터를 축소하지만 데이터의 핵심적인 부분 유지 
```
### 🟣 차원 축소 
#### ◼️ 데이터의 차원(특성, 속성의 수)을 줄이면서 중요한 정보 보존
```
차원의 수가 많을수록 계산 복잡성이 증가, 불필요한 노이즈가 추가될 수 있기 때문에,
중요한 특징만을 남겨 차원을 줄이는 것이 매우 중요
```
#### ◼️ 모든 특징을 살릴 수는 없음
```
2차원에서 1차원으로 차원을 축소할 수 있지만, 모든 특징을 살릴 수는 없으며,
최대한 중요한 부분만 보존하는 것이 차원 축소의 핵심
```
#### ◼️ 노이즈 제거
```
차원이 높아지면 데이터에 포함된 노이즈도 증가할 수 있는데, PCA와 같은 차원축소 기법은
이러한 노이즈를 제거하는데 도움을 줌 -> 데이터의 비선형성 줄이는 효과
```
### 🟣 차원 축소의 장점
```
1. 노이즈 제거
불필요한 정보와 잡음이 제거되어 데이터의 본질적인 특징을 더 잘 파악

2. 계산 효율성 증가
차원이 줄어들면 연산에 필요한 자원과 시간이 줄어들어 효율성이 높아짐

3. 데이터 시각화 용이
차원을 줄이면, 데이터가 2또는 3차원 공간에서 더 쉽게 시각화

4. 과대적합 Overfitting 방지
차원이 높으면 모델이 불필요하게 복잡해져서 학습 데이터에 과도하게 맞춰질 수 있는데,
차원 축소를 통해 이러한 문제 해결 -> 차원의 저주 문제 완화 
```

### 🟣 차원 축소의 단점
```
1. 어떤 차원을 유지할지 결정하는 어려움
어떤 변수를 제거하고 유지할지 결정하는 과정이 어려움, 잘못된 변수 제거하면 중요한 정보 손실

2. 정보 손실 위험
차원을 줄이는 과정에서 데이터의 복잡성을 충분히 표현하지 못하면 언더피팅 Underfitting 발생
이는 모델이 데이터의 패턴을 제대로 학습하지 못하는 상황 의미 
```

### 🟣 주성분 분석 PCA과 데이터 분산의 중요성
```
- 분산 Variance
데이터가 어마나 특정 방향으로 퍼져있는가 나타내는 지표
PCA는 이 분산을 최대화하는 주성분 Principal Component를 찾아냄

- 주성분
데이터의 큰 분산 방향을 따르는 성분을 말하며, 이 방향은 데이터의 주요 정보와 패턴 포함
주성분은 데이터의 변화를 가장 많이 설명하는 방향

- 주성분을 통한 차원 축소
주성분 방향으로 데이터를 투영하여 차원을 축소함으로써 정보 손실 최소화
이는 높은 차원을 줄이면서도 데이터의 중요한 특징 유지하는데 도움

- 데이터들의 차이점 명확화
PCA통해 차원축소를 하면, 데이터 간의 차이점이 명확하게 드러남 -> 데이터의 패턴 분석 용이

- 노이즈의 영향
노이즈가 많으면 데이터의 차원이 높아질 수 있으며, 이로인해 비선형성이 커짐
PCA는 노이즈의 영향을 줄여 데이터 분석을 더 효율적으로 수행할 수 있게 함 
```

### 🟣 PCA의 수행과정
```
중복성이란 데이터 간에 중복되는 정보의 정도
PCA는 데이터 간 중복성을 최소화하는 방향을 찾아 데이터를 압축
데이터의 구조를 가장 잘 반영하는 '최적의 기저'를 선택하는 것이 핵심
기저란 데이터를 표현하는 축 또는 기준선으로, PCA에서는 이 기저가 데이터의 분산 최대화하는 방향 따름
```
#### 🔴 최적의 기저
```
PCA는 데이터를 분석할 때 가장 중복성이 적은 방향으로 데이터를 투영하여, 정보가 중복되지 않도록 함
=> 데이터를 효율적으로 압축
중복성이 낮음일 때는 분산이 큰 방향으로 데이터가 퍼져있어 패턴을 찾기 쉬우며,
중복성이 높음일 때는 비슷한 정보가 중복되어 패턴을 분석하기 어려움
```
### 🟣 중복성의 문제 
```
중복성이 높은 기저를 선택하면, 비슷한 정보가 중복되어 포함되기 때문에 PCA가 본래의 목적을
(정보 압축 및 패턴 추출)을 제대로 수행하지 못하게 됨
데이터의 주요 패턴을 파악하기 어려워지고 정보 손실 발생할 가능성 커짐 
```
### 🟣 주성분 선택하는 과정
```
- 주성분 선택
PCA에서 주성분은 데이터의 분산을 최대한 설명하는 축 의미

- 분산의 중요성
데이터가 가장 넓게 퍼져있는 방향, 즉 분산이 가장 큰 방향이 데이터를 가장 잘 설명할 수 있는 축
PCA는 이 분산이 큰 방향을 주성분으로 설정해 데이터의 정보 최대한 압축 
```
### 🟣 (0,0) 좌표에서 각 점까지의 거리 
```
(0,0) 좌표에서 각 점까지의 거리 계산한 후, 이 거리가 가장 긴 축이 주성분
주성분은 데이터의 분산을 가장 크게 설명할 수 있는 방향
각 포인트에서 (0,0)까지의 거리를 합했을 때, 가장 큰 거리를 가지는 축
이 방향은 데이터의 분산을 최대화하며, 데이터의 주요 패턴 설명 
```
### 🟣 PCA 과정
#### ◼️ 1. 평균 계산
```
각 데이터의 값을 중앙값이나 평균으로부터 얼마나 떨어져있는지 계산
이 과정은 데이터를 중심화하는 첫 단계로, PCA가 데이터의 변화를 잘 설명할 수 있도록 준비하는 과정
```
#### ◼️ 2. 데이터 중심화 Centering
```
- 각 데이터에서 평균값을 빼서 데이터를 중심으로 모음
중심화 과정을 하지 않으면, 주성분 방향이 올바르게 일치하지 않아 데이터의 분포 왜곡
- 중심화는 데이터의 평균을 0으로 맞추어 PCA가 올바르게 작동하도록 하는 중요한 과정
```
#### ◼️ 3. 공분산 행렬 계산
```
공분산은 데이터의 두 변수 간의 관계를 설명하는 값, 데이터가 어떻게 분산되고 상관관계까 있는지 나타냄
공분산 행렬을 계산하여, 데이터의 분포와 상관성 분석
```
#### ◼️ 4. 고유값과 고유벡터 계산
```
공분산 행렬에 분해과정을 적용해, 데이터의 주요 축을 찾기 위해 고유값과 고유벡터 구함
고유값은 데이터의 분산을 설명하는 비율 나타냄, 고유벡터는 주성분 방향
```
#### ◼️ 5. 주성분 선택 
```
- 계산된 고유값 중 가장 큰 값을 가진 고유벡터가 첫 번째 주성분, 데이터의 분산을 가장 많이 설명하는 축
- 고유값이 큰 순서대로 고유벡터를 선택하여 여러 주성분 정의 
```
### 🟣 기저들은 서로 수직(orthogonal)
```
PCA에서 선택된 축은 서로 수직이 되어야 함 -> 데이터의 중복성 최소화, 독립적인 정보얻음

1. 독립적인 정보의 분리
한 축에서 설명되는 정보는 다른 축에서 설명되는 정보와 중복되지 않도록 하기 위한 것
두 축이 수직일 때, 각 축은 데이터의 서로 다른 측면 설정, 두 축에서 얻는 정보는 서로 중복X

2. 최대 분산 설명하는 축 찾기
PCA의 목표는 데이터의 분산을 최대화하는 방향=축을 찾는 것
첫 번째 주성분은 데이터의 분산을 가장 많이 설명하는 축
두 번째 주성분을 선택할 때는, 첫 번째에서 설명되지 않은 남은 분산을 최대한 설명
새로운 정보를 가장 잘 설명하는 방향으로 설정

3. 데이터 해석 용이성
수직으로 설정된 기저는 해석이 간단
각 축은 서로 다른 특성을 독립적으로 설명하므로, 데이터 분석 과정에서 해석하기 쉬움
기저들이 수직이 아닌 경우, 축 간 중복되는 정보가 발생하여 데이터 이해하기 어려워짐

4. 수학적 성질
PCA는 고유벡터를 계산하는데 이 고유벡터는 서로 직교인 성질을 가짐
공분산 행렬의 특성상, PCA로 도출된 고유벡터들은 수학적으로 직교 
직교하지 않은 축은 수학적 근거와 맞지 않기 때문에, 반드시 수직이어야 함

=> PCA에서 기저들이 수직이어야 하는 이유는 독립적이고 중복되지 않은 정보를 얻기 위함이며,
데이터의 분산을 최대한 잘 설명하고, 해석하기 쉽게 하기 위한 필수적인 조건
```







