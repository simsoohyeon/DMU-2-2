<div align="center">
<h2>
  4. 혼동행렬 
</h2>
</div>

### 🟣 혼동행렬
```
혼동 행렬은 분류 모델의 성능을 평가하는데 사용되는 표
주로 이진 분류 문제에서 예측한 결과와 실제 정답을 비교하여 정확도를 확인할 수 있다
혼동행렬은 모델이 올바르게 또는 잘못 예측한 결과를 4가지로 나눠 표현한 표
```
### 🟣 혼동행렬의 구조
#### 혼동행렬은 실제 클래스와 모델이 예측한 클래스의 조합으로 나눠지며, 4개의 구역으로 구분
#### ◼️ 참긍정(TP) 
```
실제도 긍정, 모델도 긍정으로 예측한 경우
```
#### ◼️ 거짓부정(FN)
```
실제로 긍정, 모델을 부정으로 잘못 예측한 경우
```
#### ◼️ 거짓긍정(FP)
```
실제로 부정, 모델을 긍정으로 잘못 예측한 경우
```
#### ◼️ 참부정(TN)
```
실제로 부정, 모델도 부정으로 예측한 경우
```
### 🟣 혼동 행렬로 계산하는 주요 지표 => 모델 성능 평가
#### ◼️ 민감도 (Sensitivity, 또는 재현율 Recall, TPR)
```
모델이 실제 긍정인 데이터 중에서 긍정으로 정확히 예측한 비율
민감도는 실제 긍정인 데이터 중에서 모델이 얼마나 잘 긍정을 찾아내는지 측정
ex) 암을 진단하는 모델에서는 재현율이 높을수록 암 환자를 놓치지 않고 진단하는 능력이 좋다는 의미 
```
<div align="center">
  <H2>TP / TP + FN </H2>
</div>

#### ◼️ 정밀도 (Precision)
```
모델이 긍정으로 예측한 데이터 중에서 실제로 긍정인 데이터의 비율
정밀도는 모델이 예측한 긍정 중에서 실제로 맞는 비율
=> 예측이 긍정일 때, 그 예측이 얼마나 신뢰할 수 있는지를 보여줌
ex) 스팸 메일 필터링에서 정밀도가 높다는 것은, 스팸으로 분류된 이메일 중 실제로 스팸인 경우가
많다는 뜻, 정밀도가 높을수록 예측 알고리즘의 안정성이 높다는 것을 의미 
```
<div align="center">
  <H2>TP / TP + FP </H2>
</div>

#### ◼️ 정확도 (Accuracy)
```
전체 데이터 중에서 모델이 올바르게 예측한 비율
정확도는 모델이 전체 데이터에서 맞춘 비율
=> 정확도가 높으면 전체 데이터 중에서 모델이 잘 예측한 경우가 많다는 뜻
클래스 불균형이 심한 데이터에서는 정확도만으로 성능을 평가하기 어려움 
```
<div align="center">
  <h2>TP + TN / TP + TN + FP + FN</h2>
</div>

![image](https://github.com/user-attachments/assets/2a8d4d8e-6b0d-4581-a05e-336c6db05eb6)

### 🟣 F1-Score
```
F1-Score는 정밀도 Precision와 재현율 Recall의 조화평균을 사용한 지표
정밀도와 재현율 사이의 균형을 고려한 지표로, 클래스 불균형이 있는 데이터에서 유용
정밀도와 재현율 중 어느 한쪽으로 치우치지 않고, 두 지표를 동시에 고려할 때 사용 
```
<div align="center">
  <h2>2 X Precision * Recall / Precision + Recall </h2>
</div>

### 🟣 2 X 2 혼동행렬
![image](https://github.com/user-attachments/assets/046214c3-21f1-4bcb-adcd-f59804d44a48)
```
1. 정확도 Accuracy
정확도는 전체 데이터 중에서 모델이 올바르게 예측한 비율
즉, 참 긍정 TP와 참 부정 TN을 더한 값을 전체 데이터 수로 나눈 값
=> 20 + 10 / 20 + 30 + 40 + 10 = 30%
```
```
2. 정밀도 Precision
정밀도는 모델이 긍정으로 예측한 데이터 중 실제로 긍정인 경우의 비율
즉, 참 긍정 TP을 참 긍정 TP와 거짓 긍정 FP의 합으로 나눈 값
=> 20 / 20 + 40 = 33%
```
```
3. 재현율 (Recall 또는 민감도 Sensitivity)
재현율은 실제 긍정인 데이터 중에서 모델이 올바르게 긍정으로 예측한 비율
즉, 참 긍정 TP을 참 긍정 TP와 거짓 부정 FN으로 나눈 값
=> TP / TP + FN = 40%
```
### 🟣 조화평균 그래프와 F1-Score 그래프
#### 🔴 조화평균 그래프, 위쪽
```
조화평균을 시각화한 것으로, F1-Score가 어떻게 정밀도와 재현율의 균형을 잡는지 보여줌
- A와 B는 각각 정밀도와 재현율을 나타냄
- a와 b는 각각 A와 B를 기준으로 F1-Score로부터의 거리
- h는 w에서의 조화평균 값
조화평균은 일반적인 산술평균보다 두 값의 균형을 더 잘 잡음
값 중 하나가 낮으면, 조화평균 값이 그 값에 더 큰 영향을 받아 낮아지므로 F1-Score는 둘 중
하나의 값이 너무 낮아도 성능을 높게 평가하지 않음 
```
#### 🔴 F1-Score 그래프, 아래쪽
```
- 파란색 Recall: 모델이 실제로 긍정인 데이터 중에서 얼마나 잘 찾아내는지 (재현율)
- 초록색 Precision: 모델이 긍정으로 예측한 것 중에서 실제로 맞는 것의 비율 (정밀도)
- 빨간색 F1-Score: 재현율과 정밀도의 조화평균으로, 두 지표의 균형을 나타냄
이 그래프는 정밀도와 재현율이 얼마나 다를 수 있는지를 나타내며, F1-Score가 두 값 사이에서
균형을 잡는 방식을 보여줌
F1-Score는 두 값이 서로 비슷할수록 높아지며, 둘 중 하나가 지나치게 낮으면 낮아지게 됨 
```
![image](https://github.com/user-attachments/assets/40f1f7ec-b7e8-44d4-ab5d-98de10ae023e)

### 🟣 3 X 3 혼동행렬
![image](https://github.com/user-attachments/assets/fa518b80-51f4-4360-9e1a-8686340ed63a)

1. 정확도 
```
전체 데이터에서 모델이 올바르게 예측한 비율을 의미
모든 클래스에서 참 TP인 값들을 전체 예측한 값으로 나눈 값 
- 모델이 맞춘 경우 -> 4번 맞춤 / 전체 데이터 수 -> 7개
정확도: 4/7 = 57%
```
2. 재현율 Recall
```
전체 재현율 = 각 클래스의 재현율 총합 / 클래스의 개수 
예시에서는 3개의 클래스의 재현율을 더한 값을 3으로 나누어 계산
3개의 클래스가 있기 때문에, 각 클래스의 재현율을 더한 뒤 클래스의 개수인 3으로 나누어 전체 재현율 계산
=> 각 클래스에 대한 재현율이 모두 중요하고, 이 값들을 평균 내어 전체적인 성능을 측정하는 방식  
재현율: 1/2 + 1/2 + 2/3 / 3 = 5/9 = 55%
```
3. 정밀도 Precision 
```
모델이 긍정으로 예측한 것 중 실제로 긍정인 비율, 즉 TP / TP + FP
정밀도: 1 + 1/3 + 2/3 / 3 = 66%
```
### 🟣 4 X 4 혼동행렬
```
1. 정확도
11 / 20 = 55 / 100 = 0.55
```
```
2. 재현율
2/5 + 1 + 3/5 + 4/5 / 4 = 14/5 / 4 = 14 / 20 = 0.7
```
```
3. 정밀도
2/3 + 5/7 + 3/4 + 2/3 / 4 = 0.67 + 0.71 + 0.75 + 0.67 / 4 = 2.8 / 4 = 0.7
```
```
4. F1-Score 조화평균 
```
### ➕ 계산할 때...
```
- 계산과정 복잡하면 분수 -> 소수로 변환해 계산하기
- 분수는 무조건 약분할 수 있는 만큼 하고 계산
```









