<div align="center">
<h2>PDF 5-1. 선형회귀, 경사하강법 </h2>
</div>

### 🟣 선형회귀의 역사 history
```
프랜시스 골턴은 아버지(x)와 아들(y)의 키에 대한 연구에서 회귀 분석(regression analysis)이라는 용어를 사용했다.
골턴은 최소자승법((least squares line)을 적용해 아버지의 키로 아들의 키를 예측할 수 있는 선형 모델을 만들었다.
아버지의 키가 평균보다 크면, 아들의 키도 평균보다 클 가능성이 있지만, 아버지의 키만큼 크지 않다라는 사실을 발견했다.
아버지의 키가 평균보다 작을 때도 동일한 효과가 관찰되었다. 아들의 키는 평균에 가까워지는 경향을 보였다.
이러한 이유로 골턴은 이 최소자승법을 회귀선(regression line)이라고 불렀다.
```
### ➕ 개념설명 
```
회귀 분석은 두 변수 간의 관계를 분석하는 통계적 기법이다. 골턴이 수행한 이 연구는 부모와 자신 간의
키 상관 관계를 조사했으며, 부모의 키가 평균보다 크거나 작을 때 자식의 키가 어느 정도로 비슷하게
나오는지 보여준다. 중요한 것은 자식의 키가 부모의 키에 영향을 받지만, 부모의 키만큼 극단적이지 않고
평균에 가까워진다는 것이다. 이 현상을 회귀라고 부르며, 자식이 부모보다 평균에 가깝게 회귀한다는 점
에서 회귀 분석이라는 용어가 유래하였다.
```

### 🟣 선형회귀란 (Linear regression)
#### ◼️ 1. 가장 간단한 모델 중 하나
```
데이터 분석에서 가장 기본적이고 단순한 예측 모델, 두 변수 간의 직선 관계를 기반으로 미래 값을 예측하고 설명
```
#### ◼️ 2. 연속된 변수를 예측하기 위한 모델
```
연속형 변수를 예측하는데 사용됨. ex) 주택 가격, 실업률 예측, 공부 시간에 따른 성적 예측
연속된 값이란, 특정 범위 내에서 끊김없이 변할 수 있는 변수를 의미
```
#### ◼️ 3. 산업 및 과학 분야에 널리 응용
```
선형 회귀는 다양한 산업 및 과학 연구에서 사용됨
ex 경제 분야에서 주식 가격 예측, 과학 분야에서 실험 데이터 분석 등 여러 방면에 응용
```
#### ◼️ 4. 두 변수 간의 관계를 분석 
```
ex 공부시간과 성적의 관계를 분석할 때, 공부시간이 늘어남에 따라 성적이 어떻게 변화하는지 설명
```
#### ◼️ 5. 선형회귀의 유형
```
1. 단순선형 회귀 Simple Linear Regression
하나의 독립 변수와 하나의 종속 변수를 사용하는 가장 기본적인 형태

2. 다중 선형 회귀 Multiple Linear Regression
여러 개의 독립 변수를 사용해 종속 변수를 예측하는 방식
ex 성적을 예측할 대 공부시간 뿐만 아니라 수면 시간, 과외 시간 등 변수를 함께 고려할 수 있다

3. 다항 회귀 Polynomial Regression
종속 변수와 독립 변수 간의 관계가 직선이 아닌 곡선일 때 사용하는 회귀 기법
이 경우, 독립 변수에 제곱, 세제곱 등 다항식을 추가하여 모델링 
```
### ➕ 추가설명
```
선형 회귀의 기본개념은 y=mx+b라는 직선 방정식을 이용해 데이터를 모델링하는 것임
여기서 m은 기울기(독립 변수의 변화에 따른 종속 변수의 변화율), b는 절편(독립 변수가 0일 때
종속 변수의 값)임 주어진 데이터 포인트들과 이 직선 사이의 오차를 최소화하는 방식으로
최적의 선형 방정식을 찾는 것이 선형 회귀의 핵심 
```
### 🟣 선형 회귀의 모형과 특성
#### ◼️ 1. 선형적 관계 분석
```
여러 특성이 특성 값과 얼마나 선형적인 관계를 가지고 있는지 파악하는데 사용됨
이는 변수들 간의 직선적인 관계를 측정하며, 예측 모델을 만드는데 중요한 기초가 됨
```
#### ◼️ 2. 상관 관계 분석
```
선형 회귀에서 상관계수 Correlation coefficient 를 통해 두 변수 간의 선형적 관계를 측정 가능함
상관계수는 -1부터 1 사이 값을 가지며, 1에 가까울 수록 두 변수 간의 관계가 강한 양의 선형 관계
반면, -1에 가까울 수록 강한 음의 상관계를, 0에 가까울수록 상관이 거의 없음을 의미함
```
#### ◼️ 3. 지도학습 Supervised Learning의 일종
```
선형 회귀는 지도학습 방법 중 하나로, 훈련 데이터에 있는 입력과 출력 관계를 학습하여
새로운 데이터에 대해 예측할 수 있도록 함
```
#### ◼️ 4. 연속형 변수를 출력
```
선형 회귀는 연속적인 값을 예측하는데에 주로 사용됨 이는 분류 문제와는 달리 타겟 값이 연속적인 변수를 의미하며,
주식 가격, 온도 변화, 주택 가격 등의 예측에 활용된다
```
#### ◼️ 5. 수식
```
선형 회귀의 대표적인 수식은 y=ax+b로 나타낼 수 있다 여기서 a는 직선의 기울기, b는 절편
이 직선은 주어진 데이터의 경향을 가장 잘 설명하는 직선을 의미함
```
### ➕ 추가설명
```
선형 회귀는 데이터 간의 관계를 직선 형태로 모델링하기 때문에 직관적이고 해석하기 쉬움
실제 데이터는 항상 직선적인 관계를 가지는 것이 아니므로, 때에 따라서는 더 복잡한 선형 모델 필요
```
### 🟣 회귀 
```
회귀는 주어진 데이터를 2차원 공간에 나타내고, 그 데이터들을 가장 잘 설명하는 직선 또는 곡선 찾는 문제
이 과정에서 데이터 포인트들의 경향을 설명하는 최적의 직선(선형 회귀) 또는 곡선(비선형 회귀) 찾는 것이 목적
y=f(x)에서 출력인 y는 입력인 x에 대한 함수값으로, 둘 모두 실수일 때 이 함수는 f(x)를 예측하는 것이 회귀의 핵심
주어진 데이터로부터 입력 x에 따른 출력 y를 가장 잘 예측하는 함수 관계를 찾는 것
```
![image](https://github.com/user-attachments/assets/bad44ee5-6983-484f-b6b0-6084e2bb677e)
### 🟣 선형 회귀 VS 비선형 회귀
```
1. 선형 회귀
첫 번째와 두 번째 그래프는 선형 관계를 설명한다 데이터를 설명하는 직선(빨간색)이 그려져 있고,
데이터 포인트(파란색)들이 근접해있는 모습이다 데이터를 가장 잘 설명하는 선형 회귀 선이다
2. 비선형 회귀
세 번째 그래프는 비선형 관계를 설명하는 곡선 회귀이다 데이터가 직선보다 곡선으로 설명될 때,
비선형 회귀를 사용하여 데이터를 모델링한다 그래프에서 볼 수 있듯, 곡선에 더 잘 맞추어져 있
```
```
+ 데이터가 직선적으로 분포하지 않을 경우 비선형 회귀가 더 적합할 수 있다
+ 선형 회귀와 비선형 회귀 모두 데이터와 모델 간의 오차를 최소화하는 방식으로 학습되며,
회귀 모델은 데이터 분석, 예측, 통계 모델링에서 광범위하게 사용됨 
```
### 🟣 선형 회귀 예제
```
<< 예제 설명 >>
1. 훈련 데이터 => 키와 몸무게 데이터가 표로 주어짐
2. 학습 과정
주어진 데이터를 기반으로 선형 회귀 모델이 학습됨 => 모델은 주어진 데이터에서 키와 몸무게
간의 관계를 학습하여 새로운 키 값을 입력받았을 때 예측할 수 있도록 함
3. 모델 예측 => 학습된 관계를 이용해 몸무게 예측
```
### 🟣 단순 선형회귀
```
하나의 독립 변수 x와 하나의 종속 변수 y 간의 관계를 설명하는 모델
f(x)=wx+b w는 기울기, b는 절편을 의미함
독립 변수(키)가 변할 때, 종속 변수(몸무게)가 어떻게 변화하는지 설명함
```
### 🟣 다중 선형회귀
```
여러 독립 변수를 사용해 종속 변수를 예측하는 모델임
ex) 매출을 예측할 때 인터넷 광고, 신문 광고, TV 광고 등 여러 변수들을 고려할 수 있음
f(x,y,z)=w0+w1x+w2y+w3z
여기서 w0는 절편, w1,w2,w3는 각 변수에 대한 가중치(기울기)를 나타냄 
```
### 🟣 데이터 좌표를 기반으로 방정식 만들기
```
샘플좌표 (-1,0) (0,1) (0,3)
<<데이터에 대한 방정식>> => y=mx+b에 대입!!!
f(-1)=-m+b=0
f(0)=b=1
f(0)=b=3
```
### 🟣 행렬 표현
```
|-1 1 |  | m |   |  0 |
|0  1 |  | b | = |  1 |
|0  1 |          |  3 |
=> 행렬의 방정식을 풀면 최적의 기울이 m과 y절편인 b를 구할 수 있게 됨
```
### 🟣 최적화 경계선
```
각 데이터 포인트와 직선 간의 차이가 '오차'이며 이를 최소화하는 방향으로 선형 회귀 모델이 학습
오차(잔차)는 실제 데이터 값 - 예측된 값으로 계산되며, 이 오차들을 최소화하는 것이 선형 회귀의 목표
```
### 🟣 손실 함수와 비용함수
#### ◼️ 손실함수
```
모델이 예측한 값과 실제 값 간의 차이를 측정하는 함수
이 함수는 예측의 정확도를 평가하는 데 사용, 차이를 줄이기 위해 기울기와 절편을 최적화하는 방향으로 모델 학습
수식: e1 = 1/2(y'i-yi)^2
=> ei는 i번째 데이터 포인트에 대한 손실값, y'i는 예측값, yi는 실제값
=> 예측된 값 y'i와 실제값 yi 사이의 차이를 제곱하여 나타냄
```
#### ◼️ 비용함수
```
여러 개의 손실 함수 값들의 평균을 취한 것으로, 손실 함수가 개별 데이터 포인트에서의 오차를 나타내면,
비용함수는 전체 데이터에 대해 얼마나 잘 작동하는 지를 나타내는 지표
비용함수는 전체 데이터에 대한 손실 값을 합산한 후 평균을 취한 값, 최소화하는 것이 목
```
### 🟣 경사하강법 gradient descent method
```
손실함수 loss function 또는 비용함수 cost function의 값을 최소화하기 위해 사용하는 최적화 알고리즘
손실함수가 정의된 어떤 공간에서 그 함수의 값을 최소화하는 최적의 매개변수 W와 b를 찾는 것이 목적
```
### 🟣 경사하강법의 특징
```
1. 일반적인 방법
손실함수의 형태나 매개변수의 수와 관계없이 사용할 수 있는 일반적인 최적화 방법
회귀 모델뿐만 아니라 다양한 머신러닝 모델에서도 사용되는 핵심 알고리즘
2. 점진적인 학습
반복적인 학습 과정을 통해 매개변수의 값을 점진적으로 업데이트함
처음에는 임의의 값을 시작으로, 손실 함수의 기울기를 계산하여 매개변수를 조금씩 수정해감
3. 비용함수 최소화
비용함수를 최소화하는 값을 찾는데에 사용 이는 주어진 데이터에 가장 잘 맞는 회귀선을 찾기 위해
비용함수의 값을 점점 줄여나가는 과정
```
### 🟣 경사하강법의 원리
```
경사하강법은 비용함수를 최소화하기 위한 방법으로, 함수의 기울기를 이용해 최소값을 찾음
기울기가 존재하는 방향으로 매개변수를 조금씩 조정하여 손실함수의 최소점을 찾아간다 
```
#### ◼️ 음의 기울기
```
함수의 기울기가 음수인 경우, 매개변수는 양의 방향으로 이동해야 함
매개변수를 증가시키면서 더 작은 손실값을 찾아나감
```
#### ⬛ 양의 기울기
```
기울기가 양수인 경우, 매개변수는 음의 방향으로 이동해야 함
매개변수를 감소시켜 손실함수를 최소화하는 방향으로 이동함
```
#### ⬛ 이동방향
```
기울기를 기준으로 이동 방향을 설정
기울기가 양수일 때는, 매개변수를 왼쪽(음의 방향)으로 이동시켜야 손실 값을 줄일 수 있다
기울기가 음수일 때는, 매개변수를 오른쪽(양의 방향)으로 이동시켜야 손실 값을 줄일 수 있다
```
### 🟣 학습률(learning rate, 알파)의 중요성
```
학습률은 매개변수를 한 번 업데이트할 때 이동하는 크기를 결정하는 값
알파라고 불리며, 학습률이 최적화 과정에서 중요한 역할을 함
```
#### ⬛ 1. 학습률이 낮을 때
```

```
#### ⬛
#### ⬛











