<div align="center">
<h2>PDF 5-1. 선형회귀, 경사하강법 </h2>
</div>

### 🟣 선형회귀의 역사 history
```
프랜시스 골턴은 아버지(x)와 아들(y)의 키에 대한 연구에서 회귀 분석(regression analysis)이라는 용어를 사용했다
골턴은 최소자승법((least squares line)을 적용해 아버지의 키로 아들의 키를 예측할 수 있는 선형 모델을 만들었다
아버지의 키가 평균보다 크면, 아들의 키도 평균보다 클 가능성이 있지만, 아버지의 키만큼 크지 않다라는 사실을 발견했다
아버지의 키가 평균보다 작을 때도 동일한 효과가 관찰되었다. 아들의 키는 평균에 가까워지는 경향을 보였다
이러한 이유로 골턴은 이 최소자승법을 회귀선(regression line)이라고 불렀다
```
### ➕ 개념설명 
```
회귀 분석은 두 변수 간의 관계를 분석하는 통계적 기법이다. 골턴이 수행한 이 연구는 부모와 자신 간의
키 상관 관계를 조사했으며, 부모의 키가 평균보다 크거나 작을 때 자식의 키가 어느 정도로 비슷하게
나오는지 보여준다. 중요한 것은 자식의 키가 부모의 키에 영향을 받지만, 부모의 키만큼 극단적이지 않고
평균에 가까워진다는 것이다. 이 현상을 회귀라고 부르며, 자식이 부모보다 평균에 가깝게 회귀한다는 점
에서 회귀 분석이라는 용어가 유래하였다.
```

### 🟣 선형회귀란 (Linear regression)
#### ◼️ 1. 가장 간단한 모델 중 하나
```
데이터 분석에서 가장 기본적이고 단순한 예측 모델, 두 변수 간의 직선 관계를 기반으로 미래 값을 예측하고 설명
```
#### ◼️ 2. 연속된 변수를 예측하기 위한 모델
```
연속형 변수를 예측하는데 사용됨. ex) 주택 가격, 실업률 예측, 공부 시간에 따른 성적 예측
연속된 값이란, 특정 범위 내에서 끊김없이 변할 수 있는 변수를 의미
```
#### ◼️ 3. 산업 및 과학 분야에 널리 응용
```
선형 회귀는 다양한 산업 및 과학 연구에서 사용됨
ex 경제 분야에서 주식 가격 예측, 과학 분야에서 실험 데이터 분석 등 여러 방면에 응용
```
#### ◼️ 4. 두 변수 간의 관계를 분석 
```
ex 공부시간과 성적의 관계를 분석할 때, 공부시간이 늘어남에 따라 성적이 어떻게 변화하는지 설명
```
#### ◼️ 5. 선형회귀의 유형
```
1. 단순선형 회귀 Simple Linear Regression
하나의 독립 변수와 하나의 종속 변수를 사용하는 가장 기본적인 형태

2. 다중 선형 회귀 Multiple Linear Regression
여러 개의 독립 변수를 사용해 종속 변수를 예측하는 방식
ex 성적을 예측할 대 공부시간 뿐만 아니라 수면 시간, 과외 시간 등 변수를 함께 고려할 수 있다

3. 다항 회귀 Polynomial Regression
종속 변수와 독립 변수 간의 관계가 직선이 아닌 곡선일 때 사용하는 회귀 기법
이 경우, 독립 변수에 제곱, 세제곱 등 다항식을 추가하여 모델링 
```
### ➕ 추가설명
```
선형 회귀의 기본개념은 y=mx+b라는 직선 방정식을 이용해 데이터를 모델링하는 것임
여기서 m은 기울기(독립 변수의 변화에 따른 종속 변수의 변화율), b는 절편(독립 변수가 0일 때
종속 변수의 값)임 주어진 데이터 포인트들과 이 직선 사이의 오차를 최소화하는 방식으로
최적의 선형 방정식을 찾는 것이 선형 회귀의 핵심 
```
### 🟣 선형 회귀의 모형과 특성
#### ◼️ 1. 선형적 관계 분석
```
여러 특성이 특성 값과 얼마나 선형적인 관계를 가지고 있는지 파악하는데 사용됨
이는 변수들 간의 직선적인 관계를 측정하며, 예측 모델을 만드는데 중요한 기초가 됨
```
#### ◼️ 2. 상관 관계 분석
```
선형 회귀에서 상관계수 Correlation coefficient 를 통해 두 변수 간의 선형적 관계를 측정 가능함
상관계수는 -1부터 1 사이 값을 가지며, 1에 가까울 수록 두 변수 간의 관계가 강한 양의 선형 관계
반면, -1에 가까울 수록 강한 음의 상관계를, 0에 가까울수록 상관이 거의 없음을 의미함
```
#### ◼️ 3. 지도학습 Supervised Learning의 일종
```
선형 회귀는 지도학습 방법 중 하나로, 훈련 데이터에 있는 입력과 출력 관계를 학습하여
새로운 데이터에 대해 예측할 수 있도록 함
```
#### ◼️ 4. 연속형 변수를 출력
```
선형 회귀는 연속적인 값을 예측하는데에 주로 사용됨 이는 분류 문제와는 달리 타겟 값이 연속적인 변수를 의미하며,
주식 가격, 온도 변화, 주택 가격 등의 예측에 활용된다
```
#### ◼️ 5. 수식
```
선형 회귀의 대표적인 수식은 y=ax+b로 나타낼 수 있다 여기서 a는 직선의 기울기, b는 절편
이 직선은 주어진 데이터의 경향을 가장 잘 설명하는 직선을 의미함
```
### ➕ 추가설명
```
선형 회귀는 데이터 간의 관계를 직선 형태로 모델링하기 때문에 직관적이고 해석하기 쉬움
실제 데이터는 항상 직선적인 관계를 가지는 것이 아니므로, 때에 따라서는 더 복잡한 선형 모델 필요
```
### 🟣 회귀 
```
회귀는 주어진 데이터를 2차원 공간에 나타내고, 그 데이터들을 가장 잘 설명하는 직선 또는 곡선 찾는 문제
이 과정에서 데이터 포인트들의 경향을 설명하는 최적의 직선(선형 회귀) 또는 곡선(비선형 회귀) 찾는 것이 목적
y=f(x)에서 출력인 y는 입력인 x에 대한 함수값으로, 둘 모두 실수일 때 이 함수는 f(x)를 예측하는 것이 회귀의 핵심
주어진 데이터로부터 입력 x에 따른 출력 y를 가장 잘 예측하는 함수 관계를 찾는 것
```
![image](https://github.com/user-attachments/assets/bad44ee5-6983-484f-b6b0-6084e2bb677e)
### 🟣 선형 회귀 VS 비선형 회귀
```
1. 선형 회귀
첫 번째와 두 번째 그래프는 선형 관계를 설명한다 데이터를 설명하는 직선(빨간색)이 그려져 있고,
데이터 포인트(파란색)들이 근접해있는 모습이다 데이터를 가장 잘 설명하는 선형 회귀 선이다
2. 비선형 회귀
세 번째 그래프는 비선형 관계를 설명하는 곡선 회귀이다 데이터가 직선보다 곡선으로 설명될 때,
비선형 회귀를 사용하여 데이터를 모델링한다 그래프에서 볼 수 있듯, 곡선에 더 잘 맞추어져 있
```
```
+ 데이터가 직선적으로 분포하지 않을 경우 비선형 회귀가 더 적합할 수 있다
+ 선형 회귀와 비선형 회귀 모두 데이터와 모델 간의 오차를 최소화하는 방식으로 학습되며,
회귀 모델은 데이터 분석, 예측, 통계 모델링에서 광범위하게 사용됨 
```
### 🟣 선형 회귀 예제
```
<< 예제 설명 >>
1. 훈련 데이터 => 키와 몸무게 데이터가 표로 주어짐
2. 학습 과정
주어진 데이터를 기반으로 선형 회귀 모델이 학습됨 => 모델은 주어진 데이터에서 키와 몸무게
간의 관계를 학습하여 새로운 키 값을 입력받았을 때 예측할 수 있도록 함
3. 모델 예측 => 학습된 관계를 이용해 몸무게 예측
```
### 🟣 단순 선형회귀
```
하나의 독립 변수 x와 하나의 종속 변수 y 간의 관계를 설명하는 모델
f(x)=wx+b w는 기울기, b는 절편을 의미함
독립 변수(키)가 변할 때, 종속 변수(몸무게)가 어떻게 변화하는지 설명함
```
### 🟣 다중 선형회귀
```
여러 독립 변수를 사용해 종속 변수를 예측하는 모델임
ex) 매출을 예측할 때 인터넷 광고, 신문 광고, TV 광고 등 여러 변수들을 고려할 수 있음
f(x,y,z)=w0+w1x+w2y+w3z
여기서 w0는 절편, w1,w2,w3는 각 변수에 대한 가중치(기울기)를 나타냄 
```
### 🟣 데이터 좌표를 기반으로 방정식 만들기
```
샘플좌표 (-1,0) (0,1) (0,3)
<<데이터에 대한 방정식>> => y=mx+b에 대입!!!
f(-1)=-m+b=0
f(0)=b=1
f(0)=b=3
```
### 🟣 행렬 표현
```
|-1 1 |  | m |   |  0 |
|0  1 |  | b | = |  1 |
|0  1 |          |  3 |
=> 행렬의 방정식을 풀면 최적의 기울이 m과 y절편인 b를 구할 수 있게 됨
```
### 🟣 최적화 경계선
```
각 데이터 포인트와 직선 간의 차이가 '오차'이며 이를 최소화하는 방향으로 선형 회귀 모델이 학습
오차(잔차)는 실제 데이터 값 - 예측된 값으로 계산되며, 이 오차들을 최소화하는 것이 선형 회귀의 목표
```
### 🟣 손실 함수와 비용함수
#### ◼️ 손실함수
```
모델이 예측한 값과 실제 값 간의 차이를 측정하는 함수
이 함수는 예측의 정확도를 평가하는 데 사용, 차이를 줄이기 위해 기울기와 절편을 최적화하는 방향으로 모델 학습
수식: e1 = 1/2(y'i-yi)^2
=> ei는 i번째 데이터 포인트에 대한 손실값, y'i는 예측값, yi는 실제값
=> 예측된 값 y'i와 실제값 yi 사이의 차이를 제곱하여 나타냄
```
#### ◼️ 비용함수
```
여러 개의 손실 함수 값들의 평균을 취한 것으로, 손실 함수가 개별 데이터 포인트에서의 오차를 나타내면,
비용함수는 전체 데이터에 대해 얼마나 잘 작동하는 지를 나타내는 지표
비용함수는 전체 데이터에 대한 손실 값을 합산한 후 평균을 취한 값, 최소화하는 것이 목
```
### 🟣 경사하강법 gradient descent method
```
손실함수 loss function 또는 비용함수 cost function의 값을 최소화하기 위해 사용하는 최적화 알고리즘
손실함수가 정의된 어떤 공간에서 그 함수의 값을 최소화하는 최적의 매개변수 W와 b를 찾는 것이 목적
```
### 🟣 경사하강법의 특징
```
1. 일반적인 방법
손실함수의 형태나 매개변수의 수와 관계없이 사용할 수 있는 일반적인 최적화 방법
회귀 모델뿐만 아니라 다양한 머신러닝 모델에서도 사용되는 핵심 알고리즘
2. 점진적인 학습
반복적인 학습 과정을 통해 매개변수의 값을 점진적으로 업데이트함
처음에는 임의의 값을 시작으로, 손실 함수의 기울기를 계산하여 매개변수를 조금씩 수정해감
3. 비용함수 최소화
비용함수를 최소화하는 값을 찾는데에 사용 이는 주어진 데이터에 가장 잘 맞는 회귀선을 찾기 위해
비용함수의 값을 점점 줄여나가는 과정
```
### 🟣 경사하강법의 원리
```
경사하강법은 비용함수를 최소화하기 위한 방법으로, 함수의 기울기를 이용해 최소값을 찾음
기울기가 존재하는 방향으로 매개변수를 조금씩 조정하여 손실함수의 최소점을 찾아간다 
```
#### ◼️ 음의 기울기
```
함수의 기울기가 음수인 경우, 매개변수는 양의 방향으로 이동해야 함
매개변수를 증가시키면서 더 작은 손실값을 찾아나감
```
#### ⬛ 양의 기울기
```
기울기가 양수인 경우, 매개변수는 음의 방향으로 이동해야 함
매개변수를 감소시켜 손실함수를 최소화하는 방향으로 이동함
```
#### ⬛ 이동방향
```
기울기를 기준으로 이동 방향을 설정
기울기가 양수일 때는, 매개변수를 왼쪽(음의 방향)으로 이동시켜야 손실 값을 줄일 수 있다
기울기가 음수일 때는, 매개변수를 오른쪽(양의 방향)으로 이동시켜야 손실 값을 줄일 수 있다
```
### 🟣 학습률(learning rate, 알파)의 중요성
```
학습률은 매개변수를 한 번 업데이트할 때 이동하는 크기를 결정하는 값
알파라고 불리며, 학습률이 최적화 과정에서 중요한 역할을 함
```

#### ⬛ 1. 학습률이 낮을 때
```
한 번에 이동하는 크기가 너무 작아져서 비용함수의 최소값에 도달하는 시간이 길어짐
-> 느리게 수렴하는 문제 발생, 이동하는 속도가 매우 느림
```
#### ⬛ 2. 학습률이 적절할 때
```
매번 적절한 크기로 이동하여 빠르고 안정적으로 최소값에 도달할 수 있다
-> 매번 비용함수의 최소값에 가까워지는 모습 
```
#### ⬛ 3. 학습률이 너무 높을 때
```
최소값을 지나치게 되어 오히려 손실값이 커짐 -> 진동, 발산 발생
-> 매번 최소값을 넘어서 오히려 값이 커지거나 발산하는 모습
```
![image](https://github.com/user-attachments/assets/94de6cf2-1af0-48f9-ba1a-679bcb0e9d62)
### 🟣 경사하강법의 초기점 Initial Point
#### ◼️ 경사하강법의 개념
```
비용함수 Cost Function L(w)를 최소화하기 위해 사용하는 알고리즘
비용함수는 매개변수 w에 따라 변화하며, 경사하강법은 이 매개변수 공간에서 가장 작은 값(최소점) 찾는 방식
```
#### ⬛ 초기점 Initial Point
```
경사 하강법은 먼저 임의의 초기점에서 출발해 기울기(경사)를 계산하여 매개변수 w를 업데이트하며 최소값으로 이동
```
#### 🔴 경사하강법 그래프
```
비용함수 L(w)가 U자 형태로 표현
매개변수 w에 따른 비용함수의 값을 나타내며, 초기점에서 출발해 경사방향을 따라 점차 비용이 줄어드는 과정
기울기 Gradient의 방향을 따라 L(w)를 최소화하는 방향으로 이동하며, 비용함수 값이 점차 감소
```
#### 🔴 상단 이미지
```
1. 랜덤하게 선택된 초기 시작점에서 출발
2. 편미분을 사용해 기울기(경사)를 계산, 각각의 축(변수)에 대해 편미분 계산해 이동할 방향 결정 
```
#### 🔴 하단 이미지
```
3. 기울기를 계산한 후, 비용함수의 값을 줄이는 방향으로 이동
4. 기울기 벡터를 따라 매개변수를 업데이트하면서 비용함수 값이 최소화하는 방향으로 이
```
### 🟣 경사하강법의 단계
```
1️⃣ 초기점에서 출발
2️⃣ 초기점에서의 기울기를 구함
3️⃣ '기울기(미분)'의 반대방향으로 움직임
4️⃣ 움직이는 정도는 학습계수(Learning Rate, 알파)통해 조절
```
#### ➕ 기울기를 구하고 반대방향으로 움직이는 이유
##### 🗒️ 기울기의 의미
```
- 기울기: 어떤 함수에서 x값이 아주 작게 변할 때 f(x)가 얼마나 변하는지 나타냄
  - 기울기가 양수이면, 해당 방향으로 함수의 값이 증가하고 있다는 의미
  - 기울기가 음수이면, 해당 방향으로 함수의 값이 감소하고 있다는 의미
```
```
기울기가 양수일 때, 함수가 증가하는 방향으로 움직이므로,
기울기의 반대 방향으로 가야 함수 값을 줄일 수 있다
=> 산을 올라가는 것과 같이 계속 올라가면 높이 y값이 점점 더 커짐

기울기가 음수일 때, 함수가 감소하는 방향으로 움직이므로,
이때는 기울기의 반대 방향으로 가면 함수 값을 더 크게 만들 수 있음
=> 산을 내려가는 것과 같이 계속 내려가면 높이 y값이 점점 더 낮아짐 
```
### 🟣 학습률
```
경사하강법에서 중요한 하이퍼파라미터로, 모델이 얼마나 빠르게 매개변수를 업데이트할지 결정
학습률을 적절하게 설정하는 것은 경사하강법의 성능에 큰 영향을 미침
학습률이 너무 크면 모델이 최적의 값에 도달하지 못하고 발산할 수 있고,
너무 작으면 최적화 과정이 너무 느리게 진행될 수 있
```
### 🟣 기본적인 학습률 설정 방법
```
- 작게 설정: 너무 크게 설정하면, 학습이 불안정
함수가 최적의 값을 지나쳐서 손실히 오히려 증가할 수도 있기 때문에, 작게 설정하는 것이 안전
- 크게 설정: 너무 작게 설정하면, 학습 과정이 너무 오래 걸림
아주 작은 변화만 일어나므로 학습 속도가 느려지기 때문에 적절한 크기를 찾아야 함
```
![image](https://github.com/user-attachments/assets/a48f4514-597f-4b29-9711-fde0d2a6413a)
### 🟣 지역최소값 & 전역최소값 
#### ◼️ 지역최소값 Local Minimum
```
지역최소값은 함수의 특정 구간에서 더 이상 작은 값이 없는 지점
but 이 값이 전체 함수에서 가장 작은 값은 아님, 그래프에서 빨간 점이 지역최소값을 나타냄 
이 점은 해당 함수의 구간에서 더 작은 값으로 이동할 수 없지만, 전역최소값(함수 전체에서 가장 작은 값)과는 다름
```
#### ◼️ 전역최소값 Global Minimum
```
전역최소값은 함수 전체에서 가장 작은 값
경사하강법의 목표는 이 전역 최소값에 도달하는 것이지만, 출발점에 따라 경사하강법이 전역 최소값에
도달하지 못하고 지역최소값에 멈출 수도 있음, 그래프에서 초록색 점이 전역 최소값 나타냄
=> 이 점이 함수 전체에서의 최소값
```
#### ◼️ 출발점
```
경사하강법은 초기 출발점에서 시작해 기울기 방향을 따라 함수의 최소값을 찾아감
그래프에서 출발점이 지역최소값 근처에 있을 경우, 기울기를 따라 이동하면 결국 지역 최소값에서 멈춤
전역최소값에 도달하려면, 출발점이 적절히 설정되어야 하며, 여러 번 시도하거나 다양한 기법 사용
```
#### ◼️오류 감소
```
그래프의 세로축은 일반적으로 손실함수의 값 또는 오류를 나타냄
```
![image](https://github.com/user-attachments/assets/a94b1d57-406b-41eb-8f1a-43184fed4661)
### 🟣 산점도와 등고선 그래프
#### 🔴 산점도 그래프 Scatter Plot
```
- 인구 수가 증가할수록 교통사고 발생 건수가 증가하는 경향, 두 변수 간 양의 상관관계
- 데이터는 어느정도 직선적인 패턴을 따르며, 선형 회귀 분석을 통해 이 관게를 모델링
```
#### 🔴 등고선 그래프 Contour Plot
```
- x축: slope, 선형 회귀에서 가중치(회귀계수)를 의미
- y축: intercept, 선형 회귀에서 y절편을 의미

이 그래프는 선형회귀에서 사용하는 비용함수의 값을 시각적으로 보여줌
비용함수는 회귀함수가 데이터에 얼마나 잘 맞는지 평가하는 함수로, 최소화하는 것이 목표

- 등고선: 비용함수의 값이며, 같은 값을 가지는 점들의 집합
등고선이 타원형으로 나타나는 것은 비용함수가 기울기와 절편에 대해 이차함수형태임을 의미
중앙에 가까울수록 비용함수 값이 작아짐

- 빨간색 별: 선형회귀에서 시작한 초기값, 비용함수를 최소화하는 방향으로 이동할 것임

- 목표: 비용함수의 최소값에 도달해, 데이터를 가장 잘 설명할 수 있는 회귀계수를 찾는 것
```
![image](https://github.com/user-attachments/assets/f511b267-7934-4a6e-937b-b9fb3bb6bd6a)
### 🟣 3D 비용함수 그래프와 2D 등고선 그래프
#### 🔴 3D 비용함수 그래프 
```
- 축: x-절편(intercept), y-기울기(slope), z축-비용함수의 값(손실함수의 값)

이 그래프는 비용함수의 값을 3차원으로 시각화한 것
기울기와 절편 값에 따라 비용함수가 어떻게 변하는지 나타냄, z축 값 최소화지점 찾는 것

- 산 모양의 그래프: 3차원 공간에서 볼록한 산 모양임
이 산의 가장 낮은 부분이 비용함수의 최소값, 최적의 기울기와 절편을 찾는 지점 = 전역최소값

- 빨간색 별: 경사하강법의 초기값,
초기값에서 출발해 기울기와 절편을 조정하면서 비용함수의 값을 최소화하는 방향으로 이동
```
#### 🔴 2D 등고선 그래프
```
- x축: 기울기 slope, y축: 절편 intercept

이 그래프는 비용함수의 값을 등고선 contour으로 표현한 것
각 선은 동일한 동일한 비용함수의 값을 나타내며, 중심에 가까워질수록 비용함수의 값이 작아짐

- 등고선의 모양: 타원형으로 나타나며, 비용함수가 볼록 형태의 함수임을 나타냄
중앙에 가까울수록 비용함수의 값이 작아져, 최적값에 가까워짐

- 빨간색 별: 경사하강법의 초기값, 중앙에 가까운 전역 최소값에 도달하게 됨 
```
![image](https://github.com/user-attachments/assets/49bbe3af-d9ad-4388-8828-49f6f3d26fd3)


