<div align="center">
  <h1>정책과 가치함수</h1>
</div>

```
- 기존 GridWorld 및 유틸
- Gridworld1 : 그리드 월드 기본 환경 및 유틸리티
- Gridworld1_2 : 학습 이후의 가치 함수 표로 그리기
- Gridworld1_3 : 학습 이후의 행동가치를 표로 그리기
- Gridworld1_4 : 학습 이후의 최적 정책을 표로 그리기
- GridWorldStateValue 각 상태별 상태 가치 산출
- GridWorldOptimalState 최적 상태 가치 산출
- GridWorldOptimalActionValueAndPolicy 최적 행동 가치 및 정책 산출
```
## 🟡 상태 가치의 벨만 방정식
```
1. 벨만 방정식
현재 상태에서 얻을 수 있는 기대 보상을 계산하는 재귀적인 수식
현재 상태에서 행동을 선택했을 때 얻을 수 있는 보상과 다음 상태에서 얻을 기대 가치를
더한 값으로 상태 가치 계산

2. 상태 전이 확률과 보상
상태 전이 확률은 현재 상태에서 특정 행동을 했을 대 다음 상태로 전이될 확률과 그에 따른 보상
이 확률과 보상을 모두 고려하여 상태 가치 계산

3. 정책 ㅠ
정책은 에이전트가 특정 상태 s에서 행동 a를 선택할 확률
에이전트가 주어진 상태에서 어떤 행동을 할지 결정하는 전략

4. 미래 보상 고려
방정식은 현재 얻는 즉시 보상 r뿐만 아니라, 감가율 r를 적용해 미래의 보상을 고려
이를 통해 장기적인 보상을 극대화하려는 것이 목표

5. 확률이 1인 경우
상태 전이 확률이 1인 경우 에이전트가 어떤 행동을 했을 때 다음 상태로 확실히 전이된다는
조건 하에서는 수식이 단순화, 이때는 보상과 다음 상태에서의 가치를 단순히 더해 계산 
```

<div align="center">
  <h1>최적 정책 및 가치 함수 실습</h1>
</div>

```
def calculate_grid_world_state_values(env):
    # 현재 가치 함수와 동일한 형태로 값을 0으로 초기화
    new_value_function = np.zeros_like(value_function)

    while True: # 가치함수의 값들이 수렴할 때까지 반복
        # value_function과 동일한 형태를 가지면서 값은 모두 0인 배열을 new_value_function에 저장
        # 새로운 가치 함수에 값 저장
        for i in range(GRID_HEIGHT):
            for j in range(GRID_WIDTH):
                # 상태 (i, j)에서 가능한 모든 행동에 대한 값을 저장하기 위한 리스트
                values = []
                
                # 주어진 상태에서 가능한 모든 행동의 결과로 다음 상태들을 계산
                for action in env.ACTIONS:
                    (next_i, next_j), reward, transition_prob = env.get_state_action_probability(
                        state=(i, j),
                        action=action
                    )

                    # Bellman 방정식 적용
                    values.append(transition_prob * (
                        reward + DISCOUNT_RATE * value_function[next_i, next_j]
                    ))

                # 다음 상태에 대한 모든 가치의 합을 새로운 가치 함수에 저장
                new_value_function[i, j] = np.sum(values)

        # 가치 함수의 수렴 여부 판단 (0.0001 == 1e-4)
        if np.sum(np.abs(value_function - new_value_function)) < 1e-4:
            break

        # 가치 함수 갱신
        value_function = new_value_function
```
```
- 이 함수는 그리드 월드 환경에서 모든 상태의 가치 계산하는 과정
- 벨만 방정식을 이용해 각 상태에서 가능한 행동에 따라 다음 상태의 가치를 계산하고
이를 현재 상태의 가치로 업데이트
- 함수는 상태 값의 변화를 추적하며, 각 상태에서 가능한 모든 행동에 대해 다음 상태와 보상을
계산한 후, 그 값을 벨만 방정식에 따라 합산
- 계산한 결과는 새로운 가치 함수에 저장하고, 일정 수준 이하의 변화만 있을 때 반복 멈춤
- 이 과정이 종료되면 모든 상태의 최적 가치가 계산된 상태 
```
### 결과 분석 >>
```
1. 양수 값
상태들이 비교적 높은 보상을 기대할 수 있는 상태

2. 음수 값
해당 상태들이 부정적인 보상, 혹은 손실을 기대할 수 있는 상태

3. 전체적인 패턴
왼쪽에서 오른쪽, 위에서 아래로 이동할수록 상태 가치가 감소하는 경향
환경에서 특정 방향으로 이동할 수록 보상이 줄어드는 것 의미, 상태별로 보상 달라짐 
```
![image](https://github.com/user-attachments/assets/5890af36-d15e-490a-b944-4ebb45c2e13d)

## 🟡 최적 상태 가치 함수 v(s)를 계산하는 벨만 최적 방정식
### 1. 최적 상태 가치 함수
```
- v(s)는 특정 상태 s에서 에이전트가 최적의 행동을 했을 때 얻을 수 있는 최대 기대 보상
- 즉, 에이전트가 어떤 상태에 있을 때, 가장 큰 보상을 주는 행동을 선택했을 때 그 상태의 가치
```
### 2. 벨만 최적 방정식
```
- 벨만 최적 방정식은 현재 상태 s에서 최적의 행동을 했을 때, 그에 다른 즉시 보상 r과
미래 상태에서의 가치 v(s')를 합한 값으로 상태 가치를 계
```
![image](https://github.com/user-attachments/assets/7fb95763-c7fe-4fb7-acac-c49cdeb0682c)
```
- max: 상태 s에서 가능한 모든 행동 a 중에서 최적의 행동 선택
- p(s'r|s,a): 현재 상태 s에서 행동 a를 했을 때, 다음 상태로 s'로 전이될 확률과 보상 r
- r+ ~~~~ : 즉시 보상 r과 미래 보상에서의 최적 상태 가치 v(s)를 할인율 감마 적용해 더한 값
```
### 3. 최적 행동 선택
```
- 상태 s에서 에이전트가 최적의 행동을 선택하는 것은 가능한 모든 행동들 중에서 가장 큰 보상을 주는 행동 고르는 것
- 즉 최대화 연산을 통해 에이전트는 현재 상태에서 가장 이익이 되는 행동 결정 
```
### 4. 상태 전이 확률
```
- 상태 전이확률 p(s',r|s,a)은 현재 상태에서 특정 행동을 했을 때 다음 상태로 전이될 확률과 그에 따른 보상
- 이 확률이 1인 경우, 즉 상태 전이가 확정적으로 일어난다면 방정식은 단순화 
```

## 🟡 다이어그램의 주요 요소 설명 
```
1. 상태 s
다이어그램의 최상단에 있는 s는 현재 상태, 에이전트는 이 상태에서 최적의 행동 선택

2. 최적 행동 선택 max
상태 s에서 가능한 여러 행동들 a중에서 최적의 행동을 선택하는 과정이 최대화 연산으로 표현
즉 에이전트는 여러 행동 중에서 어떤 행동이 가장 큰 기대 보상을 줄지 계산해 선택

3. 상태 전이확률 p(s',r|,s,a)
각 행동을 선택했을 때, 그 행동의 결과로 다음 상태 s'로 전이될 확률과 그에 따른 보상 r 나타냄
상태 s에서 행동 a를 했을 때, 다음 상태로 전이될 확률과 보상을 뜻함
여러 상태 전이가 가능할 수 있기 때문에 이 확률에 따라 계산

4. 즉시 보상 r과 할인된 미래 보상 v(s')
에이전트가 상태 s에서 행동 a를 했을 대 즉시 얻는 보상 r과 다음 상태 s'에서 얻을 수 있는
미래 보상 v(s')가 결합
이때 미래 보상은 할인율 감마가 적용되어 계산, 할인율 r은 미래 보상에 대한 중요도 결정

5. 최종 상태 가치 계산
상태 s에서 최적의 행동을 통해 얻게 되는 최종 상태 가치는 모든 가능한 행동에 대해
계산된 값들 중에서 가장 큰 값을 선택해 결정 
```
![image](https://github.com/user-attachments/assets/239bb72a-0fc7-44fa-a9f6-27bf6bfeecbc)

## 🟡 최적 가치 계산함수 실행결과 표 >>
```

```







