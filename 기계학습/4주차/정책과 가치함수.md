<div align="center">
  <h1>정책과 가치함수</h1>
</div>

```
- 기존 GridWorld 및 유틸
- Gridworld1 : 그리드 월드 기본 환경 및 유틸리티
- Gridworld1_2 : 학습 이후의 가치 함수 표로 그리기
- Gridworld1_3 : 학습 이후의 행동가치를 표로 그리기
- Gridworld1_4 : 학습 이후의 최적 정책을 표로 그리기
- GridWorldStateValue 각 상태별 상태 가치 산출
- GridWorldOptimalState 최적 상태 가치 산출
- GridWorldOptimalActionValueAndPolicy 최적 행동 가치 및 정책 산출
```
## 🟡 상태 가치의 벨만 방정식
```
1. 벨만 방정식
현재 상태에서 얻을 수 있는 기대 보상을 계산하는 재귀적인 수식
현재 상태에서 행동을 선택했을 때 얻을 수 있는 보상과 다음 상태에서 얻을 기대 가치를
더한 값으로 상태 가치 계산

2. 상태 전이 확률과 보상
상태 전이 확률은 현재 상태에서 특정 행동을 했을 대 다음 상태로 전이될 확률과 그에 따른 보상
이 확률과 보상을 모두 고려하여 상태 가치 계산

3. 정책 ㅠ
정책은 에이전트가 특정 상태 s에서 행동 a를 선택할 확률
에이전트가 주어진 상태에서 어떤 행동을 할지 결정하는 전략

4. 미래 보상 고려
방정식은 현재 얻는 즉시 보상 r뿐만 아니라, 감가율 r를 적용해 미래의 보상을 고려
이를 통해 장기적인 보상을 극대화하려는 것이 목표

5. 확률이 1인 경우
상태 전이 확률이 1인 경우 에이전트가 어떤 행동을 했을 때 다음 상태로 확실히 전이된다는
조건 하에서는 수식이 단순화, 이때는 보상과 다음 상태에서의 가치를 단순히 더해 계산 
```

<div align="center">
  <h1>최적 정책 및 가치 함수 실습</h1>
</div>

```
def calculate_grid_world_state_values(env):
    # 현재 가치 함수와 동일한 형태로 값을 0으로 초기화
    new_value_function = np.zeros_like(value_function)

    while True: # 가치함수의 값들이 수렴할 때까지 반복
        # value_function과 동일한 형태를 가지면서 값은 모두 0인 배열을 new_value_function에 저장
        # 새로운 가치 함수에 값 저장
        for i in range(GRID_HEIGHT):
            for j in range(GRID_WIDTH):
                # 상태 (i, j)에서 가능한 모든 행동에 대한 값을 저장하기 위한 리스트
                values = []
                
                # 주어진 상태에서 가능한 모든 행동의 결과로 다음 상태들을 계산
                for action in env.ACTIONS:
                    (next_i, next_j), reward, transition_prob = env.get_state_action_probability(
                        state=(i, j),
                        action=action
                    )

                    # Bellman 방정식 적용
                    values.append(transition_prob * (
                        reward + DISCOUNT_RATE * value_function[next_i, next_j]
                    ))

                # 다음 상태에 대한 모든 가치의 합을 새로운 가치 함수에 저장
                new_value_function[i, j] = np.sum(values)

        # 가치 함수의 수렴 여부 판단 (0.0001 == 1e-4)
        if np.sum(np.abs(value_function - new_value_function)) < 1e-4:
            break

        # 가치 함수 갱신
        value_function = new_value_function
```
```
- 이 함수는 그리드 월드 환경에서 모든 상태의 가치 계산하는 과정
- 벨만 방정식을 이용해 각 상태에서 가능한 행동에 따라 다음 상태의 가치를 계산하고
이를 현재 상태의 가치로 업데이트
- 함수는 상태 값의 변화를 추적하며, 각 상태에서 가능한 모든 행동에 대해 다음 상태와 보상을
계산한 후, 그 값을 벨만 방정식에 따라 합산
- 계산한 결과는 새로운 가치 함수에 저장하고, 일정 수준 이하의 변화만 있을 때 반복 멈춤
- 이 과정이 종료되면 모든 상태의 최적 가치가 계산된 상태 
```
### 결과 분석 >>
```
1. 양수 값
상태들이 비교적 높은 보상을 기대할 수 있는 상태

2. 음수 값
해당 상태들이 부정적인 보상, 혹은 손실을 기대할 수 있는 상태

3. 전체적인 패턴
왼쪽에서 오른쪽, 위에서 아래로 이동할수록 상태 가치가 감소한느 경향
환경에서 특정 방향으로 이동할 수록 보상이 줄어드는 것 의미, 상태별로 보상 달라짐 
```
![image](https://github.com/user-attachments/assets/5890af36-d15e-490a-b944-4ebb45c2e13d)

## 🟡 벨만 최적 방정식
### 주요 개념 
```
- 최적 상태 가치 함수: 상태 s에서 시작해 최선의 행동 정책을 따를 때 얻을 수 있는 최대 기대 보상
- 벨만 최적 방정식: 주어진 상태에서 최적의 행동을 선택하는 방식을 나타내는 수학적 식,
이 방정식을 통해 각 상태에서의 최적 가치를 계산 
```
### 벨만 최적 방정식의 구조
```
1. 상태-행동-보상 관계
상태 s에서 가능한 행동 a를 선택하면, 다음 상태 s'로 전이되고 그에 따른 보상 r을 얻게 됨
p(s',r|s,a): 현재 상태 s에서 행동 a를 했을 때, 상태 s'로 전이될 확률과 보상 r을 나타냄

2. 최적 가치 함수
- 상태 s에서 최적의 행동을 선택할 때 얻을 수 있는 최대 기대 보상

3. 계산 과정
- 상태 s에서 각 행동 a에 대해 상태 전이 확률과 보상 r을 모두 고려하여,
그 행동을 선택했을 때의 기대보상 계산
- 여러 행동 중 가장 큰 기대 보상을 주는 행동 선택 = 행동 a 중에서 보상이 최대화되는 값 찾음 
```
### 다이어그램 설명 >>
```
- 그림에서는 상태 s에서 여러 행동 a에 따라 다른 상태 s'로 전이되는 과정
- 각 행동에 대해, 상태 전이 확률에 따라 다른 보상 r을 받고,
다음 상태의 가치와 함께 최종 상태 가치 계산
- 이 모든 값 비교해 최적의 행동 a 선택

요약 >>
벨만 최적 방정식은 강화학습에서 상태 s에서 최적의 정책을 찾는데 중요한 역할
상태와 행동의 관계를 통해 최적의 가치를 계산하며, 이 과정을 반복해
각 상태에서 최선의 행동을 선택하는 방법 학습 
```


![image](https://github.com/user-attachments/assets/6f41398e-981f-4efd-af47-3fa74c6ea68f)











