# 강화학습의 기본 개념
## 강화학습 Reinforcement Learning
```
행동 심리학에서 시작하여, 보상을 극대화하는 행동을 학습하는 기계학습의 한 분야
```
## 인공지능을 위한 주요 기술
```
- 약 인공지능: 특정 작업에 특화된 기술
- 강 인공지능: 다양한 분야에 일반적으로 적용 가능한 기술
- 초 인공지능: 인간의 지능 초월하는 수준 
```

![image](https://github.com/user-attachments/assets/d7aba843-083f-467f-90bc-b195fea8986a)

## 심층 강화학습 Deep Reinforcement Learning
```
- 현재 환경의 상태 공간
상태 공간이 매우 크며, 특정 시간 자신의 상태에 대한 모든 환경을 경험하기 어렵다
모든 구간과 경우의 수를 돌아보는 것이 힘들며, 바둑의 경우 10^172 상태가 존재할 수도

- 에이전트의 행동 구간이 매우 클 때
주어진 문제를 해결하기 위해 여러 에이전트들이 협력하거나 경쟁할 수 있다
에이전트들은 협업을 통해 문제를 해결하거나 경쟁을 통해 더 나은 결과 도출 
```
## 동적 프로그래밍 Dynamic Programming
```
- 강화학습의 근간이 되는 알고리즘 1950: 동적 프로그래밍은 강화학습의 기초가 되는 알고리즘
리처드 벨만: 미국의 응용 수학자, 동적 프로그래밍의 창시자
```
## 동적 프로그래밍의 특징
```
- bottom-up 샹향식: 하위 문제의 해결을 기반으로 상위 문제 해결
- memorization 기억: 이미 계산한 결과를 저장하여 동일한 문제 다시 계산하지 않도록 하는 기법
- reuse 재사용: 계산된 결과 재사용하여 효율성을 높이는 방법 
```
## 동적 프로그래밍 경로
```
- start: 시작 지점, 문제 해결의 출발점
- goal: 목표 지점, 해결하고자 하는 문제의 최종 목표
- 노드(점): 각 원은 특정 상태이며, 이 상태에서 다음 상태로 전환
- 엣지(선): 노드 간의 연결, 숫자는 해당 경로로 이동하는데 드는 비용 또는 가중치
```
![image](https://github.com/user-attachments/assets/f626febe-05f5-46a1-8c44-8aa5d877b443)

## 시간차 학습 Q-Learning
```
- 강화학습의 근간이 되는 시간차 학습 방법 1980
: 리처드 서튼 >> 시간차 학습의 기초를 마련, 1981에 이 개념 발전
- Q-Learning 1989
: 크리스 왓킨 >> Q-Learning 알고리즘을 제안하여 강화학습의 가장 유명한 알고리즘 
```
## Q-Learning
```
에이전트가 환경에서 학습하면서 상태와 행동에 대한 가치를 추정하는 방법
=> 최적의 행동 선택, 오프라인 학습이 가능하여, 최적의 정책을 찾기 위해 사용
```

## 심층 강화학습의 시작
```
- 딥마인드의 DQN 2015
: 딥마인드에서 개발한 심층 강화학습 알고리즘인 DQN(Deep Q Network)
- 벽돌 게임
: DQN이 적용된 게임, 강화학습의 성능 보여줌
=> DQN이 바둑 게임과 같은 복잡한 문제에 적용되어 알파고가 발전 
```
# 강화학습의 기본 요소 >> 에이전트 / 환경 / 상태
## 에이전트 Agent
```
- 강화학습을 실행하는 주체: 강화학습 과정에서 행동을 결정, 실행
- 주어진 문제 상황에서 행동하는 주체: ex 로봇, 게임 캐릭터
```
## 환경 Environment
```
- 에이전트가 직접 상황을 작용하는 대상: 에이전트가 상호작용하는 외부 세계
- 에이전트의 행동을 입력으로 받아 처리: 에이전트의 행동에 대한 피드백으로 보상과 다음 상태 반환  
```
## 상태 state
```
- 환경으로 받은 관찰 정보, 행동, 보상 등으로 구성된 기록 정보: 에이전트가 처한 상황
- 에이전트가 환경에 받는 관찰 정보 그 자체: 환경의 상태를 에이전트가 관리하는 방식 
```
# 환경 Environment의 종류
## 1. 에피소딕 환경 Episodic Environment
```
- 상호작용이 특정 시점에서 종료되는 환경
- 에이전트가 시작 상태에서 시작해서 종료 상태까지 도달할 때까지의 과정이 에피소드
- ex) 게임이 특정 조건을 만족할 때가지 진행되고 끝남 
```
## 2. 지속적 환경 Continuing Environment
```
- 상호작용이 끝나지 않고 지속적으로 진행되는 환경
- 에이전트가 상태를 계속해서 업데이트
ex) 주식 가격 변동과 같은 연속적인 정보
```
# 에이전트의 행동에 따른 결과로 나타나는 환경의 종류
## 1. 결정적 환경 Deterministic Environment
```
- 에이전트의 행동이 결과를 항상 동일하게 만드는 환경
- 특정 상태에서 에이전트의 행동이 항상 동일한 결과(보상과 다음 상태) 가져옴
ex) 동일한 입력에 대해 항상 같은 결과가 나오므로 예측 가능 
```

## 2. 확률적 환경 
```
- 에이전트의 행동이 매번 다른 결과 가져오는 환경
- 특정 상태에서 에이전트의 행동이 결과에(보상과 다음 상태)에 대해 가변적
ex) 같은 행동을 하더라도 매번 다른 보상 
```
# 환경 상태와 관찰 정보
## 환경 상태 Environment State
```
- 환경에 내재하는 중요한 요소 도구, 장애물, 적 등
- 환경의 상태를 표현하는 모든 가능한 정보는 환경 상태를 이루는 정보
- 에이전트는 환경의 모든 세부 정보를 알 수 없을 수도,,
```
## 관찰 정보 Observation
```
- 에이전트가 환경과 상호작용하면서 얻게 되는 즉각적인 상태 표현
- 환경 상태 정보를 에이전트가 전체를 인식할 수 없기 때문에, 관찰 정보는 환경 상태 정보의 일부
- 매우 간단한 환경에서는 에이전트가 환경의 전체 상태 확인
- 환경 상태는 관찰 정보의 형태로 표현 
```
# 행동과 보상
## 행동 Action
```
- 에이전트가 환경에 전달하는 입력 정보
- 에이전트는 행동을 수행하여 환경과 상호작용
- 에이전트가 인식하는 상태에 따라 수행할 수 있는 행동 집합이 다를 수도 
```
## 보상 Reward
```
- 에이전트가 수행한 행동에 대해 환경이 에이전트에게 전달하는 값
- 보상은 에이전트의 정책을 변경하기 위한 정보
- 어떤 정책에 의해 선택된 행동을 수행한 후 받는 보상에 따라,
에이전트는 그 상태에서 다른 행동을 선택할 수 있는 기회 가짐 
```
# 강화학습의 목적
```
- 강화학습의 궁극적인 목적은 누적 보상의 기대값을 최대화
- 강화학습 에이전트는 추후에 받는 보상을 높일 수 있다면, 즉각적으로 받을 수 있는 보상 희생 
```
## 정책 policy 
```
- 에이전트가 주어진 상태에 대해 어떤 행동을 수행하는지 결정하는 기준
```
# 정책의 종류
## 1. 결정적 정책 Deterministic Policy
```
- 특정 상태에 대해 동일한 행동을 결정하는 정책
- ㅠ(s)=a 주어진 상태 s에서 행동 a를 결정 
```
## 2. 확률적 정책 Stochastic Policy
```
- 특정 상태에 대해 행동을 확률적으로 결정하는 정책
- ㅠ(a|s)=P(At=a|St=s) 상태 s에서 행동 a를 선택할 확률 
```
## 가치함수 Value Function
```
1. 상태 가치 함수 v(s)
에이전트가 위치할 수 있는 상태 s가 얼마나 좋은 상태인지?
상태 s의 가치는 에이전트가 그 상태에서 얻을 수 있는 기대 보상 반영

2. 행동 가치 함수 q(s,a)
상태 s에 대해 선택한 행동 a에 대해, 그 행동이 얼마나 유익한지?
특정 상태에서 특정 행동을 수행했을 때 얻는 기대 보상 기준

3. 가치 함수의 중요성
어떤 상태 s에서 즉각적인 보상이 좋지 않더라도, 다음 상태에서 큰 보상을 받을 수 있다면 가치 높음 
```
# 이득과 감가 이득
## 이득 Return
```
타임스텝 t 이후부터 종료 타임 스텝 T까지의 에이전트 보상을 Rt+1, Rt+2 ,,, RT
임의의 타임스텝 t에서 이득 Gt=Rt+1+Rt+2+Rt+...+RT
```
## 감가이득 Discounted Return
```
시간에 따라 보상의 중요도가 달라질 수 있음을 반영, 보상에 감가율 감마 적용해 계산
```
![image](https://github.com/user-attachments/assets/6d7d4151-9049-4627-a0a8-521ead6767b8)

```
감마는 0과 1 사이 값으로, 보상이 미래에 대한 영향을 얼마나 반영하는지 결정
보상의 합은 T까지 계속 감가되어 계산 
```
# 이득과 가치함수 Return and Value Function
## 상태 가치 함수 State Value Function
```
v(s): 에이전트가 임의의 타임스텝 t에 방문한 상태 s에서 임의의 정책 ㅠ에 따라
행동을 계속하였을 때 얻을 수 있는 기대 이득
특정 상태에서 에이전트가 받을 수 있는 보상의 기대값 
```
## 행동 가치 함수 Action Value Function or Q-Function

```
q(s,a): 에이전트가 임의의 타임 스텝 t에 방문한 상태 s에서 임의의 행동 a를 수행한 후,
임의의 정책 ㅠ에 따라 행동을 계속했을 때 얻을 수 있는 기대 이득
특정 상태에서 특정 행동을 취했을 때 기대 보상 
```
# 강화학습 수행 절차
```
1. 관찰 정보 수집 Observation
매 타임 스텝 t마다 에이전트는 환경으로부터 보상 Rt와 관찰 정보 Ot를 동시에 받고,
이 정보를 자신의 기억 저장소에 저장

2. 상태 저장 State
에이전트는 기억 저장소에서 현재 상태 St 구성

3. 행동 결정 Action
에이전트는 현재 상태 St와 정보를 활용해 행동 At를 선택하고 수행

4. 환경과 상호작용 Environment
선택한 행동 At 기반으로 환경과 상호작용해 다음 상태 St+1와 보상 Rt+1 받음

5. 반복
새로운 상태 St+1 구성 후, 이 과정을 반복해 에이전트가 최적의 정책 학습 
```
## 마르코프 속성 Markov Property
```
강화학습에서 임의의 타임 스텝 t에 대해 에이전트가 인식하는 상태 St는
다음 상태 St+1와 관련해 마르코프 속성이 있다고 가정
```
## 미래 상태와 현재 상태의 관계
```
- 미래 상태는 과거의 상태와는 관계없이 현재의 상태에만 의존
- 현재 상태 정보만으로 충분히 미래 상태를 결정할 수 있다는 점 중요 
```
# 강화학습의 특성
```
1. 기계학습과 차이
지도학습처럼 정답 데이터가 존재하지 않음, 보상 정보가 비슷한 역할 수행

2. 에이전트의 행동 결정
강화학습 에이전트가 행동을 결정하여 수행한 이후, 보상 정보 즉시 받지 못할 수도
=> 강화학습의 학습 난이도 상승

3. 타임 스텝 time step
강화학습은 타임 스텝에 따라 나눠진 일련의 상태, 행동, 보상 정보 처리
에이전트가 수행한 행동은 이후 에이전트가 받는 상태 정보에 영향 미침 
```
# 강화학습 알고리즘 종류
## 1. 가치 기반 알고리즘
```
가치 함수를 구성하여 활용하며, 정책은 명시적으로 존재하지 않음
가치함수를 통해 묵시적으로만 존재하고 활용하는 알고리즘
ex) DQN
```
## 2. 정책 기반 알고리즘 
```
별도의 가치 함수 구성하지 않음
정책 그 자체를 직접 구성하고 최적으로 발전시키는 알고리즘
ex) REINFORCE 
```
## 3. 액터-크리틱 actorp-critic 알고리즘
```
가치함수와 정책을 동시에 명시적으로 구성해 활용
최종적으로 정책을 최적으로 발전시키는 알고리즘
ex) A3C
```

# 마르코프 과정
## 확률 변수 X
```
무작위 실험 시 특정 확률로 발생하는 각 현상을 수치화
어떤 동작 결과에 대해 변수 X가 가질 수 있는 값과 그 확률이 정의
```
## 확률 변수의 예
```
동전을 두 번 던졌을 때 나올 수 있는 경우 표현
표본 공간 s={(앞,앞), (앞,뒤), (뒤,앞), (뒤,뒤)}
확률 분포 P(X=0)=1/4 P(X=1)=1/2 P(X=2)=1/4
```
## 확률 과정의 정의
```
시간에 따른 주어진 확률 변수의 변화 과정에 집중
확률 변수는 시간적 개념이 포함된 기호로 표현, Xt: 특정 시점 t의 확률 변수, t: 타임 스텝 
```
## 확률 과정 Stochastic Process
```
- 연속 동전 던지기 게임
매 타임 스텝마다 동전 하나를 던져서 앞면이 나오면 1, 뒷면 -1 부여하는 방식

- 초기 점수
시작 시 점수는 0점, P(X0=x)=1

- 타임 스텝 t에서의 확률 변수 Xt
타임 스텝 t에서 동전을 던진 뒤 얻는 누적 점수

- 확률 변수 공간
확률 변수는 상태 집합으로 구성 ex) {...,-3,-2,-1,0,1,2,3,...}
```
## 마르코프 과정
```
시간에 따라 주어진 환경의 상태 변화를 '상태 전이 확률'로 기술하는 확률 과정
- 임의의 타임 스텝 t에서의 상태는 St, 특정 상태 값 s,s' 가짐
- 상태 전이에 대해 마르코프 속성을 지니고 있음을 가정
- 임의의 타임 스텝 t에 대해 상태 St는 바로 직전의 상태 St-1에만 의존하여 결정
```
## 마르코프 과정의 예시 
```
- 상태 집합 S={start, class1, class2, class3, pass, pub, facebook, sleep}
종료 상태: sleep
```
## 마르코프 프로세스의 도식화
```
- 그래프에서 각 상태는 동그라미, 상태 간 전이를 화살표
- 화살표는 특정 상태에서 다른 상태로 이동할 확률
ex) start -> class1: 확률 1.0
```
## 상태 전이 행렬 Transition Matrix
```
상태 전이 행렬 P는 각 상태에서 다른 상태로의 전이 확률을 나타내는 테이블
ex) start에서 class1: 1.0, class1->pass: 0.6
각 행은 현재 상태, 각 열은 다음 상태
행렬의 각 원소는 특정 상태에서 다음 상태로 전이활 확률
ex) P[Class1][Pass]= 0.6, class1에서 pass로 이동할 확률이 60%
```
![image](https://github.com/user-attachments/assets/7daf3044-0c38-44c6-89ce-2b18e0c17dc1)

# 마르코프 과정에서의 에피소드
## 에피소드 Episode
```
- 환경과 에이전트 사이의 일련의 상호작용이 완료되는 단위
- 마르코프 과정이 주어지면 해당 과정으로부터 임의의 에피소드 다양하게 산출
```
![image](https://github.com/user-attachments/assets/4b238c53-0a6e-4731-b44c-a53c1c409282)

## 에피소드의 종류
```
1. 에피소딕 Episodic
종결 상태가 존재하며, 마지막 타임 스텝의 기호는 T로 표현
에이전트가 환경과 상호작용을 통해 일정한 목표를 달성한 후 종료되는 일련의 과정 포함

2. 지속적 Continuing
종결 상태 존재하지 않는 경우
에이전트가 계속해서 환경과 상호작용하여 학습을 진행하는 형태 
```
## 마르코프 보상 과정 Markov Reward Process
```
- 기존의 마르코프 보상 과정에 보상 요소 추가해,
시간에 따른 환경의 상태 변화에 따라 보상을 얻어낼 수 있는 확률 과정
임의의 타임 스텝 t에서의 보상은 Rt로 표현, 이는 특정 보상 값 r 나타냄
```
## 구성 요소
```
1. 상태 전이 확률
p(s'|s): 주어진 상태 s에서 다음 상태 s'로 전이할 확률

2. 보상함수
r(s'): 특정 상태 s'에서 받을 수 있는 보상의 기대값

3. 초기 상태 확률: 초기 상태가 s일 확률 
```
## 마르코프 과정의 예시 Markov process
```
상태 및 전이 >>
1. 상태 집합
Start, Class1, Class2, Class3, Pass, Pub, Facebook, Sleep으로 구성

2. 전이 확률
화살표는 현재 상태에서 다음 상태로의 전이 확률 나타냄
ex) start에서 class1로 이동할 확률은 0.5, facebook 상태에서 class1로 이동할 확률은 0.9

3. 보상
각 상태에서 받는 보상은 붉은 글씨
facebook에서 -1.0 보상, class1에서 -2.0 보상, pass에서 10.0의 보상,,,
```
```
- 에이전트는 주어진 상태에서 다음 상태로 전이할 때 각 전이에 대한 확률과 보상 고려하여 최적의 행동 선택
- 마르코프 과정은 특정한 환경에서의 에이전트의 행동과 그 결과를 모델링하는데 사용
```
![image](https://github.com/user-attachments/assets/70a9382d-63a1-44b0-94d2-bb4eb09e0388)

# 이득 Return과 감가이득 Discounted Return
## 이득 Return 
```
타임스텝 t 이후부터 종료 타임 스텝 T까지 에이전트가 받는 보상들을
Rt+1, Rt+2, Rt+3,,, RT라 할 때, 임의의 타임 스텝 t에서의 이득 Gt는 아래 식으로 정의
```
![image](https://github.com/user-attachments/assets/a07a84a3-6cab-47c9-9c20-9be1283164ca)

## 감가이득 Discounted Return
```
타임스텝 t 이후부터 종료 타임 스텝 T까지 에이전트가 받는 보상들을
Rt+1, Rt+2, Rt+3,,, RT라 할 때, 임의의 타임 스텝 t에서의 감가 이득 Gt는 아래 식으로 정의
여기서 감마는 감가율로, 0과 1 사이의 값 가짐 
```
![image](https://github.com/user-attachments/assets/1d91391c-9fc6-447e-b30f-5cdd37141145)

## 감가율 Discounting Rate, 감마
```
- 0<=r<=1의 값 가지고, 미래의 가치를 현재 시점에서 가치로 환산하는 비율
- 감가율이 높을수록 미래의 불확실성이 낮고, 낮을수록 불확실성 높아짐
왜???????????????????????????
```
## 지속적 작업 기반 에피소드에서 감가율에 따른 감가이득 
```
1. 감가율 r=0
Gt=Rt+1 => 현재의 보상만 고려

2. 감가율 r=1
Gt=Rt+1+Rt+2+Rt+3+,,,,, => 모든 보상 합산하여 고려

3. 적당한 감가율 r
Gt=Rt+1+rRt+2+r^2Rt+3+,,, => 각 보상에 대해 감가율 적용해 시간에 따라 중요성 감소
```

## 마르코프 결정 과정 Markov Decision Process, MDP
```
마르코프 보상 과정에서 상태 전이가 이벤트를 유발하는 요소로 행동을 추가한 것
```
## 구성 요소
```
1. 행동
임의의 타임 스텝 t에서의 행동은 At로 나타나며, 이는 특정 보상 값 r 가짐

2. 보상과 상태
임의의 타임 스텝 t에서의 보상은 Rt로 나타나며, 상태는 St
이전 상태 St-1 및 이전 행동 At-1에만 의존

3. 확률 과정
임의의 확률 과정을 따르는 확률 변수로 구성
```

## 마르코프 결정 과정 MDP
```
- 환경의 동적 특성을 결정짓는 확률
마르코프 속성에 따라 보상 Rt와 상태 St를 결정하는 요소 이전 상태 St-1과 이전 행동 At-1
```
# 강화학습의 목적과 보상의 역할
## 강화학습 목적
```
- 문제를 MDP로 표현
문제를 마르코프 결정 과정 MDP로 모델링하여 각 타임스텝마다 받는 보상의 누적 합을 최대화하는 것이 목표
- 보상의 누적합
단기적인 보상 뿐만 아니라 미래 보상의 누적합까지 고려 
```
## 보상 값의 역할
```
- 강화학습 목표를 달성하기 위해 보상 값은 중요한 역할
- 문제의 목표를 달성하는 방법을 알려주는 것이 아니라, 문제의 목표가 무엇인지 알려줌
```
```
ex) 체스 게임
상대방의 말을 잡아먹을 때마다 긍정적 보상
잘못된 경기를 이기는 전력이 아닌, 단순히 상대 말 잡아먹는 데에 관심 
```
## 보상의 활용 ex
```
1. 미로에 놓여진 로봇
로봇이 출구를 찾아서 빠져나와야 할 때, 에이전트는 매 타임 스텝마다 -1의 보상 받음
=> 빠른 시간, 작은 타임 스텝 내에 출구를 찾도록 해야 함

2. 재활용 수거 이동 로봇
빈 깡통을 수거할 때 +1의 보상 => 더 많은 깡통 수거하도록 유도

3. 자율 주행 자동차
장애물과 부딪히면 음의 보상 => 장애물을 피하도록 유도

4. 대결 게임
경기를 이기면 양의 보상 => 전략으로 학습하는데 도움 
```
## 에이전트 정책
```
1. 결정적 정책
임의의 상태에서 정책의 출력이 정확히 하나로 결정

2. 확률적 정책
임의의 상태에서 정책의 출력인 행동이 확률로 정해짐 
```
## 가치함수의 정의와 종류
```
1. 가치
주어진 환경에서 에이전트가 얻어낼 수 있는 보상과 연관된 중요도

2. 가치함수
가치나 상태나 상태, 행동 쌍에 대해 결정되는 함수

3. 강화학습에서 가치함수의 역할
대부분의 강화학습 알고리즘은 가치함수 기반으로 설계
가치함수는 에이전트의 행동을 결정하는 정책을 설게하는 기반으로 활용
```
```
1. 상태가치함수
에이전트가 위치할 수 있는 임의의 상태 그 자체가 얼마나 좋은 상태인지 나타내는 함수

2. 행동가치함수
임의의 상태에서 선택하여 행동이 얼마나 유익한지 나타내는 함수
```
## 상태 가치 함수
```
임의의 타임 스텝 t에 방문한 임의의 상태 s에서 에이전트가 정책 ㅠ에 따라
행동을 계속할 때 얻을 수 있는 기대 이득

그리드월드에서의 상태 가치 >>
그리드의 각 칸은 특정 상태의 가치 값을 나타내며, 해당 상태에서 시작하여
정책을 따라갔을 때 기대되는 누적 부상 의
```
![image](https://github.com/user-attachments/assets/5152c30a-d5a6-4314-a040-80029e3b6725)

## 행동 가치 함수
```
임의의 타임 스텝 t에 상태 s에 도달한 후, 특정 행동 a를 선택하고 이후에
정책 ㅠ를 계속 따를 때 얻을 수 있는 기대 이득
각 상태 및 행동 쌍에 대하여 주어진 정책을 따라 행동을 지속할 때 기대되는 이득값 

행동 가치 함수의 역할 >>
특정 상태에서 선택한 행동이 얼마나 좋은지를 알려줌
상태에서 가능한 모든 행동에 대한 행동 가치 함수를 계산할 수 있다면,
에이전트는 해당 상태에서 가장 높은 가치의 행동을 선택함으로써 최적의 행동 결정

단일 값 표>> 특정 상태에서 기대되는 이득 값
상태와 행동이 표시된 그리드 형태 >>
위 아래 왼 오른쪽 이동할 때 각각의 방향에 대한 기대되는 이득 값 표시 
```
## 벨만 방정식
```
현재 상태에서의 가치는 지금 상태에서 얻을 수 있는 즉각적인 보상과
이후 미래 상태에서의 예상 가치 합으로 이루어짐
에이전트가 특정 상태에 있을 때 앞으로 얻을 수 있는 장기적인 가치를
현재 보상과 미래의 예상 가치를 통해 정의하는 방식

벨만 방정식을 사용하는 이유 >>
벨만 방정식은 강화학습에서 각 상태의 가치를 점진적으로 계산하는데 도움
이를 통해 에이전트는 최적의 행동 수행 가능
현재 상테에서 미래의 가치 고려해 최고의 결정 내림
```
## 최적 정책 및 최적 가치 함수
```
1. 정책의 대소 비교
임의의 정책 ㅠ'가 다른 정책 ㅠ보다 좋은 정책이라는 것은
모든 상태 s에 대해 ㅠ'에 따른 기대 누적 보상(상태가치)가 ㅠ에 따른 기대누적보상보다 크거나 같음 의미
정책 ㅠ'가 정책 ㅠ보다 모든 상태에서 더 높은 가치 제공할 시, 더 좋은 정책으로 간주

2. 최적 정책
최적 정책이란 모든 가능한 정책 중에서 각 상태에서의 기대 보상이 최대가 되는 정책
최적 정책을 따르면 에이전트는 모든 상태에서 가장 높은 보상 얻음
```
### 최적 정책
```
1. 정의
주어진 마르코프 결정 과정에서 어떤 정책보다 더 좋은 최적 정책이 존재할 수 있음
최적 정책 ㅠ*는 모든 정책 ㅠ에 대해 각 상태에서 기대 누적 보상(상태가치)가 가장 큼

2. 최적 정책의 특성
각 상태에서 가장 큰 기대 보상을 얻을 수 있는 정책이기 때문에,
최적 정책을 따르면 에이전트는 장기적으로 최대의 보상을 기대
하나의 MDP에 대해 최소한 하나 이상의 최적 정책이 존재할 수 있음
=> 상황에 따라 여러 가지 최적의 행동 선택 방법이 존재할 수도
```
## MDP 마르코프 결정 과정 문제의 해결
```
1. 최적 행동 가치와 최적 정책
에이전트가 각 상태에서의 최적 행동 가치를 알 수 있다면, 최적 정책 이끌어냄
최적 정책이란 에이전트가 각 상태에서 최선의 행동을 선택하여 장기적으로 최대의 보상 얻는 정책

2. MDP 문제의 해결
최적 행동 가치를 통해 최적 정책을 찾았다면, 이는 MDP 문제를 해결한 것
MDP의 목표는 최적 정책을 찾아 에이전트가 모든 상태에서 최선의 선택을 하도록 만드는 것
```

## 최적 정책 계산의 여러움
```
1. 벨만 최적 방정식의 가정
최적 정책을 계산하기 위해서는 몇 가지 가정 필요

2. 필요한 가정
환경의 동적 특성: 각 상태에서 다음 상태로 전이될 확률 p알고 있어야 함
보상 값의 정확성: 각 상태 전이마다 얻을 수 있는 보상 값 정확히 알아야 함
마르코프 특성: 문제는 마르코프 성질(현재 상태가 미래를 결정하는데 충분한 정보 제공)
충분한 자원: 최적 해를 계산하기 위해 충분한 계산 성능과 메모리 자원

=> 위와 같은 가정이 성립하지 않거나 부족하면, 벨만 방정식을 통한 최적 정책 계산이 어려워질 수도 
```
## 최적 정책 계산의 어려움과 Q 테이블
```
1. Q테이블 기반의 최적 정책 계산
벨만 최적 방정식을 테이블 형태로 구현해, 각 상태와 행동에 대한 테이블에 저장
상태의 개수가 적은 문제는 상태와 행동을 행과 열로 나열에 그 값을 저장

2. Q테이블
Q테이블은 각 상태에서 특정 행동을 선택했을 때 기대보상=가치 저장하는 테이블
이 테이블 기반으로 에이전트는 가장 높은 가치를 가지는 행동 선택해 최적의 경로 찾음

3. 장점과 한계
장점: 상태와 행동의 개수가 적은 경우, Q테이블 사용해 각 상태-행동 쌍에 대한 가치 저장하고 참조 가능
한계: 개수가 만항지면 테이블이 커져 저장과 계산이 어려워지며, 자원필요 
```
## 테이블 기반 최적 정책 계산의 어려움
```
1. 어려움
모든 상태 s에 행동 a에 대해 기대보상을 구하기 위해서는 완전하고 포괄적인 탐색 계산 과정 필요
벨만 최적 방정식을 사용해 해를 산출하려면 계산 복잡도와 공간 복잡도가 매우 높아질 수 있음

2. 상태 수가 많을 때 문제점
상태의 수가 많아지면 모든 상태-행동 쌍에 대해 계산을 수행하는 것이 현실적으로 불가능
=> 최적 정책 구하는데 상당한 자원과 시간

3. 해결책 -틸링 tiling
틸링은 상태 공간을 작은 타일, 부분 영역으로 나누어 각 타일에 대해 학습하는 방식
전체 상태 공간을 복잡도를 줄이는데 도움 
```
## 상태 가치 계산 결과 

![image](https://github.com/user-attachments/assets/780e1c94-012f-4e54-a334-3a10bafd0c57)

```
표의 각 위치는 그리드 내 특정 상태, 값은 그 상태의 가치
(1,2) 위치의 값이 8.79로 가장 높아, 이 상태가 다른 상태들보다 높은 가치를 가지고 있음
상단의 값들이 하단의 값보다 높은 경향, 그리드의 상단에서 더 높은 보상 기대할 수 있음 
```
## 최적 가치 계산 결과 
![image](https://github.com/user-attachments/assets/35eae35b-2041-477b-b1d5-4d03a8b0daa9)
```
각 위치의 값은 해당 상태에서 시작했을 대 최적 경로를 따라 얻을 수 있는 기대 보상
(1,2) 위치의 값이 24.42로 가장 높아, 이 위치가 상대적으로 유리한 상태임을 나타남
상단에서 하단으로 내려갈 수록 가치가 점차 낮아짐, 에이전트가 상단에서 이동하는 것이 더 높은 보상
이 표는 최적 정책을 따랐을 때 각 상태의 기대 가치 제공하여, 에이전트가 최대 보상 얻기 위해 도움 
```
## 행동 가치 계 결과 
![image](https://github.com/user-attachments/assets/235dd07f-3dd9-4c22-9170-5982569b03dd)
```
1. 표 구성
각 셀은 하나의 상태, 화살표와 함께 네 가지 방향의 행동 가치가 주어짐
각 행동에 대해 가치는 해당 방향으로 이동했을 때의 기대 보상

2. 행동 가치 해석
각 상태에서 가장 높은 가치를 가지는 방향이 최적의 행동으로 간주
에이전트는 각 상태에서 최적의 방향을 선택해 이동
```
## 결정적 정책 구현 결과'
![image](https://github.com/user-attachments/assets/281ad0d1-afb2-460f-9d0e-03834a70a6e3)

```
각 셀의 화살표는 해당 상태에서 에이전트가 따라야 할 최적이 방향
숫자는 각 방향에 대해 정책의 우선 순위 또는 횟수, 최적 정책을 내기 위한 정보

결정적 정책 의미 >>
특정 상태에서 항상 동일한 행동을 선택하도록 함
에이전트는 주어진 상태에 도달할 때 표시된 방향으로만 이동
```
## 몬테카를로 방법
```
1. 몬테카를로 방법
특정 값을 알아내기 위해 반복적인 시행과 관찰을 통해 추정하는 방법
여러 번의 시행을 통해 결과를 관찰하여 확률적으로 값을 근사해 나감

2. 적용 방식
반복적인 시행을 통해 얻은 평균 등을 활용해 원하는 값 점진적으로 근사
이러한 접근은 정확한 계산이 어려운 경우, 확률적 특성 이용해 근사적인 답 구함
```

## 몬테카를로 기반으로 한 강화학습
```
1. 몬테카를로 방법을 통한 강화학습
가치함수와 최적 정책을 근사적으로 추정
환경에 대한 완벽한 지식이 없어도 작동하므로, 확률적 특성에 대한 정확한 정보 알기 어려운 상황에서 유용

2. 환경과의 상호작용 및 경험 샘플
환경과의 상호작용을 통해 얻은 경험 샘플을 사용해 가치 계산, 이를 최적 정책 갱신

3. 적용 제한 사항
종료상태!!가 있는 환경에서만 적용 가능, 모든 시행이 종료되는 환경이어야 함
에피소드가 끝난 후에만 정책을 개선할 수 있으며, 타임 스텝 사이에서는 정책 개선 불가 
```
## 몬테카를로 방법을 통한 가치함수와 정책 학습 과정
```
1. 가치함수와 정책의 상호작용
가치함수와 정책이 상호작용하면서 최적의 점점에 도달하도록 점진적인 개선
정책 반복 일반 방법과 몬테카를로 방법은 이러한 상호작용 통해 최적화해 나감

2. 환경에 대한 완벽한 지식의 필요성
기존 방식과 달리 몬테카를로는 환경에 대한 환경한 지식 없이도 가치 학습하고 정책 개선
환경과의 상호작용 통해 얻은 샘플 보상 사용해 점진적으로 가치함수와 정책 학습
```
## 몬테카를로 예측
```
1. 목적
임의의 정책 ㅠ에 다라 생성된 여러 에피소드를 통해 특정 상태 s에 대한 상태가치나 행동가치 추정
=> 이를 토해 주어진 정책 하에서 상태-행동 쌍의 장기적인 기대 보상 예측

2. 추정 방법
에피소드에서 특정 상태나 행동이 여러 번 반복되면, 이들의 경험적 평균 계산해 가치함수 추정
여러 에피소드에서 반복적으로 방문된 상태나 행동의 보상을 평균화하여 가능한 한 정확하게 가치 값 추정
```

## 몬테카를로 이용한 상태 가치 계산
```
1. 기대 평균
동적 프로그래밍 방법에서의 상태 가치 계산 방식, 미래 보상과 상태 가치의 가중 평균 이용해 현재 상태의 가치 계산
이를 통해 상태 s에서 앞으로 얻을 수 있는 기대 값 예측
하지만 이 방법은 환경의 전이확률과 보상에 대한 정확한 정보 알아야 함

2. 경험적 평균
상태 s를 여러 번 반복하면서 실제로 얻은 보상의 평균을 사용해 상태 가치 추정
상태 s를 방문할 때마다 얻는 보상(감가이득)을 누적해 방문 횟수로 나누어 평균을 계산
-> 경험적 평균을 통해 상태 s의 장기적인 가치 추정
```
## 몬테카를로 예측의 방법
```
1. 모든 방문 몬테카를로 예측
상태를 방문할 때마다 해당 상태에서 얻은 보상을 기록하여 평균을 계산하는 방식
하나의 에피소드에서 상태 s를 여러 번 방문하면, 각 방문 시점의 보상을 모두 반영하여 상태 가치를 추정

2. 첫 방문 몬테카를로 예측
하나의 에피소드에서 특정 상태를 처음 방문했을 때의 보상만을 기록하여 상태 가치 추정
중복된 방문이 있는 경우에도 첫 번재 방문 시점의 보상 사용, 계산 단순 
```
## 모든 방문 몬테카를로 예측 방법 사용해 상태 가치 추정
```
1. 모든 방문 몬테카를로 예측
동일한 에피소드 내에서 특정 상태 s를 여러 번 방문할 때
매번 방문 시 이후에 얻은 보상의 합을 기록
모든 방문에서 얻은 보상의 평균을 계산하여 상태 가치 추정

N: 상태 s를 방문한 횟수
Gi: i번째 상태 s의 방문이후 계산된 감가이득(보상의 합)
상태 가치는 모든 방문에서의 보상의 평균으로 계산
```

## 첫 방문 몬테카를로 예측 방법 사용해 상태 가치 추정
```
1. 첫 방문 몬테카를로 예측
동일한 에피소드 내에서 상태 s를 처음 방문했을 때만 이후의 보상을 기록해,
이를 기반으로 상태 가치 추정
이후 동일한 상태 방문에서는 보상을 계산에 포함하지 않고 첫 방문만 반영

N: 상태 s를 첫 번째로 방문한 총 횟수
Gi: i번째 에피소드에서 상태 s를 첫 방문 후 얻은 감가이득
상태 가치는 첫 방문에서의 보상의 평균으로 계산
```
## 탐험적 시작을 통한 몬테카를로 예측
```
1. 탐험적 시작
상태 s의 수가 많을 경우 모든 에피소드에서 모든 상태를 방문하기 어려움
이로 인해 특정 상태에서 얻을 수 있는 보상값을 알기 어려워지고, 최적 행동을 학습하기 힘듬
탐험적 시작은 문제 해결을 위해 에피소드가 시작할 때 초기 상태를 임의로 선택해 다양한 상태 탐험하도록 하는 방법

2. 필요성
에피소드가 항상 동일한 상태에서 시작된다면,
일부 상태는 방문되지 않을 수도 있어 해당 상태 보상과 최적 행동 학습하지 못할 수도
탐험적 시작을 통해 에피소드가 다양한 상태에서 시작되게 하여 학습과정 다양화
전체 상태 공간을 보다 포괄적으로 탐험 
```
## 몬테카를로 예측 방법의 개념과 역 갱신 다이어그램
```
1. 역갱신 다이어그램
종료 상태에서부터 역방향으로 상태 가치를 갱신하는 과정
에피소드가 종료상태에 도달하면, 해당 상태부터 역방향으로 이동해 각 상태에서 받은 보상을 감가율 고려해 누적
다이어그램의 각 계층은 다양한 에피소드에서 생성된 상태방문,
여러 에피소드에서 얻은 보상을 평균하여 해당 상태 가치 추정

2. 몬테카를로 예측 특징
이전 값을 기반으로 한 예측(부트스트랩)을 사용하지 않고,
에피소드가 끝난 후 실제 보상을 기반으로 가치 추정
각 상태에서의 가치 추정은 다른 상태와 독립적, 상태 방문 시점에 얻은 보상을 기준으로 누적해 평균값 계
```
## 몬테카를로 예측에서 행동 가치 예측의 필요성
```
1. 행동 가치 예측의 필요성
환경에 대한 지식이 부족하면 상태 가치만으로는 최적 행동 선택하기 어려움
상태-행동 쌍의 가치를 나타내는 행동 가치 필요, 특정 상태에서 어떤 행동을 취해야 하는지 알게 됨

2. 행동 가치와 Q테이블
행동 가치는 상태와 행동의 쌍으로 구성, Q테이블에 저장해 각 상태에서 가능한 행동 가치 확인
상태에 따른 가능한 행동에 따라 예상되는 보상 표시, 최적 행동 선택

3. 최적 행동 선택
Q테이블을 통해 각 상태에서 가능한 행동들의 가치를 비교하여,
가장 높은 가치를 가지는 행동을 선택함으로써 최적의 행동 취함
```
## 몬테카를로 기반 행동 가치 예측
```
1. 목적
각 상태 s와 행동 a쌍에 대해 예상되는 보상을 추정하기 위해 행동 가치 예측
에이전트는 특정 상태에서 최적의 행동 선택할 수 있도록 지원

2. 행동 가치 예측 과정
모든 상태와 해당 상태에서 가능한 모든 행동에 대해 방문횟수와 그에 다른 누적보상 기록
여러 번 방문한 결과를 평균 내 상태-행동 쌍의 가치 추정 
```
## 몬테카를로 예측을 통한 행동 가치 예측의 두가지 방법
### 모든 방문 행동 가치, 첫 방문 행동 가치
```
1. 모든 방문 행동 가치
동일한 에피소드 내에서 상태-행동 쌍 매번 방문할 때마다,
이후 얻는 누적 보상 기록해 상태-행동 가치를 추정
각 방문 시점에서 누적된 보상 평균을 사용해 행동 가치 계산

2. 첫 방문 행동 가치 예측
동일한 에피소드 내에서 처음 방문했을 때만 이후의 보상을 기록해 행동 가치 추정
첫 방문 이후의 방문은 무시, 첫 방문 시 얻은 보상의 평균만을 활용해 계
```
## 몬테카를로 제어
```
1. 개념
반복 수행을 통해 무한 번의 에피소드 실행, 행동 가치 함수 계산할 수 있다면 최적 정책에 가까운 결과 얻음
실제로 무한 번의 에피소드 실행하는 것 불가능, 일정 횟수의 에피소드로 경험 축적

2. 에피소드의 반복과 정책 개선
매 반복마다 경험을 축적하여 가치 함수를 조금씩 더 정확하게 계산
새롭게 계산된 가치함수 바탕으로 정책 개선해, 이 과정 반복 수행에 최적에 가까운 정책 얻음
완벽한 최적 정책은 아니더라도 충분히 근사적인 최적 정책 
```
## 시간차 학습 TD Learning
```
1. 시간차 학습
에피소드가 끝나기를 기다리지 않고, 타임 스텝마다 가치 함수 및 정책을 업데이트
에이전트는 상태와 행동에 대한 가치함수 정책을 실시간으로 업데이트

2. 몬테카를로 방법과의 유사성
시간차 학습은 환경에 대한 완전한 모델 없이도 학습 가능, 실제 경험 데이터 통해 학습
에이전트가 환경에서 얻은 샘플 데이터 통해 정책 학습한다는 점에서 유사

3. 동적 프로그래밍과의 유사성
시간차학습은 동적프로그래밍처럼 부트스트랩(기존의 가치 추정치를 사용해 새로운 가치 추정치 업데이트)
타임 스텝별로 가치 함수 업데이트 하기 때문에, 다른 상태의 가치 정보 활용해 정책 조정

4. 시간차학습 TD, 몬테카를로 MC, 동적프로그래밍 DP
```
## 시간차 학습 VS 몬테카를로
```
1. 몬테카를로
에피소드가 끝난 후, 특정 상태에서 얻은 실제 누적 보상을 기반으로 상태 가치 계산
여러 에피소드에서 상태를 방문할 때 얻은 보상의 평균 계산
에피소드가 끝나야 가치 함수 업데이트, 실시간 갱신 어려움

2. 시간차학습
현재 상태에서 다음 상태로의 보상과 다음 상태 가치의 합을 사용해 즉시 상태 가치 업데이트
매 타임 스텝마다 가치 함수 업데이트
실시간 갱신되므로 에피소드가 끝나기를 기다리지 않아도 됨

3. 차이점
MC는 에피소드가 끝난 후 보상의 평균을 이용해 상태 가치 갱신,
TD는 다음 상태의 가치와 즉각적인 보상을 활용해 매 스텝마다 상태 가치 갱신
TD는 부트스트랩 방식 사용해 실시간 업데이트, MC는 더 완벽한 경험으로 학습 
```
## 시간차 학습 VS 몬테카를로 갱신 방식
```
1. 몬테카를로 예측
상태에서 에피소드가 끝날 때가지 얻은 누적 보상의 평균 사용해 상태 가치 계산
여러 에피소드에서 상태를 방문한 후 얻은 보상의 평균으로 상태 가치 추정
에피소드가 끝난 후 모든 보상 고려해 상태 가치 계산하므로, 에피소드 단위로 가치 계산

2. 시간차 학습
현재 상태에서 다음 상태로의 보상과 다음 상태의 가치 합하여 즉각적으로 갱신
매 스텝마다 다음 상태의 정보 이용해 실시간으로 상태 가치 업데이트
부트스트랩 방식을 사용해 매 스텝마다 상태 가치 조정해 실시간 학습

3. 역 갱신 다이어그램 비교
시간차학습: 각 상태에서 다음 상태로 이동하며 실시간으로 갱신
몬테카를로: 종료상태에서부터 역방향으로 이전 상태까지 갱신, 종료된 후 전체 보상을 바탕으로 학습 
```

## 시간차학습의 장점
```
- 동적프로그래밍보다 좋은 점
보상 체계나 상태전이 확률 등 환경에 대한 완벽한 모델 없이 학습할 수 있어 현실적용에 유리
- 몬테카를로보다 좋은 점
매 타임 스텝마다 상태 가치 계산하므로 에피소드가 종료되지 않아도 실시간 학습 가능
지속적인 작업 환경에도 사용할 수 있으며, 종료 상태가 없는 경우에도 가능 
```
```
- 몬테카를로 대비 시간차 학습 성능
실험적으로는 시간차가 몬테보다 빠르게 학습하는 것으로 알려져 있음

- 시간차학습과 GPI
시간차 학습은 정책 반복의 일반화된 형태, 정책을 반복적으로 계산하고 평가
GPI는 모두에서 정책 평가 방식에 차이가 있을 뿐, 동일한 정책 개선이 목표 
```
## 3가지 차이
```
1. 몬테카를로 방법
전체 에피소드가 종료된 후에 실제로 경험한 보상을 사용해 상태 가치 계산
모델이 필요하지 않으며, 부트스트랩 사용 X, 에피소드 단위로 업뎃
그림에서 특정 경로로만 따라가며, 각 에피소드에서 경험한 경로의 보상 이용해 상태 가치 갱신

2. 시간차 학습
매타임스텝에서 다음상태의 가치와 즉각적인 보상을 사용해 상태 가치 계산
모델이 필요하지 않으며, 부트스트랩 방식, 타임스텝마다 업뎃
그림에서 에피소드 중 하나의 경로를 따르면서, 중간중간 즉각적인 보상 정보를 바탕으로 상태 가치 갱신

3. 동적프로그래밍
기대갱신: 모든 가능한 다음 상태와 그 전이확률을 기반으로 상태 가치 계산
환경모델이 필요, 부트스트랩 방식 사용, 타임스텝마다 갱신
가능한 모든 경로를 한꺼번에 고려해, 전체 상태 공간에 대한 기대값기반으로 가치 갱신
```
![image](https://github.com/user-attachments/assets/b1b35b2f-9717-4a78-9a18-2d7b325d7139)

## 배치 업데이트
```
강화학습에서 여러 에피소드에서 얻은 경험을 한꺼번에 모아 업데이트하는 방식

1. 배치
여러 에피소드를 실행한 후 누적 경험을 누적 경험을 하나의 뱇로 모음
에피소드 1,2,3에서 얻은 각 경험을 샘플로 저장 -> 이를 배치로 구성

2. 상태에 대한 배치
특정 상태 s에서 발생한 다양한 경험 샘플을 모아 배치로 구성
각 샘플은 (s,a,r,s')형태로 상태 s에서 행동 a를 했을 때 보상 r을 받고 다음 상태 s'로 이동한 경험
상태 s에 대해 여러 개의 샘플을 한 번에 업데이트 할 수 있으며, 강화학습 알고리즘 효율 높임
```
## 시간차학습의 배치업데이트
```
1. 배치업데이트
배치 업데이트는 여러 경험 샘플에서 얻은 시간차학습 타겟 값들을 한 번에 모아 각 상태의 가치 함수 갱신
각 상태에서 얻은 모든 겸험 샘플을 합산하고 평균 내어, 해당 상태의 가치 값 한 번에 갱신

2. 배치 업데이트 수식
각 샘플의 보상 Rt+1와 다음 상태 가치를 평균화해 상태 가치 업데이트

3. 반복학습 가능성
배치 내의 경험 샘플을 반복적으로 사용해 각 상태 가치 갱신, 주어진 정책에 맞춰 더운 정확한 가치 추정
특정 정책에 대한 안정적인 학습 가능, 반복 학습을 통해 최적 정책에 수렴
```

## 배치업데이트의 정확성
```
1. 배치 업데이트 예시
8번의 업데이트에서 얻은 경험을 사용해 상태 A와 B의 가치 추정
각 에피소드에서 상태 A또는 B를 방문하고, 보상이 T1 또는 T2 기록
이 과정을 통해 상태 전이와 보상의 패턴 학습

2. 마르코프 보상 프로세스 모델
상태 A에서 상태 B의 전이 확률이 100%, 상태 B에서는 T1과 T2로 전이될 확률 75,25%
이 확률모델 통해 여러번의 경험을 합산해 상태 가치 계산

3. 논리적 가치 계산
상태 A와 B의 가치는 각 상태에서 얻을 수 있는 보상의 기대값 바탕으로 추정
3/4로 상태 B에서 얻을 수 있는 평균적인 보상으로부터 계산 
```
![image](https://github.com/user-attachments/assets/2d705cdb-c017-41e7-94cb-517be5c47be3)
 ## 배치 업데이트 정확성 + 몬테카를로(MC) 방법을 사용하여 상태 가치 

```
1. 초기 설정
부트스트랩 사용하지 않기 때문에 V(A)=V(B)=0 V(B)는 V(A) 추정할 때 사용되지 않음

2. 배치 사용 여부에 따른 차이
사용 X: 8개의 에피소드 주어지지만, 상태 A에서 시작해서 종료상태에서 도달하는 과정에서
얻을 수 있는 이득은 에피소드당 0으로 남게 됨
배치 사용: 여전히 샘플보상 0, 각 상태에서 얻을 수 있는 기대보상 변하지 않아 V(A)=0으로 추정

3. 결론
배치업데이트를 사용하더라도 보상이 없는 경우 가치는 0으로 유지
배치업데이트가 여러 샘플을 모아 값을 추정할 수도 있지만, 주어진 샘플 자체에
보상이 없으면 가치가 변화하지 않는다!! 
```

![image](https://github.com/user-attachments/assets/6d6d0124-2d63-4e37-a8ed-1bc2edec1d6e)

## 시간차 학습 이용한 V(A) 추정
```
1. 초기 설정
상태 V(A)와 V(B)의 초기 가치는 모두 0으로 설정

2. 배치 사용하지 않을 때
V(A)를 업데이트하려고 할 때, 초기 값인 V(B)=0 사용해 계산
상태 V(A)는 업데이트되지 않고 0으로 유지

3. 배치 사용할 때
배치 내의 샘플들을 여러 번 활용해 반복학습을 진행하면, V(B)가 점진적으로 업ㄷ이트
=> V(B) => 3/4로 수렴 => 이 값을 통해 V(A)또한 점진적으로 갱신 => 3/4
```
![image](https://github.com/user-attachments/assets/5748f628-cb93-4e65-939a-e3fe60eaa85b)

## 배치 업데이트의 정확성
```
1. 배치 업데이트 효과
배치를 사용하지 않을 경우 TD와 MC모두에서 0으로 추정
배치 사용할 경우 TD는 3/4, MC는 여전히 0
배치 업데이트가 TD에서 더욱 정확한 값으로 수렴하는데 도움 줌

2. 정확한 추정이 가능한 이유
여러 샘플을 한꺼번에 고려하여 타겟 값 추정, 각 상태에 대한 가치가 정확하게 계산
동일한 샘플을 반복적으로 여러 번 사용할 수 있어, 가치 추정의 정확도 높아짐
부트스트랩 방식을 이용해 매 타입 스텝마다 가치 추정을 지속적으로 갱신

3. 배치 업뎃 장점
배치업뎃 기반의 시간차학습은 마르코프 결정 과정의 해답에 가까운 참 값 얻음
배치 업뎃은 논리저으로 추정 가능한 참 가치를 얻는데 중요 역할  
```
![image](https://github.com/user-attachments/assets/b866c848-aca2-45cb-850b-db83d7793c97)

## 랜덤 워크 환경에서 시간차 예측 성능 
```
1. 랜덤 워크 환경
초기상태값 모두 0.5, 각 상태에서의 정책: 왼 또는 오 이동 확률 0.5
목표는 시간차 예측을 통해 각 상태의 가치를 반복적으로 갱신에 참값에 수렴

2. 실험 설정
스텝 크기 알파:0.1, 감마:0.9 설정해 학습 속도 조절
여러 에피소드를 거치며 상태의 예측 가치 업뎃

3. 해석
초기 에피소드에서는 예측 값이 참값에서 벗어나지만, 에피소드가 증가할수록 가까움
시간차 예측이 반복학습을 통해 상태의 참값에 수렴 
```
![image](https://github.com/user-attachments/assets/f2fc5a57-6cd9-4be9-aec4-5495f2cb9b84)

## 그리드월드 환경에서 시간차 학습 통한 상태 가치 예측
```
1. 초기 설정
초기상태 값: 0.0 학습 파라미터: 알파 0.05, 감마: 1.0

2. 결과
a: 초기 반복 횟수에서는 참값과 큰 차이, 일부만 변화
b: 반복 횟수가 증가하면서 상태 가치가 점차 참값에 가까워짐
c: 충분한 반복 횟수에 도달, 정확한 예측에 성공

3. 비교
동적프로그래밍을 통한 참값과 비교했을 때, TD는 반복횟수가 증가할수록 참값에 수렴
TD학습이 반복학습을 통해 그리드 월드 환경에서 상태가치를 점진적으로 정확하게 예
```
![image](https://github.com/user-attachments/assets/886ea079-1c0b-4a4f-8ff5-ecbe49d6a8b0)
## 그리드 월드 환경에서 일반 시간차 VS 배치 기반 시간차 예측 성능 비교
```
왼쪽: 배치기반 TD 주황색은 일반 파라낵 선보다 RMS 오차 빠르게 감소해 참값에 수렴
오른쪽: 더 작은 학습률에서도 배치기반이 일반보다 오차가 더 빠르게 감소
학습률이 낮아졌지만 배치업데이트의 영향으로 성능이 안정적으로 향상 
```
![image](https://github.com/user-attachments/assets/20554f6c-687b-4c06-92f7-de21fcc6d11a)
