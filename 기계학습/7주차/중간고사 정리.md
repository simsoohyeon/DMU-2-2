# 강화학습의 기본 개념
## 강화학습 Reinforcement Learning
```
행동 심리학에서 시작하여, 보상을 극대화하는 행동을 학습하는 기계학습의 한 분야
```
## 인공지능을 위한 주요 기술
```
- 약 인공지능: 특정 작업에 특화된 기술
- 강 인공지능: 다양한 분야에 일반적으로 적용 가능한 기술
- 초 인공지능: 인간의 지능 초월하는 수준 
```

![image](https://github.com/user-attachments/assets/d7aba843-083f-467f-90bc-b195fea8986a)

## 심층 강화학습 Deep Reinforcement Learning
```
- 현재 환경의 상태 공간
상태 공간이 매우 크며, 특정 시간 자신의 상태에 대한 모든 환경을 경험하기 어렵다
모든 구간과 경우의 수를 돌아보는 것이 힘들며, 바둑의 경우 10^172 상태가 존재할 수도

- 에이전트의 행동 구간이 매우 클 때
주어진 문제를 해결하기 위해 여러 에이전트들이 협력하거나 경쟁할 수 있다
에이전트들은 협업을 통해 문제를 해결하거나 경쟁을 통해 더 나은 결과 도출 
```
## 동적 프로그래밍 Dynamic Programming
```
- 강화학습의 근간이 되는 알고리즘 1950: 동적 프로그래밍은 강화학습의 기초가 되는 알고리즘
리처드 벨만: 미국의 응용 수학자, 동적 프로그래밍의 창시자
```
## 동적 프로그래밍의 특징
```
- bottom-up 샹향식: 하위 문제의 해결을 기반으로 상위 문제 해결
- memorization 기억: 이미 계산한 결과를 저장하여 동일한 문제 다시 계산하지 않도록 하는 기법
- reuse 재사용: 계산된 결과 재사용하여 효율성을 높이는 방법 
```
## 동적 프로그래밍 경로
```
- start: 시작 지점, 문제 해결의 출발점
- goal: 목표 지점, 해결하고자 하는 문제의 최종 목표
- 노드(점): 각 원은 특정 상태이며, 이 상태에서 다음 상태로 전환
- 엣지(선): 노드 간의 연결, 숫자는 해당 경로로 이동하는데 드는 비용 또는 가중치
```
![image](https://github.com/user-attachments/assets/f626febe-05f5-46a1-8c44-8aa5d877b443)

## 시간차 학습 Q-Learning
```
- 강화학습의 근간이 되는 시간차 학습 방법 1980
: 리처드 서튼 >> 시간차 학습의 기초를 마련, 1981에 이 개념 발전
- Q-Learning 1989
: 크리스 왓킨 >> Q-Learning 알고리즘을 제안하여 강화학습의 가장 유명한 알고리즘 
```
## Q-Learning
```
에이전트가 환경에서 학습하면서 상태와 행동에 대한 가치를 추정하는 방법
=> 최적의 행동 선택, 오프라인 학습이 가능하여, 최적의 정책을 찾기 위해 사용
```

## 심층 강화학습의 시작
```
- 딥마인드의 DQN 2015
: 딥마인드에서 개발한 심층 강화학습 알고리즘인 DQN(Deep Q Network)
- 벽돌 게임
: DQN이 적용된 게임, 강화학습의 성능 보여줌
=> DQN이 바둑 게임과 같은 복잡한 문제에 적용되어 알파고가 발전 
```
# 강화학습의 기본 요소 >> 에이전트 / 환경 / 상태
## 에이전트 Agent
```
- 강화학습을 실행하는 주체: 강화학습 과정에서 행동을 결정, 실행
- 주어진 문제 상황에서 행동하는 주체: ex 로봇, 게임 캐릭터
```
## 환경 Environment
```
- 에이전트가 직접 상황을 작용하는 대상: 에이전트가 상호작용하는 외부 세계
- 에이전트의 행동을 입력으로 받아 처리: 에이전트의 행동에 대한 피드백으로 보상과 다음 상태 반환  
```
## 상태 state
```
- 환경으로 받은 관찰 정보, 행동, 보상 등으로 구성된 기록 정보: 에이전트가 처한 상황
- 에이전트가 환경에 받는 관찰 정보 그 자체: 환경의 상태를 에이전트가 관리하는 방식 
```
# 환경 Environment의 종류
## 1. 에피소딕 환경 Episodic Environment
```
- 상호작용이 특정 시점에서 종료되는 환경
- 에이전트가 시작 상태에서 시작해서 종료 상태까지 도달할 때까지의 과정이 에피소드
- ex) 게임이 특정 조건을 만족할 때가지 진행되고 끝남 
```
## 2. 지속적 환경 Continuing Environment
```
- 상호작용이 끝나지 않고 지속적으로 진행되는 환경
- 에이전트가 상태를 계속해서 업데이트
ex) 주식 가격 변동과 같은 연속적인 정보
```
# 에이전트의 행동에 따른 결과로 나타나는 환경의 종류
## 1. 결정적 환경 Deterministic Environment
```
- 에이전트의 행동이 결과를 항상 동일하게 만드는 환경
- 특정 상태에서 에이전트의 행동이 항상 동일한 결과(보상과 다음 상태) 가져옴
ex) 동일한 입력에 대해 항상 같은 결과가 나오므로 예측 가능 
```

## 2. 확률적 환경 
```
- 에이전트의 행동이 매번 다른 결과 가져오는 환경
- 특정 상태에서 에이전트의 행동이 결과에(보상과 다음 상태)에 대해 가변적
ex) 같은 행동을 하더라도 매번 다른 보상 
```
# 환경 상태와 관찰 정보
## 환경 상태 Environment State
```
- 환경에 내재하는 중요한 요소 도구, 장애물, 적 등
- 환경의 상태를 표현하는 모든 가능한 정보는 환경 상태를 이루는 정보
- 에이전트는 환경의 모든 세부 정보를 알 수 없을 수도,,
```
## 관찰 정보 Observation
```
- 에이전트가 환경과 상호작용하면서 얻게 되는 즉각적인 상태 표현
- 환경 상태 정보를 에이전트가 전체를 인식할 수 없기 때문에, 관찰 정보는 환경 상태 정보의 일부
- 매우 간단한 환경에서는 에이전트가 환경의 전체 상태 확인
- 환경 상태는 관찰 정보의 형태로 표현 
```
# 행동과 보상
## 행동 Action
```
- 에이전트가 환경에 전달하는 입력 정보
- 에이전트는 행동을 수행하여 환경과 상호작용
- 에이전트가 인식하는 상태에 따라 수행할 수 있는 행동 집합이 다를 수도 
```
## 보상 Reward
```
- 에이전트가 수행한 행동에 대해 환경이 에이전트에게 전달하는 값
- 보상은 에이전트의 정책을 변경하기 위한 정보
- 어떤 정책에 의해 선택된 행동을 수행한 후 받는 보상에 따라,
에이전트는 그 상태에서 다른 행동을 선택할 수 있는 기회 가짐 
```
# 강화학습의 목적
```
- 강화학습의 궁극적인 목적은 누적 보상의 기대값을 최대화
- 강화학습 에이전트는 추후에 받는 보상을 높일 수 있다면, 즉각적으로 받을 수 있는 보상 희생 
```
## 정책 policy 
```
- 에이전트가 주어진 상태에 대해 어떤 행동을 수행하는지 결정하는 기준
```
# 정책의 종류
## 1. 결정적 정책 Deterministic Policy
```
- 특정 상태에 대해 동일한 행동을 결정하는 정책
- ㅠ(s)=a 주어진 상태 s에서 행동 a를 결정 
```
## 2. 확률적 정책 Stochastic Policy
```
- 특정 상태에 대해 행동을 확률적으로 결정하는 정책
- ㅠ(a|s)=P(At=a|St=s) 상태 s에서 행동 a를 선택할 확률 
```
## 가치함수 Value Function
```
1. 상태 가치 함수 v(s)
에이전트가 위치할 수 있는 상태 s가 얼마나 좋은 상태인지?
상태 s의 가치는 에이전트가 그 상태에서 얻을 수 있는 기대 보상 반영

2. 행동 가치 함수 q(s,a)
상태 s에 대해 선택한 행동 a에 대해, 그 행동이 얼마나 유익한지?
특정 상태에서 특정 행동을 수행했을 때 얻는 기대 보상 기준

3. 가치 함수의 중요성
어떤 상태 s에서 즉각적인 보상이 좋지 않더라도, 다음 상태에서 큰 보상을 받을 수 있다면 가치 높음 
```
# 이득과 감가 이득
## 이득 Return
```
타임스텝 t 이후부터 종료 타임 스텝 T까지의 에이전트 보상을 Rt+1, Rt+2 ,,, RT
임의의 타임스텝 t에서 이득 Gt=Rt+1+Rt+2+Rt+...+RT
```
## 감가이득 Discounted Return
```
시간에 따라 보상의 중요도가 달라질 수 있음을 반영, 보상에 감가율 감마 적용해 계산
```
![image](https://github.com/user-attachments/assets/6d7d4151-9049-4627-a0a8-521ead6767b8)

```
감마는 0과 1 사이 값으로, 보상이 미래에 대한 영향을 얼마나 반영하는지 결정
보상의 합은 T까지 계속 감가되어 계산 
```
# 이득과 가치함수 Return and Value Function
## 상태 가치 함수 State Value Function
```
v(s): 에이전트가 임의의 타임스텝 t에 방문한 상태 s에서 임의의 정책 ㅠ에 따라
행동을 계속하였을 때 얻을 수 있는 기대 이득
특정 상태에서 에이전트가 받을 수 있는 보상의 기대값 
```
## 행동 가치 함수 Action Value Function or Q-Function

```
q(s,a): 에이전트가 임의의 타임 스텝 t에 방문한 상태 s에서 임의의 행동 a를 수행한 후,
임의의 정책 ㅠ에 따라 행동을 계속했을 때 얻을 수 있는 기대 이득
특정 상태에서 특정 행동을 취했을 때 기대 보상 
```
# 강화학습 수행 절차
```
1. 관찰 정보 수집 Observation
매 타임 스텝 t마다 에이전트는 환경으로부터 보상 Rt와 관찰 정보 Ot를 동시에 받고,
이 정보를 자신의 기억 저장소에 저장

2. 상태 저장 State
에이전트는 기억 저장소에서 현재 상태 St 구성

3. 행동 결정 Action
에이전트는 현재 상태 St와 정보를 활용해 행동 At를 선택하고 수행

4. 환경과 상호작용 Environment
선택한 행동 At 기반으로 환경과 상호작용해 다음 상태 St+1와 보상 Rt+1 받음

5. 반복
새로운 상태 St+1 구성 후, 이 과정을 반복해 에이전트가 최적의 정책 학습 
```
## 마르코프 속성 Markov Property
```
강화학습에서 임의의 타임 스텝 t에 대해 에이전트가 인식하는 상태 St는
다음 상태 St+1와 관련해 마르코프 속성이 있다고 가정
```
## 미래 상태와 현재 상태의 관계
```
- 미래 상태는 과거의 상태와는 관계없이 현재의 상태에만 의존
- 현재 상태 정보만으로 충분히 미래 상태를 결정할 수 있다는 점 중요 
```
# 강화학습의 특성
```
1. 기계학습과 차이
지도학습처럼 정답 데이터가 존재하지 않음, 보상 정보가 비슷한 역할 수행

2. 에이전트의 행동 결정
강화학습 에이전트가 행동을 결정하여 수행한 이후, 보상 정보 즉시 받지 못할 수도
=> 강화학습의 학습 난이도 상승

3. 타임 스텝 time step
강화학습은 타임 스텝에 따라 나눠진 일련의 상태, 행동, 보상 정보 처리
에이전트가 수행한 행동은 이후 에이전트가 받는 상태 정보에 영향 미침 
```
# 강화학습 알고리즘 종류
## 1. 가치 기반 알고리즘
```
가치 함수를 구성하여 활용하며, 정책은 명시적으로 존재하지 않음
가치함수를 통해 묵시적으로만 존재하고 활용하는 알고리즘
ex) DQN
```
## 2. 정책 기반 알고리즘 
```
별도의 가치 함수 구성하지 않음
정책 그 자체를 직접 구성하고 최적으로 발전시키는 알고리즘
ex) REINFORCE 
```
## 3. 액터-크리틱 actorp-critic 알고리즘
```
가치함수와 정책을 동시에 명시적으로 구성해 활용
최종적으로 정책을 최적으로 발전시키는 알고리즘
ex) A3C
```

# 마르코프 과정
## 확률 변수 X
```
무작위 실험 시 특정 확률로 발생하는 각 현상을 수치화
어떤 동작 결과에 대해 변수 X가 가질 수 있는 값과 그 확률이 정의
```
## 확률 변수의 예
```
동전을 두 번 던졌을 때 나올 수 있는 경우 표현
표본 공간 s={(앞,앞), (앞,뒤), (뒤,앞), (뒤,뒤)}
확률 분포 P(X=0)=1/4 P(X=1)=1/2 P(X=2)=1/4
```
## 확률 과정의 정의
```
시간에 따른 주어진 확률 변수의 변화 과정에 집중
확률 변수는 시간적 개념이 포함된 기호로 표현, Xt: 특정 시점 t의 확률 변수, t: 타임 스텝 
```
## 확률 과정 Stochastic Process
```
- 연속 동전 던지기 게임
매 타임 스텝마다 동전 하나를 던져서 앞면이 나오면 1, 뒷면 -1 부여하는 방식

- 초기 점수
시작 시 점수는 0점, P(X0=x)=1

- 타임 스텝 t에서의 확률 변수 Xt
타임 스텝 t에서 동전을 던진 뒤 얻는 누적 점수

- 확률 변수 공간
확률 변수는 상태 집합으로 구성 ex) {...,-3,-2,-1,0,1,2,3,...}
```
## 마르코프 과정
```
시간에 따라 주어진 환경의 상태 변화를 '상태 전이 확률'로 기술하는 확률 과정
- 임의의 타임 스텝 t에서의 상태는 St, 특정 상태 값 s,s' 가짐
- 상태 전이에 대해 마르코프 속성을 지니고 있음을 가정
- 임의의 타임 스텝 t에 대해 상태 St는 바로 직전의 상태 St-1에만 의존하여 결정
```
## 마르코프 과정의 예시 
```
- 상태 집합 S={start, class1, class2, class3, pass, pub, facebook, sleep}
종료 상태: sleep
```
## 마르코프 프로세스의 도식화
```
- 그래프에서 각 상태는 동그라미, 상태 간 전이를 화살표
- 화살표는 특정 상태에서 다른 상태로 이동할 확률
ex) start -> class1: 확률 1.0
```
## 상태 전이 행렬 Transition Matrix
```
상태 전이 행렬 P는 각 상태에서 다른 상태로의 전이 확률을 나타내는 테이블
ex) start에서 class1: 1.0, class1->pass: 0.6
각 행은 현재 상태, 각 열은 다음 상태
행렬의 각 원소는 특정 상태에서 다음 상태로 전이활 확률
ex) P[Class1][Pass]= 0.6, class1에서 pass로 이동할 확률이 60%
```
![image](https://github.com/user-attachments/assets/7daf3044-0c38-44c6-89ce-2b18e0c17dc1)

# 마르코프 과정에서의 에피소드
## 에피소드 Episode
```
- 환경과 에이전트 사이의 일련의 상호작용이 완료되는 단위
- 마르코프 과정이 주어지면 해당 과정으로부터 임의의 에피소드 다양하게 산출
```
![image](https://github.com/user-attachments/assets/4b238c53-0a6e-4731-b44c-a53c1c409282)

## 에피소드의 종류
```
1. 에피소딕 Episodic
종결 상태가 존재하며, 마지막 타임 스텝의 기호는 T로 표현
에이전트가 환경과 상호작용을 통해 일정한 목표를 달성한 후 종료되는 일련의 과정 포함

2. 지속적 Continuing
종결 상태 존재하지 않는 경우
에이전트가 계속해서 환경과 상호작용하여 학습을 진행하는 형태 
```
## 마르코프 보상 과정 Markov Reward Process
```
- 기존의 마르코프 보상 과정에 보상 요소 추가해,
시간에 따른 환경의 상태 변화에 따라 보상을 얻어낼 수 있는 확률 과정
임의의 타임 스텝 t에서의 보상은 Rt로 표현, 이는 특정 보상 값 r 나타냄
```
## 구성 요소
```
1. 상태 전이 확률
p(s'|s): 주어진 상태 s에서 다음 상태 s'로 전이할 확률

2. 보상함수
r(s'): 특정 상태 s'에서 받을 수 있는 보상의 기대값

3. 초기 상태 확률: 초기 상태가 s일 확률 
```
## 마르코프 과정의 예시 Markov process
```
상태 및 전이 >>
1. 상태 집합
Start, Class1, Class2, Class3, Pass, Pub, Facebook, Sleep으로 구성

2. 전이 확률
화살표는 현재 상태에서 다음 상태로의 전이 확률 나타냄
ex) start에서 class1로 이동할 확률은 0.5, facebook 상태에서 class1로 이동할 확률은 0.9

3. 보상
각 상태에서 받는 보상은 붉은 글씨
facebook에서 -1.0 보상, class1에서 -2.0 보상, pass에서 10.0의 보상,,,
```
```
- 에이전트는 주어진 상태에서 다음 상태로 전이할 때 각 전이에 대한 확률과 보상 고려하여 최적의 행동 선택
- 마르코프 과정은 특정한 환경에서의 에이전트의 행동과 그 결과를 모델링하는데 사용
```
![image](https://github.com/user-attachments/assets/70a9382d-63a1-44b0-94d2-bb4eb09e0388)

# 이득 Return과 감가이득 Discounted Return
## 이득 Return 
```
타임스텝 t 이후부터 종료 타임 스텝 T까지 에이전트가 받는 보상들을
Rt+1, Rt+2, Rt+3,,, RT라 할 때, 임의의 타임 스텝 t에서의 이득 Gt는 아래 식으로 정의
```
![image](https://github.com/user-attachments/assets/a07a84a3-6cab-47c9-9c20-9be1283164ca)

## 감가이득 Discounted Return
```
타임스텝 t 이후부터 종료 타임 스텝 T까지 에이전트가 받는 보상들을
Rt+1, Rt+2, Rt+3,,, RT라 할 때, 임의의 타임 스텝 t에서의 감가 이득 Gt는 아래 식으로 정의
여기서 감마는 감가율로, 0과 1 사이의 값 가짐 
```
![image](https://github.com/user-attachments/assets/1d91391c-9fc6-447e-b30f-5cdd37141145)

## 감가율 Discounting Rate, 감마
```
- 0<=r<=1의 값 가지고, 미래의 가치를 현재 시점에서 가치로 환산하는 비율
- 감가율이 높을수록 미래의 불확실성이 낮고, 낮을수록 불확실성 높아짐
왜???????????????????????????
```
## 지속적 작업 기반 에피소드에서 감가율에 따른 감가이득 
```
1. 감가율 r=0
Gt=Rt+1 => 현재의 보상만 고려

2. 감가율 r=1
Gt=Rt+1+Rt+2+Rt+3+,,,,, => 모든 보상 합산하여 고려

3. 적당한 감가율 r
Gt=Rt+1+rRt+2+r^2Rt+3+,,, => 각 보상에 대해 감가율 적용해 시간에 따라 중요성 감소
```

## 마르코프 결정 과정 Markov Decision Process, MDP
```
마르코프 보상 과정에서 상태 전이가 이벤트를 유발하는 요소로 행동을 추가한 것
```
## 구성 요소
```
1. 행동
임의의 타임 스텝 t에서의 행동은 At로 나타나며, 이는 특정 보상 값 r 가짐

2. 보상과 상태
임의의 타임 스텝 t에서의 보상은 Rt로 나타나며, 상태는 St
이전 상태 St-1 및 이전 행동 At-1에만 의존

3. 확률 과정
임의의 확률 과정을 따르는 확률 변수로 구성
```

## 마르코프 결정 과정 MDP
```
- 환경의 동적 특성을 결정짓는 확률
마르코프 속성에 따라 보상 Rt와 상태 St를 결정하는 요소 이전 상태 St-1과 이전 행동 At-1
```
# 강화학습의 목적과 보상의 역할
## 강화학습 목적
```
- 문제를 MDP로 표현
문제를 마르코프 결정 과정 MDP로 모델링하여 각 타임스텝마다 받는 보상의 누적 합을 최대화하는 것이 목표
- 보상의 누적합
단기적인 보상 뿐만 아니라 미래 보상의 누적합까지 고려 
```
## 보상 값의 역할
```
- 강화학습 목표를 달성하기 위해 보상 값은 중요한 역할
- 문제의 목표를 달성하는 방법을 알려주는 것이 아니라, 문제의 목표가 무엇인지 알려줌
```
```
ex) 체스 게임
상대방의 말을 잡아먹을 때마다 긍정적 보상
잘못된 경기를 이기는 전력이 아닌, 단순히 상대 말 잡아먹는 데에 관심 
```
## 보상의 활용 ex
```
1. 미로에 놓여진 로봇
로봇이 출구를 찾아서 빠져나와야 할 때, 에이전트는 매 타임 스텝마다 -1의 보상 받음
=> 빠른 시간, 작은 타임 스텝 내에 출구를 찾도록 해야 함

2. 재활용 수거 이동 로봇
빈 깡통을 수거할 때 +1의 보상 => 더 많은 깡통 수거하도록 유도

3. 자율 주행 자동차
장애물과 부딪히면 음의 보상 => 장애물을 피하도록 유도

4. 대결 게임
경기를 이기면 양의 보상 => 전략으로 학습하는데 도움 
```









