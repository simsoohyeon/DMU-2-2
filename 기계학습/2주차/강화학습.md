<div align="center">
  <h1>강화학습의 기본요소</h1>
</div>

## 🟡 강화학습 Reinforcement Learning
```
강화학습은 기계학습의 한 분야로, 컴퓨터가 스스로 행동을 결정하는 방식을 학습하는 방법
강화학습은 주어진 환경에서 컴퓨터가 상호작용을 통해 행동을 학습하고, 그 행동의 결과로 얻는
보상(또는 벌)을 통해 스스로 최적의 행동 방식을 찾아가는 학습 방법, 목표는 누적보상의 최대화
기계학습 내 위치: 강화학습은 지도학습(정답을 모두 알고 학습하는 방법)과 비지도학습(답을 모른채
패턴을 학습하는 방법)과 더불어 기계학습의 한축을 담당
- 핵심 아이디어: 시스템은 정답을 명시적으로 주어지는 것이 아니라, 시도와 결과에 따른
보상 피드백을 통해 점진적으로 최적의 행동 찾음
=> 강화학습은 복잡한 환경에서의 의사결정 문제를 해결하기 위한 주요 기술로 자리잡음 
```
## 🟡 인공지능의 발전 단계
```
- 약 인공지능: 인공지능 기술이 특정 일부 분야에서만 적용되는 단계
- 강 인공지능: 모든 분야에 일반적으로 적용 가능한 인공지능
- 초 인공지능: 인간의 지능을 뛰어넘는 수준의 인공지능 
```

![image](https://github.com/user-attachments/assets/a485a276-7908-4c1a-8b2e-14ddaefe3488)

## 🟡 심층 강화학습 Deep Reinforcement Learning
### 1. 상태 공간의 크기
```
- 현재 환경의 상태 공간이 매우 큼: 심층 강화학습에서 에이전트가 학습하는 환경의 상태 공간이
매우 크기 때문에, 제한된 시간 내에 에이전트가 모든 상태를 경험하는 것은 불가능
즉, 모든 상황을 하나하나 다 경험해보는 것은 현실적으로 어려운 문제
ex) 바둑: 바둑의 상태 공간은 10^172개의 경우의 수를 가지고 있을 정도로 매우 방대
=> 다양한 상태를 모두 탐색하는 것은 힘듦
```
### 2. 행동 공간의 크기
```
- 에이전트의 행동 공간이 매우 클 때: 상태뿐만 아니라 에이전트가 취할 수 있는 행동의 경우의 수도 매우 클 수도,, 에이전트는 주어진 상태에서 최적의 행동을 선택하는 것이 어려워짐
```
### 3. 협력과 경쟁
```
- 다중 에이전트: 주어진 문제를 해결하기 위해 여러 에이전트가 협력하거나 경쟁하는 상황이 존재
이런 상황에서 에이전트 간의 상호작용을 통해 문제를 해결하게 됨
- 협업과 경쟁의 병행: 때로는 여러 에이전트가 협력하면서도 경쟁적인 관계에 있을 수도 있으며,
이런 환경에서 강화학습 적용
=> 상태 공간과 행동 공간이 방대할 때는 모든 상황을 다 경험하는 것은 어렵기 때문에,
효율적인 탐색 방법과 협력 전력 필요 
```
## 🟡 Deep Reinforcement Learning
```
1. 심층 강화학습
- 딥러닝과 강화학습을 결합한 학습 방법으로, 알고리즘의 성능, 학습속도, 안정성 높이는 것이 특징
- 다중 에이전트 환경: 여러 에이전트가 통신하거나 협력, 경쟁하면서 문제를 해결하는 복잡한 상황 다루며,
에이전트 간 상호작용이 중요한 역할을 함
2. 딥마인드의 대표적인 기술
- 딥마인드는 2013 영국에서 시작된 회사, 사람보다 게임을 잘하는 Deep Q-Network (DQN) 기술 개발
- 이 기술은 2016 알파고, 2019 알파스타로 발전
```
## 🟡 스키너 상자 Skinner Box
```
- 스키너 상자: 1931 하버드 대학의 심리학자 스키너가 고안한 도구로, 강화학습의 중요한 모티브
- 실험 방법: 실험용 동물에게 빛이나 소리와 같은 신호에 대해 특정 반응을 하도록 유도하는 장치
ex 지렛대를 누르면 보상(음식)이 주어지거나, 잘못된 행동을 하면 처벌이 주어짐
- 목적: 동밀이 정확한 행동을 할 때 보상을 받고, 잘못된 행동을 하면 처벌을 줌으로써 조건 반응 연구
```
![image](https://github.com/user-attachments/assets/e546a544-b113-4c3e-b6a9-04a6e2a8244d)

## 🟡 동적 프로그래밍 Dynamic Programming
```
- 동적 프로그래밍: 1954 리처드 벨만이 창시한 알고리즘 기법, 문제를 작은 하위 문제로 나누고
그 결과를 재사용하면서 해결하는 방법, 이는 강화학습의 근간이 되는 중요한 알고리즘
특징 >>>
- Bottom-up(샹향식): 작은 문제부터 차근차근 풀어나가는 방식
- Memorization(기억): 이미 해결한 하위 문제의 결과를 저장하여 반복적인 계산 방지
- Reuse(재사용): 저장된 결과를 다시 사용하여 효율적으로 문제를 해결 
```
### 동적 프로그래밍에서 이 문제를 해결하는 방법 >>
```
- 최적의 해를 구하기 위해 각 경로의 총비용을 계산하여 가장 비용이 적은 경로를 선택
- 하위 문제 해결 후 재사용: 모든 경로에 대한 비용 계산한 후, 그 결과를 저장해 필요할 때
다시 사용하여 효율적으로 문제 해결 가능 
```

![image](https://github.com/user-attachments/assets/4a36a96d-b98f-481d-8db6-b767c4de800b)

## 🟡 시간차 학습 Temporal Difference Learning
```
- 1980 제안된 강화학습의 근간이 되는 학습방법으로, 리처드 서튼이 1981 연구
- Q-Learning: 1989 크리스왓킨스가 개발한 강화학습에서 가장 유명한 알고리즘
에이전트가 환경과 상호작용하면서 최적의 행동을 학습하게 되는 방법
```
## 🟡 심층 강화학습의 시작
```
- 딥마인드의 DQN(deep q-network): 2015 딥마인드가 발표한 알고리즘으로, 이 알고리즘은
컴퓨터가 벽돌깨기 게임을 다시 한 번 풀 수 있도록 만들어짐
- 알파고: DQN 기술 이후 바둑 게임에 적용되어 알파고의 발전에 기여
```

## 1. 에이전트 Agent
```
강화학습을 실행하는 주체로, 주어진 문제나 상황에서 행동을 수행하는 역할
에이전트는 로봇일 수도 있고, 게임에서의 캐릭터나 플레이어처럼 상황에 따라 다양한 형태 가짐
```
## 2. 환경 Environment
```
에이전트가 상호작용하는 대상
에이전트가 행동을 취하면 환경이 그 행동을 받아들이고, 반응으로 보상과 다음 관찰 정보를 에이전트에게 돌려줌
```
## 3. 상태 State
```
환경에서 에이전트가 관찰하는 정보, 행동, 보상 등을 기반으로 구성된 정보
에이전트는 이 상태 정보를 통해 환경에 대한 이해를 쌓고, 이후 행동을 결정하는데 활용
상태는 에이전트가 환경에서 받을 수 있는 관찰 정보 자체를 의미
```
```
에이전트는 환경 속에서 주어진 상태 정보를 바탕으로 행동을 선택하고,
환경으로부터 보상과 새로운 상태 정보를 받아 학습을 계속하게 된다
```
## 🟡 환경의 종류 (종료 상태 유무에 따른 구분):
### 1. 에피소드틱 환경 Episodic Environment
```
- 종료 상태가 있는 환경
- 시작 상태에서 시작해 종료 상태에서 도달할 때까지의 과정을 하나의 에피소드로 간주해 학습
ex) 게임처럼 명확한 시작과 끝이 있는 상황에서 활용
```
### 2. 지속적 환경 Continuing Environment
```
- 종료 상태가 없는 환경
- 상태가 계속해서 변하며, 끝이 없이 지속
```
## 에이전트의 행동에 따른 결과가 항상 일정한지 여부에 따른 환경의 종류:
### 1. 결정적 환경 Deterministic Environment
```
에이전트가 특정 상태에서 행동을 취할 때, 그 결과가 항상 일정하고 변하지 않는 환경
ex 같은 행동을 반복하면 동일한 결과가 나오는 상황 
```
### 2. 확률적 환경 Stochastic Environment
```
에이전트의 행동 결과가 항상 일정하지 않고, 동일한 상태에서 동일한 행동을 취해도 매번 다른 결과를
얻을 수 있는 가변적인 환경, 결과가 확률적으로 결정되는 환경

=> 두 가지 환경은 강화학습에서 에이전트가 학습하는 방식에 영향을 미치며,
결정적 환경은 결과 예측이 쉽지만, 확률적 환경에서는 예측이 어렵고 복잡한 환경 필요 
```

## 🟡 << 환경 >>
### 1. 환경 상태 Environment State
```
환경 내에서 존재하는 모든 중요한 요소를 포함하는 정보 의미
환경 상태는 환경을 완전히 표현하는 정보로 이루어져 있지만, 에이전트에게는 이 정보가 모두 노출되지 않을 수도,, 
```
### 2. 관찰 정보 Observation
```
에이전트가 환경과 상호작용하면서 즉각적으로 얻는 정보
관찰 정보는 환경 상태의 일부분만을 포함하며, 에이전트는 전체 환경 상태를 알지 못할 수도,,
단순한 환경에서는 에이전트가 환경의 전체 상태를 관찰할 수 있지만,
복잡한 환경에서는 관찰 정보가 제한적일 수도 있다 
```

### 1. 에피소드틱 환경 Episodic Environment
```
- 종료 상태가 있는 환경
- 에이전트가 시작상태에서 출발해 종료상태에 도달할 때까지의 과정을 하나의 에피소드로 간주
각 에피소드의 목표는 특정한 시간에 종료되는 상호작용을 통해 목표를 달성하는 것 
```
### 2. 지속적 환경 Continuing Environment
```
- 종료 상태가 없는 환경으로, 상호작용이 끊임없이 지속
- 에이전트는 끝없이 이어지는 환경에서 학습해야 하며, 매순간이 새로운 상태로 이어짐
```
## 🟡 행동과 보상
### 1. 행동 Action
```
- 에이전트가 환경에 전달하는 입력정보로, 에이전트는 행동을 통해 환경과 상호작용
- 수행할 수 있는 행동의 집합은 에이전트가 처한 상태마다 다르게 나타날 수도 있다
```
### 2. 보상 Reward
```
- 에이전트가 수행한 행동에 대해 환경이 에이전트에게 주는 값
보상은 에이전트가 학습하는데 중요한 정보로, 에이전트가 정책(행동 전략)을 수정하는데 사용
- 특정 행동이 낮은 보상을 받으면, 에이전트는 다음에 그 상태에서 다른 행동을 선택하려고 습
```
## 🟡 강화학습의 목적
```
- 강화학습의 목적은 누적 보상의 기대값을 최대화하는 것
즉, 에이전트가 시간이 지나면서 받을 보상의 총합을 최대화하는 것이 목표
```
### 보상 가설
```
- 에이전트는 즉각적으로 받을 수 있는 보상과 나중에 더 큰 보상을 받을 수 있는 가능성을 비교해 결정
즉, 당장의 작은 보상을 받을지, 더 나중에 더 큰 보상을 받기 위해 현재의 보상을 포기할지 선택
ex) 지금 당장 눈 앞의 떡볶이를 먹을 것인지, 아니면 시험을 잘 봐서 나중에 더 큰 선물 받을 것인가
```
## 🟡 정책 Policy
```
에이전트가 주어진 상태에서 어떤 행동을 수행할지 결정하는 규칙
즉, 에이전트가 상태에 따라 취할 행동을 정하는 전략
```
### 1. 결정적 정책 Deterministic Policy
```
- 주어진 상태에서 에이전트가 하나의 특정 행동을 항상 수행
- 상태 s에서 행동 a는 정확하게 하나로 결정
- 수식 표현 ㅠ(s)=a
```
### 2. 확률적 정책 Stochastic Policy
```
- 주어진 상태에서 에이전트의 행동이 확률적으로 결정
- 상태 s에서 행동 a가 특정 확률로 선택
- 수식 표현 ㅠ(a|s)=P(At=a|St=s)
```

## 🟡 가치함수
### 1. 상태가치함수 (v(s))
```
에이전트가 특정 상태 s가 있을 때, 그 상태 자체가 얼마나 좋은지 나타내는 함수
즉 상태가 미래에 어떤 보상을 가져올지 예측하는 값
```
### 2. 행동가치함수 (q(s,a))
```
특정 상태 s에서 행동 a를 선택했을 때, 그 행동이 얼마나 유익한지 나타내는 함수
상태에서 어떤 행동을 취하면 미래에 얼마나 더 큰 보상을 얻을 수 있는지 평가

=> 어떤 상태가 즉각적인 보상은 좋지 않을 수 있지만, 그 상태에서 이후에 큰 보상을 얻을
가능성이 있다면 그 상태의 가치는 높다고 할 수 있다 
```
## 🟡 이득 Return
```
에이전트가 어떤 시점부터 시작해서 미래에 받을 모든 보상의 합을 의미
즉 타임스텝 t 이후에 에이전트가 끝날 때까지 받는 보상들을 모두 더한 값 
```
### 감가이득 Discounted Return
```
미래에 받을 보상일수록 그 중요도를 줄이는 방법
즉, 현재 시점에서 가까운 보상은 크게 고려하고, 먼 미래의 보상은 조금 덜 중요하게 보는 방식
이를 통해 보상을 계산할 때 할인율(감가율)을 적용하여 미래의 보상들을 평가 
```

##########################


## 🟡 이득과 가치함수
### 1. 상태가치함수 State Value Function
```
에이전트가 특정 상태 s에 있을 때, 그 상태에서 정책 ㅠ에 따라 행동을 계속 수행했을 때
기대할 수 있는 이득(보상의 합)을 나타냄
V(s): 특정 상태에서 모든 가능한 행동을 고려한 기대 이득을 평가 
ex) 만약 에이전트가 현재 상태에서 어떤 행동을 하든 상관없이, 앞으로 어떤 보상을 받을 수 있을지 평가
```

### 2. 행동가치함수 Action Value Function or Q-Function
```
에이전트가 특정 상태 s에서 행동 a를 선택했을 때, 이후에 정책 ㅠ에 따라 행동을 계속했을 때
기대할 수 있는 이득을 나타냄
Q(s,a): 특정 상태에서 특정 행동을 선택했을 때의 기대 이득을 평가 
ex) 에이전트가 현재 상태에서 특정 행동을 선택할 경우, 그 행동으로 인해 얻을 보상을 평가  
```
## 🟡 강화학습 수행 절차
```
1. 환경과 상호작용
에이전트는 매 타임스텝마다 t마다 환경으로부터 보상 Rt와 관찰 정보 Ot를 받음
이 정보들은 에이전트의 기억에 저장

2. 상태 정보 구성
에이전트는 자신의 기억에 저장된 정보들을 바탕으로 현재 상태 St를 구성

3. 행동 선택 및 수행
에이전트는 현재 상태 St에서 수집된 정보를 바탕으로 행동 At를 선택하여 실행

4. 다음 상태로 이동
에이전트가 행동을 수행하면, 환경은 그에 따른 새로운 보상 Rt+1과 다음 관찰 정보 Ot+1를 반환

5. 상태 업데이트
에이전트는 새로운 관찰을 통해 상태를 업데이트하고, 다음 행동을 선택하는 과정 반복 
```
![image](https://github.com/user-attachments/assets/85776a70-86e3-4918-a8e9-8a42f440ad7e)

## 🟡 마르코프 속성 Markov Property
```
에이전트가 인식하는 상태 St는 마르코프 속성을 가진다고 가정
이는 미래의 상태 St+1가 오직 현재 상태 St에만 의존하며, 과거의 상태는 의존하지 않는다고 의미
미래의 상태는 과거와 상관없이 현재 상태만으로 결정될 수 있음
즉, 현재 상태에 포함된 정보만으로 미래 상태를 예측할 수 있다는 가정 
```
## 🟡 강화학습의 특징
```

```
