
<div align="center">
  <h1> 마르코프 과정</h1>
</div>

## 🟡 확률 과정 Stochastic Processes
### 1. 확률변수 X
```
- 무작위 실험에서 특정 확률로 발생하는 현상을 수치로 표현한 변수
ex) 동전을 두 번 던졌을 때 앞면이 나오는 횟수를 확률변수 X로 나타내며, 0,1,2 중 하나
```
### 2. 표본공간
```
- 모든 가능한 실험 결과들의 집합
- 동전을 두번 던질 경우 s=(앞,앞),(앞,뒤),(뒤,앞),(뒤,뒤)
```
### 3. 확률과정 Stochastic Process
```
- 확률변수의 변화과정을 시간에 따라 추적하는 것
- 시간이 지나면서 확률 변수가 어떻게 변화하는지 중점적으로 다룸
- Xt는 확률변수 X가 시간 t에 따라 변화함을 나타냄
=> 확률과정은 시간에 따라 확률변수의 변화과정을 연구하는 것 ㅅ
```

## 🟡 연속 동전 던지기 게임을 통한 확률 과정
### 1. 연속 동전 던지기 게임
```
- 매시간 스텝마다 동전을 던짐
- 앞면이 나오면 1점을 더하고, 뒷면이 나오면 -1점을 뺌
- 초기점수는 0점에서 시작하며, P(X0=X)=1, 즉 처음 상태에서 확률변수 X0는 0
```
### 2. 시간에 따른 누적 점수
```
시간이 흐르면서 각 시간 스텝 t에서 던진 동전의 결과로 얻는 점수를 누적하여 나타냄
ex)
X0 = -1 or 1 (첫 번째 동전 던지기 결과)
X1 = -2,0,2 (두 번째 동전 던지기 결과)
X2 = -3,-1,1,3 (세 번째 던지기 이후 누적 점수) => 이와 같이 누적
```
### 3. 확률변수 공간
```
누적 점수는 상태집합으로 구성되며, 모든 가능한 누적점수는 무한히 확장
S = { …, -3, -2, -1, 0, 1, 2, 3, …. }
=> 이미지는 10번과 1000번 반복한 동전 던지기 후의 그래프,
누적 점수가 시간에 다라 어떻게 변화하는지? 
```
![image](https://github.com/user-attachments/assets/ff3942b6-ec56-41e1-af94-13f99f36edd5)

## 🟡 마르코프과정 (Markov Process/Markov Chain)
### 1. 마르코프 과정이란?
```
- 시간에 따라 상태가 변화하는 과정을 다루는 이론
- 이때 상태의 변화는 바로 이전 상태에만 의존하는 특징이 있음
즉, 현재 상태가 다음 상태를 결정하는데 있어서, 그 전의 상태들은 중요하지 않다는 것 의미
```
### 2. 상태전이 확률
```
특정시간 t에서 상태 St는 이전 시간 t-1에서의 상태 St-1에만 영향을 받아 결정 => 마르코프 성질
```
### 3. 마르코프 과정의 특징
```
현재 상태가 주어지면 미래 상태는 과거 상태와는 무관하게 결정, 이게 핵심적인 마르코프 성질 
```
### 4. 실생활 예
```
날씨를 예로 들음, 오늘의 날씨가 내일의 날씨에 영향을 미치지만, 그저께의 날씨는 내일의 날씽
영향을 미치지 않는다고 가정하는 것과 같음 오늘의 상태만 알고 있으면 내일의 상태를 예측할 수
있는 것이 마르코프의 성질

=> 마르코프 과정은 상태가 시간에 따라 변화하는데, 그 변화가 바로 이전 상태에만 의존하는 확률과
```
## 🟡 마르코프 과정의 예시
### 1. 상태 집합 S
```
- 주어진 상태들은 {Start,Class1,Class2,Class3,Pass,Pub,Facebook,Sleep}로 구성
- 즉 학생이 수업에 참여하거나, 공부 통과, 술집이나 페북 사용한 뒤 마지막으로 잠으로 가는 과정
```
### 2. 마르코프 과정의 흐름
```
- 과정의 시작은 start에서 시작하고, 각 상태 사이의 화살표는 다른 상태로 전이될 확률 나타냄
- 예를 들어 학생이 c1 -> c2에서 이동할 확률은 0.5, pub -> c1으로 갈 확률은 0.2
- 모든 상태는는 결국 sleep 상태로 끝남
```
### 3. 상태 전이 행렬
```
- 오른쪽의 표는 각 상태에서 다른 상태로 전이될 확률을 수치로 나타낸 행렬
- 행: 현재 상태, 열: 다음 상태, 값: 현재 상태에서 다음 상태로 전이할 확률
ex) start->class1: 1.0, class1->class2: 0.5, class1->facebook: 0.5
class2->class3: 0.8, facebook->facebook: 0.9
- 각 행의 확률 값은 1이 됨, 이는 시스템이 반드시 어떤 상태로 전이한다는 것을 보상
- 값이 0인 경우는 해당 상태에서 다른 특정 상태로 이동할 수 없다는 것 의미
=> 각 상태에서 가능한 다른 상태로의 전이 확률을 통해 시스템의 전반적인 행동 예측 
```
![image](https://github.com/user-attachments/assets/28049bd1-7ddd-43c6-815c-cfad01a3bcd3)

## 🟡 에피소드 episode
### 1. 에피소드란
```
- 시작점부터 끝점까지의 경로로, 하나의 환경에서 에이전트(참여자)가 상호작용을 완료하는 단위
- 특정한 과정이 주어지면, 그 과정에서 발생할 수 있는 여러 가지 가능한 경로들을 에피소드
```
### 2. 마르코프 과정에서의 에피소드
```
- 마르코프 과정에서 시작 상태와 종료 상태가 주어졌을 때, 다양한 경로를 따라 여러 에피소드가 만들어질 수 있음
- 예를 들어 시작이 start이고 종료가 sleep인 경우, 여러 에피소드 존
```
### 3. 예시
```
Episode 1: Start → Class1 → Class2 → Class3 → Pass → Sleep
Episode 2: Start → Class1 → Facebook → Facebook → Class1 → Class2 → Sleep
Episode 3: Start → Class1 → Class2 → Class3 → Pub → Class2 → Class3 → Pass → Sleep
```
## 🟡 에피소드의 종류
### 1. 에피소딕 episodic
```
- 종료상태가 명확히 존재하는 에피소드
- 마지막 타임 스텝 T에 도달하면 끝나는 과정
ex) 시작점과 종료점이 명확환 여정이나 게임과 같은 상황 
```
### 2. 지속적 continuing 
```
- 종료 상태가 존재하지 않는 에피소드
- 과정이 계속 반복되고 끝이 없어서 무한히 지속
ex) 끝없이 진행되는 상태 변화나 순환적인 시스템이 해당 
```
## 🟡 마르코프 보상 과정 Markov Reward Process
```
기존 마르코프 과정에 보상 개념을 추가한 것!

1. 보상 요소 추가
마르코프 과정에 보상이라는 개념을 추가하여, 각 상태에서 얻을 수 있는 보상 포함
상태 St에서의 보상 Rt는 특정 보상 값 r로 표현

2. 보상의 의미
시간에 따라 상태가 변할 때마다 해당 상태에서 얻을 수 있는 보상을 계산하는 과정
ex) 어떤 상태에 도달하면 보상(이익, 점수)을 받는다고 가정

3. 구성 요소
마르코프 보상 과정은 상태전이확률, 보상함수, 그리고 감가율 등을 포함해 설계
감가율 r은 보상이 시간이 지남에 다라 얼마나 가치가 감소하는지 나타냄

=> 마르코프 보상 과정은 상태 변화와 더불어 각 상태에서의 보상을 추적하는 확률 과정,
각 상태에서 얻는 보상과 그 변화의 흐릉을 모델링하는데 사용 
```
## 🟡 마르코프 보상과정을 시각적으로 표현한 다이어그램
```
1. 상태 state
각 원은 상태를 나타내며, "Start", "Class1", "Class2", "Pub", "Facebook", "Sleep", "Pass"
학습하는 과정 중에 학생이 겪는 여러 상태로 볼 수 있음

2. 상태 전이 확률 transition probability
각 화살표는 상태 간의 이동을 나타내며, 화살표 옆 숫자는 한 상태에서 다른 상태로 이동할 확률
ex) "Class1"에서 "Class2"로 갈 확률은 0.5, "Class1"에서 "Facebook"으로 갈 확률은 0.1

3. 보상 reward
각 상태 또는 상태 간 전이에서 얻게 되는 보상 R이 붉은 색으로 표시
ex) "Class1" 상태에서 "Facebook"으로 전이하면 보상은 R=−1.0, "Pass" 상태로 도달하면 
R=10.0의 높은 보상
"Pub" 상태에서 보상은 R=1.0, 반면 "Class2"와 "Class3"에서는 보상이 R=−2.0로,
어려운 학업 상태에서 부정적인 보상을 받는 것

4. 최종 상태
sleep 상태로 가면 R=0.0의 보상을 받으며, 모든 과정이 끝나는 것 의미
pass 상태로 가면 학업을 성공적으로 마쳤다는 의미 R=10.0의 높은 보상 받음

=> 각 상태에서 받을 수 있는 보상 표현
긍정적 행동은 높은 보상, 부정적 행동은 낮은 보상을 받음 
```
![image](https://github.com/user-attachments/assets/dd678f40-f0da-4d04-befd-cf84ef40f5f0)

## 🟡 이득과 감가 이득 Return and Discounted Return
### 1. 이득 Return
```
- 에이전트가 특정 시간 스텝 t부터 종료 스텝 T까지 얻는 보상의 총합 의미
- Gt = Rt+1 + Rt+2 + Rt+3 + --- + RT로 계산, 앞으로 받을 모든 보상을 단순히 더한 값
- 이는 미래 보상을 모두 동일한 가치로 여기는 상황 
```
### 2. 감가이득 Discounted Return
```
- 미래에 받을 봇방의 가치가 시간에 지남에 따라 줄어들도록 감가율 r를 적용해 계산한 이득
- Gt = Rt+1 + rRt+2 + r^2Rt+3 + --- 로 계산
- 여기서 r은 감가율로, 미래의 보상을 현재보다 낮은 가치로 평가하기 위한 파라미터
- 이는 가까운 미래의 보상을 더 중요하게 여기고, 먼 미래의 보상은 덜 중요하게 보는 상황 반영
=> 이득은 단순한 보상의 총합이며, 감가이득은 시간이 지남에 따라 보상의 가치를 줄여가며 계산산
```
## 🟡 감가율 Discounting Rate, r
### 1. 감가율의 정의
```
- 감가율은 미래의 가치를 현재 시점에서 할인하여 계산하는 비율
- 0<=r<=1 범위에서 값을 가짐
- 할인율과 유사한 개념으로, 미래에 받게 될 보상을 현재의 가치로 환산
- 미래가 불확실할수록 감가율이 낮아짐, 먼 미래의 보상은 현재보다 적은 가치로 평가 
```
### 2. 감가율에 따른 이득
```
- 감가율 r=0: 오직 다음 스텝에서 얻는 보상 Rt+1만 고려, 먼 미래의 보상은 완전히 무시
- 감가율 r=1: 모든 미래 보상을 동일한 가치로 평가, Gt는 단순히 모든 보상의 총합
- 적절한 감가율 r: 시간이 지날수록 보상이 감소하여, 현재 가치는 Rt+1 + rRt+2 + r^2Rt+3 + --- 로 계산

=> 감가율은 시간이 지나면서 미래 보상의 가치를 낮추는 역할
감가율이 높으면 미래 보상의 중요도가 높고, 감가율이 낮으면 먼 미래의 보상을 덜 중요하게 
```
## 🟡 감가이득의 재귀적 정의
```
1. 감가이득의 개념
- 감가 이득은 미래의 보상들을 감가율을 적용하여 현재의 가치로 계산하는 방법
- 감가율 r은 시간이 지남에 따라 보상의 가치를 줄이는 비율 나타냄

2. 재귀적 정의
- 감가이득은 현재의 보상 Rt+1이 더해, 나머지 미래 보상들도 감가율을 적용하여 순차적으로 더한 방식
- 미래의 모든 보상들의 총합을 현재 가치로 환산하여 표현, 재귀적으로 계산

3. 핵심 아이디어
- 현재 스텝에서의 이득 Gt는 다음 스텝에서의 보상 Rt+1와 다음 스텝에서의 이득 Gt+1을 감가율을 적용한 형태로 결합한 것
- 현재의 이득은 현재 보상 + 미래 보상의 감가된 합으로 구성된다는 점 
```
![image](https://github.com/user-attachments/assets/3d60f37f-d063-4c67-a891-44334c5aa7fd)

## 🟡 감가이득 예시 
```
마르코프 보상 과정을 기반으로 한 특정 에피소드에서의 감가이득 계산을 보여줌
왼쪽은 상태 전이 다이어그램, 오른쪽은 각각의 에피소드에서 감가이득을 계산한 결과

왼쪽 상태전이 다이어그램 >>
- 학생이 학습 활동 중 각 상태에서 다른 상태로 이동할 때의 전이확률과 보상 보여줌
- 각 상태 전이는 화살표로 연결되어 있고, 전이확률과 보상 R이 함께 표시

오른쪽 에피소드별 감가이득 계산 >>
각 에피소드에서 학생이 어떤 경로로 이동했는지에 따라 감가이득 계산
감가율 r을 적용해 시간이 지남에 따라 미래 보상의 가치를 줄여서 계산한 것 
```
![image](https://github.com/user-attachments/assets/0d1183ae-f6ba-4a15-be6f-b40b6fa0d770)

![image](https://github.com/user-attachments/assets/e992cddc-07a4-4d97-b244-46692ec84088)


## 🟡 마르코프 결정 과정 MDP, Markov Decision Process
```
1. 마르코프 보상 과정에 '행동' 추가
마르코프 결정 과정은 기존의 마르코프 보상 과정에 행동 요소를 추가한 모델
에이전트는 각 시간 스텝에서 특정 행동 At를 선택하게 되며, 이 행동은 다음 상태와 보상에 영향

2. 상태와 보상
에이전트가 선택한 행동 At는 현재 상태 St에서의 보상 Rt와 다음 상태 St+1에 영향 미침
다음 상태와 보상은 이전 상태 St-1과 이전 행동 At-1에 의존, 에이전트의 행동이 미래의 결과 결정

3. 목표
MDP의 목표는 에이전트가 주어진 환경에서 최적의 행동을 선택해 최대의 보상을 얻는 것
이를 위해 강화학습과 같은 알고리즘을 사용해 최적의 정책을 학습
ex) 로봇이 특정 장소에서 점수를 얻는 상황을 가정했을 때, 에이전트가 어느 장소에 갈지 결정하는 문제

=> MDP는 에이전트가 현재 상태에서 어떤 행동을 선택하느냐에 따라 미래 상태와 보상이 달라지는 과정,
최적의 행동을 찾아 최대의 보상을 얻는 것이 목적 
```

## 🟡 마르코프 결정 과정 MDP, markov decision process에서 확률 P
### 1. 확률 p
```
- 환경의 동적 특성을 결정하는 중요한 요소로, 상태 전이와 보상이 결정될 확률
- 에이전트가 특정 상태 St에서 특정 행동 At를 선택했을 때 그에 다라 다음 상
```










