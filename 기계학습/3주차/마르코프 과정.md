
<div align="center">
  <h1> 마르코프 과정</h1>
</div>

## 🟡 확률 과정 Stochastic Processes
### 1. 확률변수 X
```
- 무작위 실험에서 특정 확률로 발생하는 현상을 수치로 표현한 변수
ex) 동전을 두 번 던졌을 때 앞면이 나오는 횟수를 확률변수 X로 나타내며, 0,1,2 중 하나
```
### 2. 표본공간
```
- 모든 가능한 실험 결과들의 집합
- 동전을 두번 던질 경우 s=(앞,앞),(앞,뒤),(뒤,앞),(뒤,뒤)
```
### 3. 확률과정 Stochastic Process
```
- 확률변수의 변화과정을 시간에 따라 추적하는 것
- 시간이 지나면서 확률 변수가 어떻게 변화하는지 중점적으로 다룸
- Xt는 확률변수 X가 시간 t에 따라 변화함을 나타냄
=> 확률과정은 시간에 따라 확률변수의 변화과정을 연구하는 것 ㅅ
```

## 🟡 연속 동전 던지기 게임을 통한 확률 과정
### 1. 연속 동전 던지기 게임
```
- 매시간 스텝마다 동전을 던짐
- 앞면이 나오면 1점을 더하고, 뒷면이 나오면 -1점을 뺌
- 초기점수는 0점에서 시작하며, P(X0=X)=1, 즉 처음 상태에서 확률변수 X0는 0
```
### 2. 시간에 따른 누적 점수
```
시간이 흐르면서 각 시간 스텝 t에서 던진 동전의 결과로 얻는 점수를 누적하여 나타냄
ex)
X0 = -1 or 1 (첫 번째 동전 던지기 결과)
X1 = -2,0,2 (두 번째 동전 던지기 결과)
X2 = -3,-1,1,3 (세 번째 던지기 이후 누적 점수) => 이와 같이 누적
```
### 3. 확률변수 공간
```
누적 점수는 상태집합으로 구성되며, 모든 가능한 누적점수는 무한히 확장
S = { …, -3, -2, -1, 0, 1, 2, 3, …. }
=> 이미지는 10번과 1000번 반복한 동전 던지기 후의 그래프,
누적 점수가 시간에 다라 어떻게 변화하는지? 
```
![image](https://github.com/user-attachments/assets/ff3942b6-ec56-41e1-af94-13f99f36edd5)

## 🟡 마르코프과정 (Markov Process/Markov Chain)
### 1. 마르코프 과정이란?
```
- 시간에 따라 상태가 변화하는 과정을 다루는 이론
- 이때 상태의 변화는 바로 이전 상태에만 의존하는 특징이 있음
즉, 현재 상태가 다음 상태를 결정하는데 있어서, 그 전의 상태들은 중요하지 않다는 것 의미
```
### 2. 상태전이 확률
```
특정시간 t에서 상태 St는 이전 시간 t-1에서의 상태 St-1에만 영향을 받아 결정 => 마르코프 성질
```
### 3. 마르코프 과정의 특징
```
현재 상태가 주어지면 미래 상태는 과거 상태와는 무관하게 결정, 이게 핵심적인 마르코프 성질 
```
### 4. 실생활 예
```
날씨를 예로 들음, 오늘의 날씨가 내일의 날씨에 영향을 미치지만, 그저께의 날씨는 내일의 날씽
영향을 미치지 않는다고 가정하는 것과 같음 오늘의 상태만 알고 있으면 내일의 상태를 예측할 수
있는 것이 마르코프의 성질

=> 마르코프 과정은 상태가 시간에 따라 변화하는데, 그 변화가 바로 이전 상태에만 의존하는 확률과
```
## 🟡 마르코프 과정의 예시
### 1. 상태 집합 S
```
- 주어진 상태들은 {Start,Class1,Class2,Class3,Pass,Pub,Facebook,Sleep}로 구성
- 즉 학생이 수업에 참여하거나, 공부 통과, 술집이나 페북 사용한 뒤 마지막으로 잠으로 가는 과정
```
### 2. 마르코프 과정의 흐름
```
- 과정의 시작은 start에서 시작하고, 각 상태 사이의 화살표는 다른 상태로 전이될 확률 나타냄
- 예를 들어 학생이 c1 -> c2에서 이동할 확률은 0.5, pub -> c1으로 갈 확률은 0.2
- 모든 상태는는 결국 sleep 상태로 끝남
```
### 3. 상태 전이 행렬
```
- 오른쪽의 표는 각 상태에서 다른 상태로 전이될 확률을 수치로 나타낸 행렬
- 행: 현재 상태, 열: 다음 상태, 값: 현재 상태에서 다음 상태로 전이할 확률
ex) start->class1: 1.0, class1->class2: 0.5, class1->facebook: 0.5
class2->class3: 0.8, facebook->facebook: 0.9
- 각 행의 확률 값은 1이 됨, 이는 시스템이 반드시 어떤 상태로 전이한다는 것을 보상
- 값이 0인 경우는 해당 상태에서 다른 특정 상태로 이동할 수 없다는 것 의미
=> 각 상태에서 가능한 다른 상태로의 전이 확률을 통해 시스템의 전반적인 행동 예측 
```
![image](https://github.com/user-attachments/assets/28049bd1-7ddd-43c6-815c-cfad01a3bcd3)

## 🟡 에피소드 episode
### 1. 에피소드란
```
- 시작점부터 끝점까지의 경로로, 하나의 환경에서 에이전트(참여자)가 상호작용을 완료하는 단위
- 특정한 과정이 주어지면, 그 과정에서 발생할 수 있는 여러 가지 가능한 경로들을 에피소드
```
### 2. 마르코프 과정에서의 에피소드
```
- 마르코프 과정에서 시작 상태와 종료 상태가 주어졌을 때, 다양한 경로를 따라 여러 에피소드가 만들어질 수 있음
- 예를 들어 시작이 start이고 종료가 sleep인 경우, 여러 에피소드 존
```
### 3. 예시
```
Episode 1: Start → Class1 → Class2 → Class3 → Pass → Sleep
Episode 2: Start → Class1 → Facebook → Facebook → Class1 → Class2 → Sleep
Episode 3: Start → Class1 → Class2 → Class3 → Pub → Class2 → Class3 → Pass → Sleep
```
## 🟡 에피소드의 종류
### 1. 에피소딕 episodic
```
- 종료상태가 명확히 존재하는 에피소드
- 마지막 타임 스텝 T에 도달하면 끝나는 과정
ex) 시작점과 종료점이 명확환 여정이나 게임과 같은 상황 
```
### 2. 지속적 continuing 
```
- 종료 상태가 존재하지 않는 에피소드
- 과정이 계속 반복되고 끝이 없어서 무한히 지속
ex) 끝없이 진행되는 상태 변화나 순환적인 시스템이 해당 
```
## 🟡 마르코프 보상 과정 Markov Reward Process
```
기존 마르코프 과정에 보상 개념을 추가한 것!

1. 보상 요소 추가
마르코프 과정에 보상이라는 개념을 추가하여, 각 상태에서 얻을 수 있는 보상 포함
상태 St에서의 보상 Rt는 특정 보상 값 r로 표현

2. 보상의 의미
시간에 따라 상태가 변할 때마다 해당 상태에서 얻을 수 있는 보상을 계산하는 과정
ex) 어떤 상태에 도달하면 보상(이익, 점수)을 받는다고 가정

3. 구성 요소
마르코프 보상 과정은 상태전이확률, 보상함수, 그리고 감가율 등을 포함해 설계
감가율 r은 보상이 시간이 지남에 다라 얼마나 가치가 감소하는지 나타냄

=> 마르코프 보상 과정은 상태 변화와 더불어 각 상태에서의 보상을 추적하는 확률 과정,
각 상태에서 얻는 보상과 그 변화의 흐릉을 모델링하는데 사용 
```
## 🟡 마르코프 보상과정을 시각적으로 표현한 다이어그램
```
1. 상태 state
각 원은 상태를 나타내며, "Start", "Class1", "Class2", "Pub", "Facebook", "Sleep", "Pass"
학습하는 과정 중에 학생이 겪는 여러 상태로 볼 수 있음

2. 상태 전이 확률 transition probability
각 화살표는 상태 간의 이동을 나타내며, 화살표 옆 숫자는 한 상태에서 다른 상태로 이동할 확률
ex) "Class1"에서 "Class2"로 갈 확률은 0.5, "Class1"에서 "Facebook"으로 갈 확률은 0.1

3. 보상 reward
각 상태 또는 상태 간 전이에서 얻게 되는 보상 R이 붉은 색으로 표시
ex) "Class1" 상태에서 "Facebook"으로 전이하면 보상은 R=−1.0, "Pass" 상태로 도달하면 
R=10.0의 높은 보상
"Pub" 상태에서 보상은 R=1.0, 반면 "Class2"와 "Class3"에서는 보상이 R=−2.0로,
어려운 학업 상태에서 부정적인 보상을 받는 것

4. 최종 상태
sleep 상태로 가면 R=0.0의 보상을 받으며, 모든 과정이 끝나는 것 의미
pass 상태로 가면 학업을 성공적으로 마쳤다는 의미 R=10.0의 높은 보상 받음

=> 각 상태에서 받을 수 있는 보상 표현
긍정적 행동은 높은 보상, 부정적 행동은 낮은 보상을 받음 
```
![image](https://github.com/user-attachments/assets/dd678f40-f0da-4d04-befd-cf84ef40f5f0)

## 🟡 이득과 감가 이득 Return and Discounted Return
### 1. 이득 Return
```
- 에이전트가 특정 시간 스텝 t부터 종료 스텝 T까지 얻는 보상의 총합 의미
- Gt = Rt+1 + Rt+2 + Rt+3 + --- + RT로 계산, 앞으로 받을 모든 보상을 단순히 더한 값
- 이는 미래 보상을 모두 동일한 가치로 여기는 상황 
```
### 2. 감가이득 Discounted Return
```
- 미래에 받을 봇방의 가치가 시간에 지남에 따라 줄어들도록 감가율 r를 적용해 계산한 이득
- Gt = Rt+1 + rRt+2 + r^2Rt+3 + --- 로 계산
- 여기서 r은 감가율로, 미래의 보상을 현재보다 낮은 가치로 평가하기 위한 파라미터
- 이는 가까운 미래의 보상을 더 중요하게 여기고, 먼 미래의 보상은 덜 중요하게 보는 상황 반영
=> 이득은 단순한 보상의 총합이며, 감가이득은 시간이 지남에 따라 보상의 가치를 줄여가며 계산산
```
## 🟡 감가율 Discounting Rate, r
### 1. 감가율의 정의
```
- 감가율은 미래의 가치를 현재 시점에서 할인하여 계산하는 비율
- 0<=r<=1 범위에서 값을 가짐
- 할인율과 유사한 개념으로, 미래에 받게 될 보상을 현재의 가치로 환산
- 미래가 불확실할수록 감가율이 낮아짐, 먼 미래의 보상은 현재보다 적은 가치로 평가 
```
### 2. 감가율에 따른 이득
```
- 감가율 r=0: 오직 다음 스텝에서 얻는 보상 Rt+1만 고려, 먼 미래의 보상은 완전히 무시
- 감가율 r=1: 모든 미래 보상을 동일한 가치로 평가, Gt는 단순히 모든 보상의 총합
- 적절한 감가율 r: 시간이 지날수록 보상이 감소하여, 현재 가치는 Rt+1 + rRt+2 + r^2Rt+3 + --- 로 계산

=> 감가율은 시간이 지나면서 미래 보상의 가치를 낮추는 역할
감가율이 높으면 미래 보상의 중요도가 높고, 감가율이 낮으면 먼 미래의 보상을 덜 중요하게 
```
## 🟡 감가이득의 재귀적 정의
```
1. 감가이득의 개념
- 감가 이득은 미래의 보상들을 감가율을 적용하여 현재의 가치로 계산하는 방법
- 감가율 r은 시간이 지남에 따라 보상의 가치를 줄이는 비율 나타냄

2. 재귀적 정의
- 감가이득은 현재의 보상 Rt+1이 더해, 나머지 미래 보상들도 감가율을 적용하여 순차적으로 더한 방식
- 미래의 모든 보상들의 총합을 현재 가치로 환산하여 표현, 재귀적으로 계산

3. 핵심 아이디어
- 현재 스텝에서의 이득 Gt는 다음 스텝에서의 보상 Rt+1와 다음 스텝에서의 이득 Gt+1을 감가율을 적용한 형태로 결합한 것
- 현재의 이득은 현재 보상 + 미래 보상의 감가된 합으로 구성된다는 점 
```
![image](https://github.com/user-attachments/assets/3d60f37f-d063-4c67-a891-44334c5aa7fd)

## 🟡 감가이득 예시 
```
마르코프 보상 과정을 기반으로 한 특정 에피소드에서의 감가이득 계산을 보여줌
왼쪽은 상태 전이 다이어그램, 오른쪽은 각각의 에피소드에서 감가이득을 계산한 결과

왼쪽 상태전이 다이어그램 >>
- 학생이 학습 활동 중 각 상태에서 다른 상태로 이동할 때의 전이확률과 보상 보여줌
- 각 상태 전이는 화살표로 연결되어 있고, 전이확률과 보상 R이 함께 표시

오른쪽 에피소드별 감가이득 계산 >>
각 에피소드에서 학생이 어떤 경로로 이동했는지에 따라 감가이득 계산
감가율 r을 적용해 시간이 지남에 따라 미래 보상의 가치를 줄여서 계산한 것 
```
![image](https://github.com/user-attachments/assets/0d1183ae-f6ba-4a15-be6f-b40b6fa0d770)

![image](https://github.com/user-attachments/assets/e992cddc-07a4-4d97-b244-46692ec84088)


## 🟡 마르코프 결정 과정 MDP, Markov Decision Process
```
1. 마르코프 보상 과정에 '행동' 추가
마르코프 결정 과정은 기존의 마르코프 보상 과정에 행동 요소를 추가한 모델
에이전트는 각 시간 스텝에서 특정 행동 At를 선택하게 되며, 이 행동은 다음 상태와 보상에 영향

2. 상태와 보상
에이전트가 선택한 행동 At는 현재 상태 St에서의 보상 Rt와 다음 상태 St+1에 영향 미침
다음 상태와 보상은 이전 상태 St-1과 이전 행동 At-1에 의존, 에이전트의 행동이 미래의 결과 결정

3. 목표
MDP의 목표는 에이전트가 주어진 환경에서 최적의 행동을 선택해 최대의 보상을 얻는 것
이를 위해 강화학습과 같은 알고리즘을 사용해 최적의 정책을 학습
ex) 로봇이 특정 장소에서 점수를 얻는 상황을 가정했을 때, 에이전트가 어느 장소에 갈지 결정하는 문제

=> MDP는 에이전트가 현재 상태에서 어떤 행동을 선택하느냐에 따라 미래 상태와 보상이 달라지는 과정,
최적의 행동을 찾아 최대의 보상을 얻는 것이 목적 
```

## 🟡 마르코프 결정 과정 MDP, markov decision process에서 확률 P
### 1. 확률 p
```
- 환경의 동적 특성을 결정하는 중요한 요소로, 상태 전이와 보상이 결정될 확률
- 에이전트가 특정 상태 St에서 특정 행동 At를 선택했을 때 그에 따라 다음 상태 St+1로 전이,
보상 Rt를 받을 확률을 p(s',r|s,a)로 나타냄
- 이 확률은 현재 상태와 선택한 행동에 의해 결정되며, 미래 상태와 보상이 어떻게 될지 예측하는데 사용
```
### 2. 마르코프 성질
```
다음 상태 St+1와 보상 Rt는 오직 현재 상태 St와 행동 At에만 의존
즉, 과거의 상태는 고려하지 않고 현재만을 기반으로 미래를 결정하는 것이 마르코프 성질
```
### 3. 조건부 확률
```
이 확률은 모든 가능한 상태와 보상에 대한 조건부 확률로 정의되며, 모든 경우의 합이 1이 되는 조건 만족
이는 에이전트가 어떤 상태에서 어떤 행동을 하든, 반드시 하나의 결과로 이어진다는 것을 보장

=> MDP에서 확률 p는 현재 상태와 행동에 따라 다음 상태와 보상이 어떻게 결정되는지를 나타내는
중요한 요소, 이 확률을 통해 에이전트는 최적의 행동 선택 
```
## 🟡 상태 전이 확률과 보상함수
### 1. 상태 전이 확률
```
- 상태 전이 확률은 에이전트가 특정 상태 St에서 특정 행동 At를 했을 때, 다음 상태 St+1로 전이될 확률
- 수식으로는 p(s'|s,a)로 표현, 이는 현재 상태와 행동에 따라 다음 상태가 결정되는 확률
- 상태 전이는 기본적으로 확률적 요소가 포함되며, 이를 통해 시스템의 미래 상태 예측 
```
### 2. 보상함수
```
- 보상함수는 특정 상태 s에서 특정 행동 a를 했을 때 기대할 수 있는 기대 보상을 의미
- 수식으로는 r(s,a)로 표현되며, 이는 상태와 행동에 따라 얻게 될 보상의 평균값 계산
이 보상은 모든 가능한 상태 전이와 보상에 대한 확률을 기반으로 계산

=> 상태 전이 확률은 에이전트가 현재 상태에서 행동을 선택했을 때 다음 상태로 전이될 확률을 나타내고,
보상함수는 그 상태와 행동에 따른 기대 보상을 계산하는 함수
이를 통해 에이전트는 최적의 행동을 선택하는데 필요한 정보 얻음
```
## 🟡 마르코프 결정 과정의 주요 구성 요소
### 1. MDP의 구성 요소
```
- 상태 집합 S: 에이전트가 처할 수 있는 모든 상태들의 집합
- 행동 집합 A: 각 상태에서 에이전트가 선택할 수 있는 가능한 행동들의 집합
- 상태 전이 확률 p: 에이전트가 특정 상태 s에서 행동 a를 했을 때, 다음 상태 s'로 전이될 확률
- 보상 함수 r(s,a): 특정 상태에서 특정 행동을 했을 때 에이전트가 받게 되는 기대 보상
- 감가율 r: 시간이 지나면서 미래 보상의 가치를 얼마나 할인할지 나타내는 비율
- 초기확률 u: 에이전트가 처음 시작할 상태가 어떤 상태일지 결정하는 확률 분포
```
### 2. 상태전이확률 p(s'|s,a)
```
에이전트가 현재 상태에서 어떤 행동을 했을 때, 다음 상태로 전이될 확률 나타냄
이는 상태 변화의 불확실성을 반영하며, MDP에서 중요한 요소
```
### 3. 보상함수 r(s,a)
```
각 상태에서 에이전트가 어떤 행동을 했을 때 기대할 수 있는 보상
이 보상은 에이전트가 특정 목표를 달성하는 데 얼마나 가까워지는지 평가하는데 사용 
```
## 🟡 마르코프 결정 과정 (MDP)
```
1. 상태: 각 원은 에이전트가 있을 수 있는 상태
"Start", "Class1", "Pub", "Facebook", "Sleep", "Pass" 등이 상태

2. 행동
각 상태에서 에이전트는 행동을 선택할 수 있음, "Class1" 상태에서 에이전트는 "Study" 또는 "Play"를 선택
행동 선택에 따라 다음 상태로 전이
 "Class1"에서 "Study"를 선택하면 "Class2"로, "Play"를 선택하면 "Facebook"으로 전이

3. 상태 전이 확률
각 화살표 옆에 있는 숫자는 상태 전이 확률
"Class1"에서 "Study"를 선택할 경우 50% 확률로 "Class2"로 이동

4. 보상
각 상태나 행동에 따른 보상 주어짐
"Class1"에서 "Study"를 하면 보상은 R=−2.0이고, "Pass" 상태에 도달하면 R=10.0의 높은 보상

5. 목표
에이전트의 목표는 최적의 행동을 선택하여 장기적으로 최대의 보상 얻는 것
공부를 통해 pass상태에 도달하여 높은 보상을 얻는 것이 목표 
```

![image](https://github.com/user-attachments/assets/d7b7ee9f-e535-4d89-a9dc-fa1fc8ced798)

## 🟡 보상 기반 강화학습의 목적
```
1. 행동의 추가
마르코프 보상 과정에 행동이라는 요소 추가, 에이전트는 각 시간 스텝에서 특정 행동 At를 선택

2. 보상 상태
- 에이전트가 선택한 행동에 따라 보상 Rt를 어고, 그 보상과 다음 상태 St+1가 결정
- 이 보상과 상태는 이전 상태 St-1와 이전 행동 At-1에만 의존, 현재 상태와 행동이 미래 결과를 결정

3. 확률적 과정
에이전트가 행동을 취하고 상태가 변할 확률은 확률 변수로 나타나며, 이 확률 과정에 따라 미래 상태와 보상 결정 
```

## 🟡 보상 기반 강화학습의 목적
```
1. 강화학습의 목적
마르코프 결정 과정으로 표현하며, 각 시간 스텝에서 받는 보상의 누적 합을 최대화하는 것이 목표
보상을 최대화한다는 것은 단기적인 보상 뿐만 아니라 미래의 보상까지 고려하여 장기적으로 높은 보상 얻는 것 의미

2. 보상값의 역할
- 강화학습에서 보상은 에이전트가 어떤 행동이 문제 해결에 가까운지를 알려주는 역할
- 보상은 문제의 목표를 직접적으로 가르쳐주는 것이 아니라, 목표에 가까워지는 방법 간접적으로 알려주는 수단

3. 게임의 예시
게임에서 상대방의 말을 잡아먹을 때마다 긍정적인 보상을 설정할 경우,
에이전트는 상대 말을 잡아먹는 데만 집중할 수 있음 즉 경기를 이기는 전략 대신 보상을
많이 얻는 행동만 하려 할 수 있다는 것 
```

## 🟡 보상 기반 강화학습에서 보상을 활용하는 예시
```
1. 미로 탈출 로봇
로봇이 미로에서 출구를 찾아 나가야 하는 상황에서, 로봇은 시간이 지날 때마다 -1 보상 받음
이는 로봇이 빠르게 탈출하도록 유도하는 보상 구조

2. 재활용 수거 로봇
로봇이 빈 깡통을 수거할 때마다 +1의 보상을 받음
이는 더 많은 깡통을 수거하도록 로봇을 유도하는 보상 시스템

3. 자율 주행 자동차
장애물에 부딪히면 음의 보상을 받기 때문에, 자동차는 장애물을 피하려는 행동을 학습 

4. 대결 게임
경기를 이기면 양의 보상을 얻으며, 이는 이기는 전략을 학습하도록 유도하는 역할 
```

## 🟡 에이전트 정책
### 1. 결정적 정책 Deterministic Policy
```
- 정확한 행동이 상태에 대해 하나로 결정되는 정책
- 에이전트가 특정 상태 s에 있을 때, 정책에 따라 반드시 특정 행동 a를 선택
즉, 하나의 상태에 대해 하나의 행동이 결정, 수식으로는 ㅠ(s)=a
```

### 2. 확률적 정책 Stochastic Policy
```
- 확률에 따라 행동이 결정되는 정책
- 에이전트가 상태 s에 있을 때, 여러 행동을 확률적으로 선택
즉, 상태에 따라 행동이 확률적으로 결정, 수식으로는 ㅠ(a|s)로, 상태 s에서 행동 a가 선택될 확률 
```

## 🟡 에이전트 정책
```
1. 결정적 정책 Deterministic Policy
- 에이전트가 상태 low에 있을 때, 반드시 recharge 행동을 선택하고,
상태 high에 있을 때 search 행동을 선택함
- 즉 상태에 따라 하나의 행동이 정확하게 결정
- ex) ㅠ(low) = recharge, ㅠ(high) = search 

2. 확률적 정책 Stochastic Policy
- 상태에 다라 행동이 확률적으로 결정
- 상태 low에서 에이전트는 50%확률로 recharge, 40% 확률로 wait, 10% 확률로 search 선택
- 상태 high에서는 90% 확률로 search, 10% 확률로 wait 선택
- ex) ㅠ(recharge|low)=0.5, ㅠ(wait|low)=0.4, ㅠ(search|low)=0.1

3. 결정적 정책과 확률적 정책의 기술 방식
- 결정적 정책도 확률적 정책처럼 기술할 수 있음
ex) 예를 들어, 상태 low에서는 100% 확률로 recharge를 선택, 나머지 행동은 0% 확률로 선택
ㅠ(recharge|low)=1.0, ㅠ(wait|low)=0.0, ㅠ(search|low)=0.0

=> 결정적 정책은 특정 상태에서 하나의 행동이 확실되는 반면, 확률적 정책은 삳태에서 여러 행동이 확률적으로 결정 
```
![image](https://github.com/user-attachments/assets/5ee542c2-da95-49de-95f3-0eb060461ecd)

## 🟡 에이전트의 정책과 강화학습 과정
### 1. 에이전트 정책
```
- 정책은 에이전트가 현재 상황에서 어떤 행동을 선택할지 결정하는 규칙
- 에이전트는 환경을 관찰하고, 그에 따라 상태를 인식한 후 정책을 사용해 행동을 결정 
```
### 2. 강화학습
```
- 에이전트는 강화학습 알고리즘을 통해 더 나은 정책 학습
즉, 시간이 지남에 따라 미래 보상을 최대화할 수 있는 최적의 정책 찾음
- 에이전트가 행동을 할 때마다 보상을 받으며, 이 보상을 바탕으로 정책을 계속해서 개선

=> 에이전트는 현재 상태에서 정책에 따라 행동을 선택하고,
행동의 결과로 받은 보상을 바탕으로 정책을 학습하여 더 나은 행동을 선택하는 방법 배움  
```
![image](https://github.com/user-attachments/assets/110718e5-aa9d-4d0f-b195-6bde1c66125a)

## 🟡 가치함수의 정의와 역할
```
1. 가치
- 주어진 환경에서 에이전트가 얻을 수 있는 보상과 연관된 중요도 읨
- 특정 상태에서 미래에 얻을 수 있는 보상의 기대값

2. 가치함수
- 가치는 상태 또는 상태-행동 쌍에 의해 결정
즉, 특정 상태에 있을 때 또는 특정 행동을 했을 때 미래에 얻을 수 있는 보상의 예상 값
- 이 함수는 상태나 행동을 입력으로 받아, 그 상태에서의 가치 계산

3. 강화학습에서의 역할
- 대부분의 강화학습 알고리즘은 이 가치 함수를 기반으로 설계
에이전트가 어떤 행동을 해야 하는지 결정할 때 가치함수를 사용해 최적의 정책 학습

=> 가치함수는 에이전트가 특정 상태나 행동에서 얻을 수 있는 보상의 기대값을 계산하는 함수,
강화학습의 알고리즘과 정책 설계의 핵심 역할 
```

![image](https://github.com/user-attachments/assets/f062e2f6-a907-42b7-bd35-4877bbd3cb84)

## 🟡 가치함수의 종류
```
1. 상태 가치 함수 state-value function
- 에이전트가 특정 상태에 있을 때, 그 상태 자체가 얼마나 좋은지 평가하는 함수
- 즉 현재 상태가 얼마나 유익한 상태인지 나타냄

2. 행동 가치 함수 action-value function
- 에이전트가 특정 상태에서 어떤 행동을 선택했을 때 그 행동이 얼마나 유익한지를 나타내느 함수
- 상태 뿐만 아니라 그 상태에서 선택한 행동의 가치 평가

=> 상태 가치 함수에서는 상태 자체의 가치를, 행동 가치 함수는 상태에서의 특정 행동 가치를 나타내는 함수 
```
## 🟡 에이전트 정책의 보상 활용 예시
```
1. 미로 탈출 로봇
로봇이 미로에서 출구를 찾아나가는 과정에서 매 타임 스텝마다 -1의 보상을 받음
이를 통해 로봇이 빠르게 탈출하도록 유도

2. 재활용 수거 로봇
빈 깡통을 수거할 대마다 +1의 보상을 받음, 이는 더 많은 깡통을 수거하도록 로봇 장려

3. 자율 주행 자동차
장애물에 부딪히면 음의 보상을 받아, 자동차가 장애물을 피하도록 유도

4. 대결 게임
경기를 이기면 양의 보상을 받으며, 이는 이기는 전략을 학습하도록 유도하는 역할

각 예시는 에이전트가 주어진 목표를 달성하기 위해 보상을 활용해 행동을 학습하는 방식 
```
## 🟡 백업 다이어그램 
```
1. 백업 다이어그램 Backup Diagram
- 상태 가치와 행동 가치를 계산하는 과정을 시각적으로 나타낸 다이어그램
- 에이전트가 상태에서 행동을 선택하고, 그 결과로 다음 상태로 전이되며, 보상을 얻는 과정 표현

2. 상태 가치 State Value
- 왼쪽 트리는 특정 상태에서 가능한 모든 행동들의 결과를 고려하여 그 상태 자체의 가치를 계산
- 상태 s에서 정책 ㅠ에 따라 행동 a를 선택하고, 그로 인해 상태 s'로 전이되는 과정

3. 행동 가치 Action Value
- 오른쪽 트리는 상태와 함께 그 상태에서 선택한 특정 행동들이 얼마나 유용한지 계산
- 상태 s에서 행동 a를 선택했을 때, 보상과 함께 다음 상태로 전이되는 결과를 고려해 행동 가치 계산 
```

![image](https://github.com/user-attachments/assets/4229edc7-ea25-4b79-b2a9-f106aa236e4c)


## 🟡 상태 가치 함수 Vㅠ(s)
### 1. 상태 가치 함수
```
- 특정 상태 s에서 정책 ㅠ를 따를 때, 에이전트가 앞으로 얻을 수 있는 기대 이득의 합
- 이는 에이전트가 상태 s에 있을 때, 정책에 따라 계속 행동했을 때 미래에 받을 보상의 합
```
### 2. 그리드월드에서의 상태 가치
```
- 그리드월드 환경에서 각 상태의 가치 보여줌
- 좌측 상단의 상태는 -6의 가치, 이는 해당 상태에서 앞으로 얻을 수 있는 보상의 합이 -6임을 의미 
```
![image](https://github.com/user-attachments/assets/42287d02-cfbe-4873-93cd-cd2111a74fe0)

## 🟡 행동 가치 함수 qㅠ(s,a)
### 1. 행동 가치 함수
```
- 특정 상태 s에서 특정 행동 a를 선택한 후, 정책 ㅠ에 따라 계속 행동했을 때
에이전트가 얻을 수 있는 기대 이득(미래의 총 보상) 나타냄
- 상태에서 행동을 선택한 후 미래 보상을 계산하여, 해당 행동이 얼마나 좋은지 평가하는 함수
```
### 2. 행동 선택에 도움
```
- 이 함수는 에이전트가 특정 상태에서 여러 행동들 중 어떤 행동이 가장 좋은지 선택할 수 있도록 도와줌
- 각 상태에서 가능한 모든 행동들의 가치를 계산하여, 에이전트가 최선의 행동 선택할 수 있게 함
```
## 🟡 행동 가치 함수
### 1. 행동 가치 함수 정의
```
- 특정 상태에서 특정 행동을 선택했을 때, 그 행동을 수행한 후에 정책에 따라 계속 행동했을 때
얻을 수 있는 기대되는 이득(보상) 나타내는 함수
- 어떤 상태에서 행동을 선택한 후 앞으로 받을 보상의 기대값 평가 
```
### 2. 그리드월드 예시
```
- 그림에서 각 상태는 그 상테에서 선택할 수 있는 행동들에 대한 가치 표시
- 각 상태의 화살표는 해당 상태에서 가능한 행동 방향 위, 아래, 좌, 우 나타내며,
숫자는 그 행동을 선택했을 때 예상되는 가치
- 예를 들어, 상태에서 오른쪽으로 읻ㅇ하면 4의 가치 얻고, 위쪽으로 이동하면 2의 가치 얻
```
![image](https://github.com/user-attachments/assets/43ae9da0-7a7e-4232-9651-570ee8e8c0ba)

## 🟡 벨만 방정식 Bellman Equation
### 1. 벨만 방정식
```
- 현재 상태에서의 가치(미래에 받을 보상의 기대값)를, 현재 보상과 다음 상태에서의 가치로 표현한 재귀적 관계식
- 현재 상태의 가치는 현재 얻을 보상과 미래 상태에서 얻을 보상의 합으로 계산
```
### 2. 현재 가치와 미래 가치의 관계
```
에이전트가 현재 상태에서 얻는 보상과, 다음 상태로 전이된 후의 미래 가치를 감가율
(미래 보상의 가치를 조금 낮게 평가하는 정도)를 적용해 합친 것이 현재 상태의 가치

=> 벨만 방정식은 현재 상태의 가치가 미래 상태의 가치와 어떻게 연결되는지 나타냄
이를 통해 에이전트는 미래 보상을 고려한 최적의 행동 선택
벨만 방정식은 현재 상태와 미래 상태의 가치 사이의 관계를 정의하여,
에이전트가 각 상태에서 얻을 수 있는 최적의 보상을 계산하는 방법 제공 
```

## 🟡 벨만 방정식 Bellman Equation
```
1. 벨만 방정식이란
현재 상태에서의 가치를 계산하는 공식
현재 상태에서 얻는 보상과, 그 이후의 미래 상태에서 얻을 수 있는 가치를 결합한 형태

2. 상태 전이 확률이 1일 때
상태 전이 확률이 1이라는 것은 에이전트가 특정 행동을 했을 때 다음 상태로 전이될 확률이 100%
즉 행동을 하면 결과가 확실히 결정된다는 상황

3. 벨만 방정식의 계산 과정
- 에이전트는 정책에 따라 어떤 행동을 선택
각 행동의 결과로 얻는 보상과 그 후의 미래 상태의 가치를 합하여 현재 상태의 가치 계산
- 각 행동이 일어날 확률과 그 행동이 가져다줄 보상을 모두 고려해,
현재 상태에서의 최적의 기대 가치를 계산하는 방식 
```
## 🟡 벨만 방정식을 행동 가치 함수에 적용한 내용
```
1. 행동 가치의 재귀적 정의
- 행동 가치 함수는 에이전트가 특정 상태에서 특정 행동을 했을 때, 그 행동 이후에 얻을 수 있는 기대되는 보상 계산
- 여기에서 벨만 방정식은 현재 행동으로 얻는 보상과, 그 후의 미래 상태에서의 기대 가치를 결합해
행동 가치를 평가

2. 미래 가치와 현재 보상의 관계
- 에이전트가 행동을 취했을 때 바로 얻는 보상과 그 다음 상태에서의 행동 가치(미래 보상 포함)
를 감가율을 적용해 계산
- 즉 현재 행동으로 얻는 보상과 그 행동 이후에 도달하는 상태에서의 행동 가치 (미래 보상)를
더해서 행동 가치를 평가하는 방식

=> 행동 가치 함수의 벨만 방정식은 현재 행동으로 얻는 보상과 미래 상태에서의 기대 가치를
재귀적으로 계산해, 에이전트가 특정 상태에서 특정 행동을 했을 때의 가치 평가하는 방법 
```
## 🟡 벨만 방정식을 행동 가치 함수에 적용한 경우
```
1. 행동 가치 함수의 벨만 방정식
- 에이전트가 특정 상태에서 특정 행동을 했을 때, 그 행동으로부터 얻게 될 보상을 계산하는 방법
- 이 계산은 현재 얻을 수 있는 즉시 보상과 그 다음 상태에서의 미래 가치(다음 상태에서의
행동을 얻을 수 있는 보상)을 결합하여 이루어짐

2. 상태 전이 확률이 1일 때
- 상태 전이 확률이 1이라는 것은, 어떤 상태에서 행동을 하면 다음 상태로 확실하게 전이된다는 의미
즉 에이전트가 행동을 선택하면 그에 따른 결과가 확정된다는 가정
- 이 상황에서는 계산이 단순해지며, 다음 상태에서의 기대 가치에 현재 보상을 더해 계산

3. 재귀적 계산
- 현재 상태에서 행동을 했을 때의 가치는 현재 얻을 보상과, 다음 상태에서 최적의 행동을 통해
얻게 될 미래 가치를 합산하여 계산
- 이렇게 현재와 미래의 가치를 함께 고려하는 방식으로 행동 가치가 계산 
```
## 🟡 상태 가치 함수와 행동 가치 함수 간의 관계
```
1. 상태 가치 함수 vㅠ(s):
특정 상태 s에서 정책 ㅠ를 따를 때, 그 상태에서 앞으로 얻을 수 있는 기대 보상의 평균 값

2. 행동 가치 함수 qㅠ(s,a):
특정 상태 s에서 특정 행동 a를 선택했을 때 얻을 수 있는 기대 보상

3. 상태 가치와 행동 가치의 관계
상태 가치 vㅠ(s)는 해당 상태에서 가능한 모든 행동 a들에 대한 행동가치의 평균 값으로 계산
여기서 각 행동 a가 선택될 확률 (정책에 따라 결정될 확률) ㅠ(a|s)에 따라 가중 평균 구하는 방식

=> 상태 가치는 해당 상태에서 에이전트가 취할 수 있는 여러 행동들의 기대 보상의 가중평균,
각 행동의 가치는 그 행동을 선택했을 때 얻을 수 있는 보상의 기대값 
```
## 🟡 정책의 대소 비교와 최적 정책
```
1. 정책의 대소 비교
- 정책 ㅠ'가 정책 ㅠ보다 더 좋다고 할 수 있는 기준은 "모든 상태에서 기대되는 누적 보상
(상태 가치)"가 더 큰 경우
- 즉 같은 상태에서 두 정책을 비교할 때, 정책 ㅠ'가 더 나은 보상을 가져다 준다면 ㅠ'는 ㅠ보다 좋은 정책

2. 최적 정책
- 최적 정책은 모든 상태에서 기대되는 보상이 가장 큰 정책
즉 에이전트가 어떤 상태에 있든, 그 상태에서 가장 높은 보상을 얻을 수 있는 행동을 선택하는 정책

3. 그리드 월드
- 이미지의 하단에는 두 가지 정책이 그리드월드 상태에서 어떻게 적용되는지 보여줌
- 왼쪽은 정책 ㅠ에 따른 상태 가치, 오른쪽은 정책ㅠ'에 따른 상태가치
정책 ㅠ'에서 상태 가치가 모든 상태에서 더 크기 때문에 정책 ㅠ'가 더 좋은 정책 
```

![image](https://github.com/user-attachments/assets/63a0cee5-0e2a-4800-8863-7f1db3ba3356)

## 🟡 최적 정책
```
1. 최적 정책
최적 정책은 "주어진 마르코프 결정 과정"에서 가장 좋은 정책 의미
즉, 에이전트가 어떤 상태에서든 그 상태에서 선택할 수 있는 행동 중에서
가장 큰 기대 보상을 얻을 수 있는 정책
최적 정책 ㅠ'는 모든 다른 정책 ㅠ보다 기대되는 보상이 큼

2. 최적 정책의 특징
"기대 누적 보상(상태 가치)"가 가장 큰 정책
즉 에이전트가 각 상태에서 최적의 행동을 선택함으로써 미래에 얻을 수 있는 보상이 가장 크도록 하는 정책
주어진 MDP에서는 최소 한 개 이상의 최적 정책 존재할 수 있음

3. 그리드월드 예
그림에서 보이는 화살표들은 각 상태에서 최적 정책에 따라 에이전트가 이동해야 할 방향 나타냄
최적 정책은 각 상태에서 에이전트가 목표에 가장 빠르게 도달하도록 유도하는 방향 가리킴 
```
![image](https://github.com/user-attachments/assets/e53f6bec-e3c6-416d-af73-18c723a65407)

## 🟡 최적 가치 함수
```
1. 최적 상태 가치 함수
- 최적 상태 가치 함수는 최적 정책을 따랐을 때 각 상태에서 얻을 수 있는 최대 기대 보상
- 즉 에이전트가 최선의 행동을 선택할 때, 그 상태에서 받을 수 있는 보상의 기대값
- 모든 상태에서의 최적 가치를 나타내며, 이를 통해 에이전트는 상태가 얼마나 좋은지 평가

2. 최적 행동 가치 함수
- 최적 행동 가치 함수는 최적 정책에 따라 특정 행동을 선택했을 때 얻을 수 있는 최대 기대 보상
- 즉 특정 상태에서 최적의 행동을 했을 때 미래에 얻을 수 있는 보상의 기대값
- 상태뿐만 아니라, 각 행동에 대한 가치를 평가하여 에이전트가 어떤 행동을 선택해야 하는지 알 수 있게 함 
```
## 🟡 최적 상태 가치 함수와 최적 행동 가치 함수 사이의 관계
```
1. 최적 행동 가치 함수
- 최적 행동 가치 함수는 특정 상태에서 특정 행동을 선택했을 때, 앞으로 얻을 수 있는 최대 기대 보상
- 즉 에이전트가 특정 상태에서 행동을 선택한 후 얻게 될 즉시 보상과 미래의 최적 상태 가치를 종합해 계산
- 미래 상태로 전이될 확률을 고려해 보상과 미래 가치를 더한 값이 현재 행동의 가치 나타냄

2. 최적 상태 가치 함수
- 최적 상태 가치 함수는 어떤 상태에서 선택할 수 있는 행동 중 가장 큰 보상을 주는 행동을 선택했을 때 얻을 수 있는 최대 기대 보상
- 즉 현재 상태에서 할 수 있는 모든 행동에 대해 계산된 최적 행동 가치들 중 가장 높은 값
선택 -> 그 상태의 가치 결정

요약 >>
최적 행동 가치함수는 특정 행동을 했을 때의 기대 보상을 나타내고,
최적 상태 가치함수는 ""모든 가능한 행동 중 가장 큰 기대 보상을 주는 행동""을 선택했을 때
그 상태에서 얻을 수 있는 보상 나타냄  
```

## 🟡 최적 상태 가치 함수와 그에 대한 벨만 최적 방정식
```
1. 최적 상태 가치 함수
최적 상태 가치함수는 에이전트가 특정 상태에 있을 때, 그 상태에서 가능한 행동들 중에서
가장 큰 기대 보상을 주는 행동을 선택했을 때 얻을 수 있는 최대 기대 보상 나타냄
즉 에이전트가 최선의 행동을 선택했을 때 그 상태에서 얻을 수 있는 최대 보상

2. 벨만 최적 방정식
벨만 방정식은 현재 상태에서의 가치를 미래 상태에서의 가치와 연결하는 재귀적인 식
여기서 최정 상태 가치는 모든 가능한 행동들 중에서 가장 큰 보상을 주는 행동을 선택하는 방식
상태에서 행동을 하면, 그 행동에 따른 보상과, 다음 상태에서의 가치를 더한 것이 현재 상태의 가치

3. 최적 행동 선택
최적 상태 가치 함수는 모든 가능한 행동들에 대한 기대 보상을 계산한 후, 그 중 가장 큰 보상을 주는 행동 선택
어떤 상태에서든 최선의 선택을 하여 최대 보상을 얻는 방식으로 상태 가치를 계산 
```

## 🟡 최적 행동 가치 함수와 벨만 최적 방정식
```
1. 최적 행동 가치 함수
특정 상태에서 특정 행동을 했을 때 얻을 수 있는 최대 기대 보상을 계산하는 함수
에이전트가 어떤 상태에서 어떤 행동을 했을 때 얻을 수 있는 보상과, 그 후의 최적 상태에서
얻을 수 있는 최대 보상을 결합하여 계산

2. 벨만 최적 방정식
현재 상태에서 행동을 했을 때 얻을 수 있는 보상과, 그 이후에 얻을 수 있는 최대 보상 결합하는 방식
상태에서 행동을 하면 즉시 얻을 보상과 다음 상태에서의 최적 행동으로 얻게 될 보상을 결합해
행동의 가치를 계산

3. 최적 행동 선택
미래의 보상은 다음 상태에서 가능한 모든 행동들 중에서 최대 보상을 주는 행동을 선택한 결과 반영
따라서 현재 상태에서의 행동 가치는 현재 보상과 다음 상태에서의 최선의 행동에 따른 미래 보상 합친 값 
```

## 🟡 최적 가치 함수와 최적 정책

```
1. 최적 정책
주어진 상태에서 최적 행동 가치 함수를 알고 있다면, 에이전트는 탐욕적 방식으로 최적의 행동 선택
즉 에이전트는 가장 높은 보상을 주는 행동을 즉시 선택하여, 현재 상태에서 최적의 행동 결정

2. 결정적 최적 정책
만약 특정 상태에서 하나의 최적 행동만 존재한다면, 그 행동을 선택하는 방식으로 최적 정책 결정

3. 비결정적 최적 정책
특정 상태에서 여러 최적 행동이 존재할 경우, 각 행동을 균등한 확률로 선택
이는 여러 행동들이 동일한 기대 보상을 가질 때 발생할 수 있는 상황

4. 탐욕적 방식
에이전트는 가능한 행동들 중에서 가장 행동 가치를 주는 행동을 선택하는 방식으로 최적 정책 형성 
```
### 최적 행동을 선택할 때 가능한 경우의 수
```
1. 하나의 최적 행동만 있는 경우
- 특정 상태에서 최적 상태의 가치를 최대화하는 행동이 하나만 존재하는 경우
- 즉 그 상태에서 최선의 행동이 명확하게 하나로 결정, 에이전트는 그 행동만을 선택

2. 여러 최적 행동이 있는 경우
- 특정 상태에서 최적 상태 가치를 최대화할 수 있는 행동이 여러 개 존재할 수 있음
- 이 경우 여러 행동들이 동일한 기대 보상을 주며, 에이전트는 이들 중 어느 행동을
선택하더라도 최적의 결과 얻음 
```

## 🟡 마르코프 결정 과정 MDP
```
1. MDP 문제의 해결
MDP 문제를 해결하기 위해서는 최적 행동 가치 함수를 알아야 함
최적 행동 가치 함수는 특정 상태에서 가능한 행동 중 어떤 행동이 가장 큰 기대 보상을 주는지 계산해주는 함수

2. 최적 정책 노출
최적 행동 가치 함수를 알게 되면, 그 상태에서 어떤 행동을 해야할지 알게 되어 최적 정책 도출
최적 정책은 에이전트가 주어진 상태에서 가장 큰 보상을 얻을 수 있는 행동을 선택하도록 안내하는 규칙

3. MDP 문제 해결
최적 행동 가치와 최적 정책을 구함으로써, MDP문제 해결
주어진 환경에서 에이전트가 어떤 행동을 해야 가장 좋은 결과를 얻는지 결정할 수 ㅣㅇㅆ음

4. 그리드 월드 예시
이미지의 그리드월드 그림에서는 각 상태에서 가능한 행동의 가치가 숫자로 표시,
그 중에서 최적의 행동을 선택하여 문제를 해결하는 과정 보여줌

=> 만약 최적 행동 가치를 알면, 최적 정책을 이끌어 낼 수 있고
이는 곧 주어진 MDP문제를 해결했다고 볼 수 있음 
```

![image](https://github.com/user-attachments/assets/5f3b2e38-c7d5-49cc-b82a-e213da17b3f1)

## 🟡 최적 정책 계산의 어려움
```
1. 벨만 최적 방정식의 가정
벨만 최적 방정식을 사용하여 최적 정책을 계산할 때는 몇 가지 중요한 가정 필요
첫째, 환경의 동작 특성을 나타내는 상태 전이 확률 p를 정확하게 알고 있어야 함
에이전트가 어떤 행동을 했을 때, 그 결과로 어떤 상태로 전이될지 예측할 있다는 의미
둘째, 각 상태 전이에서 얻을 수 있는 보상 값을 정확하게 알고 있어야 함
에이전트가 행동을 했을 때 어떤 보상을 받을지를 알아야 함
셋째, 문제는 마르코프 속성을 가져야 함
현재 상태와 행동만으로 미래 상태가 결정되어야 하며, 과거 상태는 중요하지 않음

2. 계산 자원의 필요성
최적 정책을 계산하려면 충분한 계산 성능을 가진 자원과 메모리가 필요
복잡한 MDP 문제에서는 계산이 매우 어려워질 수 있기 때문에,
자원이 제한적이면 최적 정책을 계산하는데 어려움을 겪을 수 있음 
```
## 🟡 Q 테이블을 통해 벨만 최적 방정식 적용한 예시
```
1. 게임보드 Game Board
그림의 왼쪽의 보드에는 에이전트(자동차)가 목표(컵)으로 이동하는 상황
현재 상태는 000 010으로 표시되어 있어, 이는 현재 에이전트가 보드에서 위치한 상태 의미

2. Q 테이블(Q table)
오른쪽 표는 각 상태에서의 행동 가치(Q값)을 나타냄
각 행은 특정 상태를 나타내고, 각 열은 해당 상태에서의 가능한 행동에 대한 가치
ex) 000 100 상태에서 위로 이동했을 때의 가치는 0.3,
아래로 이동했을 때의 가치는 -0.5로 나타남,
이 값들은 에이전트가 해당 행동을 했을 때 얻을 수 있는 예상 보상

3. 벨만 방정식과 상태-행동 가치 계산
벨만 방정식은 각 상태에서 에이전트가 최적의 행동을 선택하여 얻을 수 있는 최대보상을 계산하는 과정
Q 테이블을 사용하면 각 상태에서 어떤 행동이 최적인지 선택할 수 있으며,
이는 가능한 행동들 중 가장 높은 Q 값을 가진 행동을 선택하는 방식

4. Q 테이블의 어려움
상태의 개수가 상대적으로 적은 Q 테이블을 사용하는 경우
상태와 행동의 개수가 적으면 계산이 간단하지만, 수가 많아지면 테이블을 구성하고 계산하는 것이 어려움 
```
![image](https://github.com/user-attachments/assets/952d9afb-3113-4226-80a2-74fd77c68b39)

## 🟡 최적 정책 계산의 어려움
```
1. 테이블 기반의 최적 정책 계산 어려움
최적 정책을 계산하기 위해서는 모든 상태와 행동에 대해 가능한 모든 값을 계산하고 탐색하는 과정 필요
이 과정은 완전 탐색이기 때문에 시간이 오래 걸리고 복잡

2. 벨만 방정식의 복잡성
벨만 방정식을 사용하여 최적 정책을 구할 때, 계산 과정이 복잡하며,
특히 상태와 행동의 수가 많을 경우 계산에 필요한 공간과 시간이 급격히 증가

3. 상태가 많을 때의 문제
상태의 개수가 많을 때는 모든 상태에서 최정 정책을 구하는 것이 비현실적

4. 해결방법 - 딥러닝
복잡성을 해결하기 위해 딥러닝이 제시됨
딥러닝은 복잡한 계산을 효율적으로 처리할 수 있으며, 많은 상태와 행동이 존재하는
문제에서도 최적 정책을 학습할 수 있음
=> 최적 정책 계산은 상태와 행동의 수가 많을 수록 매우 어려워지며,
이를 해결하기 위해 딥러닝 같은 기술을 활용해 복잡한 문제 처리 
```

<div align="center">
  <h1> 최적 정책과 최적 가치 함수 실습</h1>
</div>

## 🟡 자료의 구성(자료구조)와 최적 가치 함수 계산
```
1. value_function
: 첫 번째 테이블인 value_function은 현재 상태에서의 가치를 저장하는 배열
이 배열은 상태의 그리드로 이루어져 있으며, 초기값 0
각 셀은 특정 상태에서의 가치를 나타내며, 이 값은 에이전트가 해당 상태에서 얻을 수 있는 기대보상

2. new_value_function
: 두 번째 테이블인 new_value_function은 value_function의 값을 업데이트하는데 사용
새로운 값을 계산하여 현재 상태의 가치가 업데이트될 때, new_value_function이 활용
이후 value_function과 new_value_funcion의 값을 비교하여 최적 가치를 찾는 과정

3. 최적 가치 계산 과정
value_function과 new_value_function을 반복하면서, 상태의 가치가 수렴할 때까지 진행
이를 통해 각 상태에서 최적 가치 찾아냄 

```
![image](https://github.com/user-attachments/assets/ec197e7a-a90a-4128-b3e3-8b64936ec40e)


## 🟡 웜홀이 있는 그리드월드 환경에서 상태별 가치 산출 
```
1. 그리드 월드 환경
그리드 월드는 격자 형태의 환경, 에이전트가 각 격자 칸에서 다른 칸으로 이동하며 보상얻음
격자판 가장자리 근처의 상태들은 일반적으로 상태 가치가 낮으며, 움직일 때 -1의 보상 받음

2. 특정 상태에서의 높은 보상
A지역: 상태 가치가 가장 높은 곳, 에이전트가 A에서 행동을 취하면 항상 A'로 이동하며 +10의 보상 얻음
B지역: 두 번째로 상태 가치가 높은 곳, B에서 에이전트는 B'로 이동하며 +5의 보상 얻음

3. 가치 함수 테이블
테이블에서 각 상태별로 계산된 가치가 나열되어 있음
높은 보상을 얻을 수 있는 A와 B지역의 상태는 상대적으로 높은 값을 가지고 있으며,
격자 가장자리에 있는 상태들은 가치가 낮음
이 가치는 에이전트가 해당 상태에서 얻을 수 있는 기대 보상을 나타냄 
```
![image](https://github.com/user-attachments/assets/abed9ea1-1ca6-4862-a58a-ae3322236106)

## 🟡 웜홀이 있는 그리드월드 환경에서 상태별 가치 산출
```
1. 벨만 방정식을 통한 가치 계산
상태 A의 가치: A에서 A'로 이동하면, +10의 보상 얻을 수 있지만,
계산된 가치는 8.79로 +10보다 낮음 이동 중에 받는 보상(-1) 때문에 발생하는 차이
상태 B의 가치: B에서 B'로 이동하면 +5의 보상을 얻을 수 있지만, 계산된 가치는 5.32로
+5보다 약간 높음 B의 상태 가치는 중앙에 위치한 상태의 특성으로 인해 더 높음

2. 그리드 월드 구조
A에서 A'로 이동하는 경로는 가장자리 근처에 있으며, B에서 B'로 이동하는 경로는 중앙에 위치
이 상태들의 위치와 경로에 따라 각 상태에서 받을 수 있는 보상이 달라짐

3. 가치 함수 테이블
각 상태별로 계산된 가치가 테이블로 나와 있음 8.79와 5.32는 상태 A, B에서의 가치,
다른 상태들보다 높게 나타남 
```
![image](https://github.com/user-attachments/assets/3a691d71-47a1-454b-8eb3-c0e318c46af4)
















